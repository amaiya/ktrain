{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative AI with *ktrain*\n",
    "\n",
    "As of v0.38.x, the `generative_ai` module in **ktrain** is now powered by our [OnPrem.LLM](https://github.com/amaiya/onprem) package.  The `generative_ai.LLM` class replaces the older `generative_ai.GenerativeAI` class. \n",
    "\n",
    "Think of the `generative_ai` module in **ktrain** as a lightweight version of ChatGPT that can be run locally on your own machine. Since it does not communicate with external APIs like OpenAI, it can be used with non-public data and within air-gapped networks (e.g., behind corporate firewalls). For lighter-weight deployments, you can also install and use **OnPrem.LLM** separately without the rest of **ktrain**, if you'd like:\n",
    "\n",
    "\n",
    "```python\n",
    "!pip install onprem\n",
    "from onprem import LLM\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "By default, the `generative_ai.LLM` module uses a CPU. To speed up inference with a GPU (even a not-very-powerful one), you can simply perform the following steps:\n",
    "1. Install `llama-cpp-python` with `cuBlas` support:\n",
    "```sh\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.69 --no-cache-dir\n",
    "```\n",
    "2. Supply the `n_gpu_layers` paramger to `GenerativeAI` (with a value based on your availble GPU memory).\n",
    "```python\n",
    "llm = LLM(n_gpu_layers=35)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we will use an NVIDIA Titan V GPU with 12GB memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are about to download the LLM Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin to the /home/amaiya/onprem_data folder. Are you sure? (Y/n) Y\n",
      "[██████████████████████████████████████████████████]"
     ]
    }
   ],
   "source": [
    "from ktrain.text.generative_ai import LLM # or use \"from onprem import LLM\" instead\n",
    "llm = LLM(n_gpu_layers=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this model is instruction-fine-tuned, you should supply prompts in the form of instructions of what you want the model to do for you.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA TITAN V, compute capability 7.0\n",
      "  Device 1: NVIDIA TITAN V, compute capability 7.0\n",
      "llama.cpp: loading model from /home/amaiya/onprem_data/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA TITAN V) as main device\n",
      "llama_model_load_internal: mem required  = 1862.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 384 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4956 MB\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Name]:  David Melvin\n",
      "[Position]: Investment and Financial Services Professional\n",
      "[Company]: CITIC CLSA"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Extract the Name, Position, and Company from the following sentences.  Here are some examples.\n",
    "[Text]: Fred is a serial entrepreneur. Co-founder and CEO of Platform.sh, he previously co-founded Commerce Guys, a leading Drupal ecommerce provider. His mission is to guarantee that as we continue on an ambitious journey to profoundly transform how cloud computing is used and perceived, we keep our feet well on the ground continuing the rapid growth we have enjoyed up until now. \n",
    "[Name]: Fred\n",
    "[Position]: Co-founder and CEO\n",
    "[Company]: Platform.sh\n",
    "###\n",
    "[Text]: Microsoft (the word being a portmanteau of \"microcomputer software\") was founded by Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. Steve Ballmer replaced Gates as CEO in 2000, and later envisioned a \"devices and services\" strategy.\n",
    "[Name]:  Steve Ballmer\n",
    "[Position]: CEO\n",
    "[Company]: Microsoft\n",
    "###\n",
    "[Text]: Franck Riboud was born on 7 November 1955 in Lyon. He is the son of Antoine Riboud, the previous CEO, who transformed the former European glassmaker BSN Group into a leading player in the food industry. He is the CEO at Danone.\n",
    "[Name]:  Franck Riboud\n",
    "[Position]: CEO\n",
    "[Company]: Danone\n",
    "###\n",
    "[Text]: David Melvin is an investment and financial services professional at CITIC CLSA with over 30 years’ experience in investment banking and private equity. He is currently a Senior Adviser of CITIC CLSA.\n",
    "\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar and Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't want to go."
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Correct the grammar and spelling in the supplied sentences.  Here are some examples.\n",
    "[Sentence]:\n",
    "I love goin to the beach.\n",
    "[Correction]: I love going to the beach.\n",
    "[Sentence]:\n",
    "Let me hav it!\n",
    "[Correction]: Let me have it!\n",
    "[Sentence]:\n",
    "It have too many drawbacks.\n",
    "[Correction]: It has too many drawbacks.\n",
    "[Sentence]:\n",
    "I do not wan to go\n",
    "[Correction]:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Positive"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Classify each sentence as either positive, negative, or neutral.  Here are some examples.\n",
    "[Sentence]: I love going to the beach.\n",
    "[[Classification]: Positive\n",
    "[Sentence]: It is 10am right now.\n",
    "[Classification]: Neutral\n",
    "[Sentence]: I just got fired from my job.\n",
    "[Classification]: Negative\n",
    "[Sentence]: The reactivity of  your team has been amazing, thanks!\n",
    "[Classification]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing and Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text reads as follows: \"After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.\""
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Paraphrase the following text delimited by triple backticks. \n",
    "```After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.```\n",
    "\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The GPU plan."
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Answer the Question based on the Context.  Here are some examples.\n",
    "[Context]: \n",
    "NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
    "Question: When was NLP Cloud founded?\n",
    "[Answer]: \n",
    "2021\n",
    "###\n",
    "[Context]:\n",
    "NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
    "[Question]: \n",
    "What did NLP Cloud develop?\n",
    "[Answer]:\n",
    "API\n",
    "###\n",
    "[Context]:\n",
    "All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
    "[Question]:\n",
    "When can plans be stopped?\n",
    "[Answer]:\n",
    "Anytime\n",
    "###\n",
    "[Context]:\n",
    "The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
    "[Question]:\n",
    "Which plan is recommended for GPT-J?\n",
    "Answer:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Product Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comfortable and stylish t-shirts for men, starting from $39."
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Generate a Sentence from the Keywords. Here are some examples.\n",
    "[Keywords]:\n",
    "shoes, women, $59\n",
    "[Sentence]:\n",
    "Beautiful shoes for women at the price of $59.\n",
    "###\n",
    "[Keywords]:\n",
    "trousers, men, $69\n",
    "[Sentence]:\n",
    "Modern trousers for men, for $69 only.\n",
    "###\n",
    "[Keywords]:\n",
    "gloves, winter, $19\n",
    "[Sentence]: \n",
    "Amazingly hot gloves for cold winters, at $19.\n",
    "###\n",
    "[Keywords]: \n",
    "t-shirt, men, $39\n",
    "[Sentence]:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Climate change is like a fire, we need to put it out before it becomes an uncontrollable blaze."
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Generate a tweet based on the supplied Keyword. Here are some examples.\n",
    "[Keyword]:\n",
    "markets\n",
    "[Tweet]:\n",
    "Take feedback from nature and markets, not from people\n",
    "###\n",
    "[Keyword]:\n",
    "children\n",
    "[Tweet]:\n",
    "Maybe we die so we can come back as children.\n",
    "###\n",
    "[Keyword]:\n",
    "startups\n",
    "[Tweet]: \n",
    "Startups should not worry about how to put out fires, they should worry about how to start them.\n",
    "###\n",
    "[Keyword]:\n",
    "climate change\n",
    "[Tweet]:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Draft Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Dear Shareholder,\n",
      "I am writing today as a fellow shareholder of Tesla and supporter of Elon Musk's mission to accelerate the world's transition to sustainable energy. \n",
      "As you may know, Tesla has achieved numerous milestones in this pursuit, including the launch of the Model S sedan, the introduction of the Powerwall home battery system, and most recently, the unveiling of the Model 3 electric vehicle. These achievements are just a few examples of Tesla's ongoing innovation and dedication to environmental sustainability.\n",
      "Looking ahead, I believe that Tesla has an exciting future ahead as it continues to expand its product offerings, reach new markets, and increase production capacity. With the global climate crisis becoming an increasingly pressing issue, now more than ever we need companies like Tesla to lead the way in promoting sustainable solutions.\n",
      "I am excited to be a part of this journey with you as a shareholder, and I encourage you to join me in supporting Tesla's continued efforts toward a cleaner, brighter future for all.\n",
      "Sincerely,"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Generate an email introducing Tesla to shareholders.\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk to Your Documents\n",
    "\n",
    "The `GenerativeAI` module also allows you to talk to your documents. Place your PDFs, TXT, Microsoft Word, or PowerPoint documents in a folder and point `GenerativeAI.ingest` to it. \n",
    "\n",
    "The default model used above is a heavily quantized 7B parameter model that may hallucinate on question-answering applications like this. Thus, we will use a slightly bigger and better model here by supplying the `use_larger=True` to `LLM`. (You may need to reload this noteobok and begin with the cell below.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are about to download the LLM wizardlm-13b-v1.2.ggmlv3.q4_0.bin to the /home/amaiya/onprem_data folder. Are you sure? (Y/n) Y\n",
      "[██████████████████████████████████████████████████]"
     ]
    }
   ],
   "source": [
    "from ktrain.text.generative_ai import LLM\n",
    "llm = LLM(use_larger=True, n_gpu_layers=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll download the **ktrain** paper from ArXiv and ask it a question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created temporary directory /tmp/tmpu280u_gz\n",
      "Creating new vectorstore\n",
      "Loading documents from /tmp/tmpu280u_gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 new documents from /tmp/tmpu280u_gz\n",
      "Split into 57 chunks of text (max. 500 tokens each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "!wget --user-agent=\"Mozilla\" https://arxiv.org/pdf/2004.10703.pdf -O /tmp/downloaded_paper.pdf -q\n",
    "import tempfile, shutil, os\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    print('created temporary directory', tmpdirname)\n",
    "    shutil.copyfile('/tmp/downloaded_paper.pdf', os.path.join(tmpdirname, 'downloaded_paper.pdf'))\n",
    "    llm.ingest(tmpdirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ktrain is a low-code library that focuses on automating some aspects of the machine learning workflow, particularly for domain experts who may not be experienced in machine learning or software coding. It is designed to complement and augment human engineers rather than replace them, and it uses automation to facilitate the full ML workow from data curation and preprocessing to training, tuning, troubleshooting, and model application."
     ]
    }
   ],
   "source": [
    "answer, sources = llm.ask('What is ktrain? Remember to only use the provided context.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "The constructor for `GenerativeAI` accepts additional parameters such as `max_tokens` that controls the maxmum number of tokens to generate. You can adjust them, as necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
