{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative AI with *ktrain*\n",
    "\n",
    "*ktrain* supports a Generative AI module that is currently based on an instruction-fine-tuned version of GPT-J trained on NLP Cloud. Think of it as a lightweight version of ChatGPT that can be run locally on your own machine. As a smaller model, it will not perform as well as GPT-4, ChatGPT, etc.  However, since it does not communicate with external APIs like OpenAI, it can be used with non-public data.\n",
    "\n",
    "The model requires a GPU with at least 16GB of GPU memory or VRAM.  If you have less than this (or no GPU at all), you can use a CPU (provided you have enough RAM) by supplying `device=\"cpu\"` as an argument:\n",
    "```python\n",
    "model = GenerativeAI(device='cpu')\n",
    "```\n",
    "In this notebook, we omit the `device` argument, in which case a GPU will be used if available (and a CPU will be used if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktrain.text.generative_ai import GenerativeAI\n",
    "model = GenerativeAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this model is instruction-fine-tuned, you should supply prompts in the form of instructions of what you want the model to do for you.  \n",
    "\n",
    "**Tip**: Due to the way the model was trained, you should only use newlines to separate cohesive units of information fed to the model. This is illustrated through various examples below. If the model encounters a misplaced newline character it doesn't like, it may output nothing.\n",
    "\n",
    "We also caution that the model has no guard rails against producing content that is potentially biased, offensive, or factually-incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar and Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Zero-shot* prompting is when you do not provide any examples for how to complete the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not want to go.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Correct spelling and grammar from the following text.\n",
    "I do not wan to go\n",
    "\"\"\"\n",
    "print(model.execute(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model returned the right answer.  If the model answers **incorrectly**, you can provide some illustrative examples to help it perform its assigned task (known as *few-shot* prompting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not want to go.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Correct the grammar and spelling in the supplied sentences.  Here are some examples.\n",
    "[Sentence]:\n",
    "I love goin to the beach.\n",
    "[Correction]:\n",
    "I love going to the beach.\n",
    "###\n",
    "[Sentence]:\n",
    "Let me hav it!\n",
    "[Correction]:\n",
    "Let me have it!\n",
    "###\n",
    "[Sentence]:\n",
    "It have too many drawbacks.\n",
    "[Correction]:\n",
    "It has too many drawbacks.\n",
    "###\n",
    "[Sentence]:\n",
    "I do not wan to go\n",
    "[Correction]:\"\"\"\n",
    "print(model.execute(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This default model for Generative AI is small enough to run on a single GPU/CPU, but is also sensitive to the structure of the prompt and where newlines are inserted.  You can use the examples in this notebook for guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Tell me whether the following sentence is positive,  negative, or neutral in sentiment.\n",
    "The reactivity of  your team has been amazing, thanks!\"\"\"\n",
    "print(model.execute(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Name]: David Melvin\n",
      "[Position]: Senior Adviser of CITIC CLSA\n",
      "[Company]: CITIC CLSA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Extract the Name, Position, and Company from the following sentences.  Here are some examples.\n",
    "[Text]: Fred is a serial entrepreneur. Co-founder and CEO of Platform.sh, he previously co-founded Commerce Guys, a leading Drupal ecommerce provider. His mission is to guarantee that as we continue on an ambitious journey to profoundly transform how cloud computing is used and perceived, we keep our feet well on the ground continuing the rapid growth we have enjoyed up until now. \n",
    "[Name]: Fred\n",
    "[Position]: Co-founder and CEO\n",
    "[Company]: Platform.sh\n",
    "###\n",
    "[Text]: Microsoft (the word being a portmanteau of \"microcomputer software\") was founded by Bill Gates on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. Steve Ballmer replaced Gates as CEO in 2000, and later envisioned a \"devices and services\" strategy.\n",
    "[Name]:  Steve Ballmer\n",
    "[Position]: CEO\n",
    "[Company]: Microsoft\n",
    "###\n",
    "[Text]: Franck Riboud was born on 7 November 1955 in Lyon. He is the son of Antoine Riboud, the previous CEO, who transformed the former European glassmaker BSN Group into a leading player in the food industry. He is the CEO at Danone.\n",
    "[Name]:  Franck Riboud\n",
    "[Position]: CEO\n",
    "[Company]: Danone\n",
    "###\n",
    "[Text]: David Melvin is an investment and financial services professional at CITIC CLSA with over 30 yearsâ€™ experience in investment banking and private equity. He is currently a Senior Adviser of CITIC CLSA.\n",
    "\"\"\"\n",
    "print(model.execute(prompt))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James Gandolfini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence:Paul Newman is a great actor.\n",
    "People:\n",
    "Paul Newman\n",
    "Sentence:\n",
    "I like James Gandolfini's acting.\n",
    "People:\"\"\"\n",
    "print(model.execute(prompt))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paraphrasing\n",
    "**Pro-Tip**: Remove any embedded newlines in the text you're paraphrasing or it will confuse the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After a 20-year war, Trump and Biden's decision to withdraw American troops from Afghanistan resulted in the fall of Kabul to the Taliban, without resistance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Paraphrase the following text. \n",
    "After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.\"\"\"\n",
    "print(model.execute(prompt))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a GPU plan is recommended for GPT-J.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Answer the Question based on the Context.  Here are some examples.\n",
    "[Context]: \n",
    "NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
    "Question: When was NLP Cloud founded?\n",
    "[Answer]: \n",
    "2021\n",
    "###\n",
    "[Context]:\n",
    "NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
    "[Question]: \n",
    "What did NLP Cloud develop?\n",
    "[Answer]:\n",
    "API\n",
    "###\n",
    "[Context]:\n",
    "All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
    "[Question]:\n",
    "When can plans be stopped?\n",
    "[Answer]:\n",
    "Anytime\n",
    "###\n",
    "[Context]:\n",
    "The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
    "[Question]:\n",
    "Which plan is recommended for GPT-J?\n",
    "Answer:\"\"\"\n",
    "print(model.execute(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Product Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A t-shirt for men, with a great design and color options, at $39.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"Generate a Sentence from the Keywords. Here are some examples.\n",
    "[Keywords]:\n",
    "shoes, women, $59\n",
    "[Sentence]:\n",
    "Beautiful shoes for women at the price of $59.\n",
    "###\n",
    "[Keywords]:\n",
    "trousers, men, $69\n",
    "[Sentence]:\n",
    "Modern trousers for men, for $69 only.\n",
    "###\n",
    "[Keywords]:\n",
    "gloves, winter, $19\n",
    "[Sentence]: \n",
    "Amazingly hot gloves for cold winters, at $19.\n",
    "###\n",
    "[Keywords]: \n",
    "t-shirt, men, $39\n",
    "[Sentence]:\"\"\"\n",
    "print(model.execute(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Change is real and not a distant threat--it's happening now and will only get worse unless we take action.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Generate a tweet based on the supplied Keyword. Here are some examples.\n",
    "[Keyword]:\n",
    "markets\n",
    "[Tweet]:\n",
    "Take feedback from nature and markets, not from people\n",
    "###\n",
    "[Keyword]:\n",
    "children\n",
    "[Tweet]:\n",
    "Maybe we die so we can come back as children.\n",
    "###\n",
    "[Keyword]:\n",
    "startups\n",
    "[Tweet]: \n",
    "Startups should not worry about how to put out fires, they should worry about how to start them.\n",
    "###\n",
    "[Keyword]:\n",
    "climate change\n",
    "[Tweet]:\"\"\"\n",
    "print(model.execute(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Email Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear [Shareholder],\n",
      "\n",
      "Tesla is taking the world by storm and we'd like to share our exciting news with you. We are committed to creating a future that is powered by renewable energy and we want to make sure that we provide our shareholders with the most innovative and exciting products and services. \n",
      "\n",
      "We invite you to join us in our mission to make electric cars and energy services accessible to all. Join us as a shareholder of Tesla and receive groundbreaking news and updates about our company and products.\n",
      "\n",
      "Sincerely, \n",
      "Tesla Team\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Generate an email introducing Tesla to shareholders.\"\"\"\n",
    "print(model.execute(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "The constructor for `GenerativeAI` accepts parameters that are fed directly to the `generate` method of the underlying model.  You can change them as necessary.  The default value for `max_new_tokens` (the upper limt on generated answers) has been set to 512 and `do_sample=True`.  All other paramters (e.g., `num_beams`) have been left at their defaults in the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
