<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ktrain.text.ner.data API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ktrain.text.ner.data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ...imports import *
from ... import utils as U
from .. import textutils as TU
from . import preprocessor as pp
from .preprocessor import NERPreprocessor

from .anago.preprocessing import IndexTransformer


MAXLEN = 128
WORD_COL = pp.WORD_COL
TAG_COL = pp.TAG_COL
SENT_COL = pp.SENT_COL



def entities_from_gmb(train_filepath, 
                      val_filepath=None,
                      use_char=False,
                      word_column=WORD_COL,
                      tag_column=TAG_COL,
                      sentence_column=SENT_COL,
                       encoding=None,
                       val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Loads sequence-labeled data from text file in the  Groningen
    Meaning Bank  (GMB) format.
    &#34;&#34;&#34;


    return entities_from_txt(train_filepath=train_filepath,
                             val_filepath=val_filepath,
                             use_char=use_char,
                             word_column=word_column,
                             tag_column=tag_column,
                             sentence_column=sentence_column,
                             data_format=&#39;gmb&#39;,
                             encoding=encoding,
                             val_pct=val_pct, verbose=verbose)


        
def entities_from_conll2003(train_filepath, 
                            val_filepath=None,
                            use_char=False,
                            encoding=None,
                            val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Loads sequence-labeled data from a file in CoNLL2003 format.
    &#34;&#34;&#34;
    return entities_from_txt(train_filepath=train_filepath,
                             val_filepath=val_filepath,
                             use_char=use_char,
                             data_format=&#39;conll2003&#39;,
                             encoding=encoding,
                             val_pct=val_pct, verbose=verbose)




def entities_from_txt(train_filepath, 
                      val_filepath=None,
                      use_char=False,
                      word_column=WORD_COL,
                      tag_column=TAG_COL,
                      sentence_column=SENT_COL,
                      data_format=&#39;conll2003&#39;,
                      encoding=None,
                      val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Loads sequence-labeled data from comma or tab-delmited text file.
    Format of file is either the CoNLL2003 format or Groningen Meaning
    Bank (GMB) format - specified with data_format parameter.

    In both formats, each word appars on a separate line along with
    its associated tag (or label).  
    The last item on each line should be the tag or label assigned to word.
    
    In the CoNLL2003 format, there is an empty line after
    each sentence.  In the GMB format, sentences are deliniated
    with a third column denoting the Sentence ID.


    
    More information on CoNLL2003 format: 
       https://www.aclweb.org/anthology/W03-0419

    CoNLL Example (each column is typically separated by space or tab)
    and  no column headings:

       Paul     B-PER
       Newman   I-PER
       is       O
       a        O
       great    O
       actor    O
       !        O

    More information on GMB format:
    Refer to ner_dataset.csv on Kaggle here:
       https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/version/2

    GMB example (each column separated by comma or tab)
    with column headings:

      SentenceID   Word     Tag    
      1            Paul     B-PER
      1            Newman   I-PER
      1            is       O
      1            a        O
      1            great    O
      1            actor    O
      1            !        O
    

    Args:
        train_filepath(str): file path to training CSV
        val_filepath (str): file path to validation dataset
        use_char(bool):    If True, data will be preprocessed to use character embeddings in addition to word embeddings
        word_column(str): name of column containing the text
        tag_column(str): name of column containing lael
        sentence_column(str): name of column containing Sentence IDs
        data_format(str): one of colnll2003 or gmb
                          word_column, tag_column, and sentence_column
                          ignored if &#39;conll2003&#39;
        encoding(str): the encoding to use.  If None, encoding is discovered automatically
        val_pct(float): Proportion of training to use for validation.
        verbose (boolean): verbosity
    &#34;&#34;&#34;



    # set dataframe converter
    if data_format == &#39;gmb&#39;:
        data_to_df = pp.gmb_to_df
    else:
        data_to_df = pp.conll2003_to_df
        word_column, tag_column, sentence_column = WORD_COL, TAG_COL, SENT_COL

    # detect encoding
    if encoding is None:
        with open(train_filepath, &#39;rb&#39;) as f:
            encoding = TU.detect_encoding(f.read())
            U.vprint(&#39;detected encoding: %s (if wrong, set manually)&#39; % (encoding), verbose=verbose)

    # create dataframe
    train_df = data_to_df(train_filepath, encoding=encoding)


    val_df = None if val_filepath is None else data_to_df(val_filepath, encoding=encoding)
    return entities_from_df(train_df,
                            val_df=val_df,
                            word_column=word_column,
                            tag_column=tag_column,
                            sentence_column=sentence_column,
                            use_char=use_char,
                            val_pct=val_pct, verbose=verbose)



def entities_from_df(train_df,
                     val_df=None,
                     word_column=WORD_COL,
                     tag_column=TAG_COL,
                     sentence_column=SENT_COL,
                     use_char=False,
                     val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Load entities from pandas DataFrame
    Args:
      train_df(pd.DataFrame): training data
      val_df(pdf.DataFrame): validation data
      word_column(str): name of column containing the text
      tag_column(str): name of column containing lael
      sentence_column(str): name of column containing Sentence IDs
      use_char(bool):    If True, data will be preprocessed to use character embeddings  in addition to word embeddings
      verbose (boolean): verbosity

    &#34;&#34;&#34;
    # process dataframe and instantiate NERPreprocessor
    x, y  = pp.process_df(train_df, 
                          word_column=word_column,
                          tag_column=tag_column,
                          sentence_column=sentence_column,
                          verbose=verbose)

    # get validation set
    if val_df is None:
        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=val_pct)
    else:
        x_train, y_train = x, y
        (x_valid, y_valid)  = pp.process_df(val_df,
                                            word_column=word_column,
                                            tag_column=tag_column,
                                            sentence_column=sentence_column,
                                            verbose=0)

    # preprocess and convert to generator
    p = IndexTransformer(use_char=use_char)
    preproc = NERPreprocessor(p)
    preproc.fit(x_train, y_train)
    trn = pp.NERSequence(x_train, y_train, batch_size=U.DEFAULT_BS, p=p)
    val = pp.NERSequence(x_valid, y_valid, batch_size=U.DEFAULT_BS, p=p)

    return ( trn, val, preproc)



def entities_from_array(x_train, y_train,
                        x_test=None, y_test=None,
                        use_char=False,
                        val_pct=0.1,
                        verbose=1):
    &#34;&#34;&#34;
    Load entities from arrays
    Args:
      x_train(list): list of list of entity tokens for training
                     Example: x_train = [[&#39;Hello&#39;, &#39;world&#39;], [&#39;Hello&#39;, &#39;Cher&#39;], [&#39;I&#39;, &#39;love&#39;, &#39;Chicago&#39;]]
      y_train(list): list of list of tokens representing entity labels
                     Example:  y_train = [[&#39;O&#39;, &#39;O&#39;], [&#39;O&#39;, &#39;B-PER&#39;], [&#39;O&#39;, &#39;O&#39;, &#39;B-LOC&#39;]]
      x_test(list): list of list of entity tokens for validation 
                     Example: x_train = [[&#39;Hello&#39;, &#39;world&#39;], [&#39;Hello&#39;, &#39;Cher&#39;], [&#39;I&#39;, &#39;love&#39;, &#39;Chicago&#39;]]
      y_test(list): list of list of tokens representing entity labels
                     Example:  y_train = [[&#39;O&#39;, &#39;O&#39;], [&#39;O&#39;, &#39;B-PER&#39;], [&#39;O&#39;, &#39;O&#39;, &#39;B-LOC&#39;]]
     use_char(bool):    If True, data will be preprocessed to use character embeddings  in addition to word embeddings
     val_pct(float):  percentage of training to use for validation if no validation data is supplied
     verbose (boolean): verbosity

    &#34;&#34;&#34;
    # TODO: converting to df to use entities_from_df - needs to be refactored
    train_df = pp.array_to_df(x_train, y_train) 
    val_df = None
    if x_test is not None and y_test is not None:
        val_df = pp.array_to_df(x_test, y_test)
    if verbose:
        print(&#39;training data sample:&#39;)
        print(train_df.head())
        if val_df is not None:
            print(&#39;validation data sample:&#39;)
            print(val_df.head())
    return entities_from_df(train_df, val_df=val_df, val_pct=val_pct, 
                            use_char=use_char, verbose=verbose)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ktrain.text.ner.data.entities_from_array"><code class="name flex">
<span>def <span class="ident">entities_from_array</span></span>(<span>x_train, y_train, x_test=None, y_test=None, use_char=False, val_pct=0.1, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Load entities from arrays</p>
<h2 id="args">Args</h2>
<p>x_train(list): list of list of entity tokens for training
Example: x_train = [['Hello', 'world'], ['Hello', 'Cher'], ['I', 'love', 'Chicago']]
y_train(list): list of list of tokens representing entity labels
Example:
y_train = [['O', 'O'], ['O', 'B-PER'], ['O', 'O', 'B-LOC']]
x_test(list): list of list of entity tokens for validation
Example: x_train = [['Hello', 'world'], ['Hello', 'Cher'], ['I', 'love', 'Chicago']]
y_test(list): list of list of tokens representing entity labels
Example:
y_train = [['O', 'O'], ['O', 'B-PER'], ['O', 'O', 'B-LOC']]
use_char(bool):
If True, data will be preprocessed to use character embeddings
in addition to word embeddings
val_pct(float):
percentage of training to use for validation if no validation data is supplied
verbose (boolean): verbosity</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entities_from_array(x_train, y_train,
                        x_test=None, y_test=None,
                        use_char=False,
                        val_pct=0.1,
                        verbose=1):
    &#34;&#34;&#34;
    Load entities from arrays
    Args:
      x_train(list): list of list of entity tokens for training
                     Example: x_train = [[&#39;Hello&#39;, &#39;world&#39;], [&#39;Hello&#39;, &#39;Cher&#39;], [&#39;I&#39;, &#39;love&#39;, &#39;Chicago&#39;]]
      y_train(list): list of list of tokens representing entity labels
                     Example:  y_train = [[&#39;O&#39;, &#39;O&#39;], [&#39;O&#39;, &#39;B-PER&#39;], [&#39;O&#39;, &#39;O&#39;, &#39;B-LOC&#39;]]
      x_test(list): list of list of entity tokens for validation 
                     Example: x_train = [[&#39;Hello&#39;, &#39;world&#39;], [&#39;Hello&#39;, &#39;Cher&#39;], [&#39;I&#39;, &#39;love&#39;, &#39;Chicago&#39;]]
      y_test(list): list of list of tokens representing entity labels
                     Example:  y_train = [[&#39;O&#39;, &#39;O&#39;], [&#39;O&#39;, &#39;B-PER&#39;], [&#39;O&#39;, &#39;O&#39;, &#39;B-LOC&#39;]]
     use_char(bool):    If True, data will be preprocessed to use character embeddings  in addition to word embeddings
     val_pct(float):  percentage of training to use for validation if no validation data is supplied
     verbose (boolean): verbosity

    &#34;&#34;&#34;
    # TODO: converting to df to use entities_from_df - needs to be refactored
    train_df = pp.array_to_df(x_train, y_train) 
    val_df = None
    if x_test is not None and y_test is not None:
        val_df = pp.array_to_df(x_test, y_test)
    if verbose:
        print(&#39;training data sample:&#39;)
        print(train_df.head())
        if val_df is not None:
            print(&#39;validation data sample:&#39;)
            print(val_df.head())
    return entities_from_df(train_df, val_df=val_df, val_pct=val_pct, 
                            use_char=use_char, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.data.entities_from_conll2003"><code class="name flex">
<span>def <span class="ident">entities_from_conll2003</span></span>(<span>train_filepath, val_filepath=None, use_char=False, encoding=None, val_pct=0.1, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads sequence-labeled data from a file in CoNLL2003 format.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entities_from_conll2003(train_filepath, 
                            val_filepath=None,
                            use_char=False,
                            encoding=None,
                            val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Loads sequence-labeled data from a file in CoNLL2003 format.
    &#34;&#34;&#34;
    return entities_from_txt(train_filepath=train_filepath,
                             val_filepath=val_filepath,
                             use_char=use_char,
                             data_format=&#39;conll2003&#39;,
                             encoding=encoding,
                             val_pct=val_pct, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.data.entities_from_df"><code class="name flex">
<span>def <span class="ident">entities_from_df</span></span>(<span>train_df, val_df=None, word_column='Word', tag_column='Tag', sentence_column='SentenceID', use_char=False, val_pct=0.1, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Load entities from pandas DataFrame</p>
<h2 id="args">Args</h2>
<dl>
<dt>train_df(pd.DataFrame): training data</dt>
<dt>val_df(pdf.DataFrame): validation data</dt>
<dt>word_column(str): name of column containing the text</dt>
<dt>tag_column(str): name of column containing lael</dt>
<dt>sentence_column(str): name of column containing Sentence IDs</dt>
<dt>use_char(bool):
If True, data will be preprocessed to use character embeddings
in addition to word embeddings</dt>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boolean</code></dt>
<dd>verbosity</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entities_from_df(train_df,
                     val_df=None,
                     word_column=WORD_COL,
                     tag_column=TAG_COL,
                     sentence_column=SENT_COL,
                     use_char=False,
                     val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Load entities from pandas DataFrame
    Args:
      train_df(pd.DataFrame): training data
      val_df(pdf.DataFrame): validation data
      word_column(str): name of column containing the text
      tag_column(str): name of column containing lael
      sentence_column(str): name of column containing Sentence IDs
      use_char(bool):    If True, data will be preprocessed to use character embeddings  in addition to word embeddings
      verbose (boolean): verbosity

    &#34;&#34;&#34;
    # process dataframe and instantiate NERPreprocessor
    x, y  = pp.process_df(train_df, 
                          word_column=word_column,
                          tag_column=tag_column,
                          sentence_column=sentence_column,
                          verbose=verbose)

    # get validation set
    if val_df is None:
        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=val_pct)
    else:
        x_train, y_train = x, y
        (x_valid, y_valid)  = pp.process_df(val_df,
                                            word_column=word_column,
                                            tag_column=tag_column,
                                            sentence_column=sentence_column,
                                            verbose=0)

    # preprocess and convert to generator
    p = IndexTransformer(use_char=use_char)
    preproc = NERPreprocessor(p)
    preproc.fit(x_train, y_train)
    trn = pp.NERSequence(x_train, y_train, batch_size=U.DEFAULT_BS, p=p)
    val = pp.NERSequence(x_valid, y_valid, batch_size=U.DEFAULT_BS, p=p)

    return ( trn, val, preproc)</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.data.entities_from_gmb"><code class="name flex">
<span>def <span class="ident">entities_from_gmb</span></span>(<span>train_filepath, val_filepath=None, use_char=False, word_column='Word', tag_column='Tag', sentence_column='SentenceID', encoding=None, val_pct=0.1, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads sequence-labeled data from text file in the
Groningen
Meaning Bank
(GMB) format.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entities_from_gmb(train_filepath, 
                      val_filepath=None,
                      use_char=False,
                      word_column=WORD_COL,
                      tag_column=TAG_COL,
                      sentence_column=SENT_COL,
                       encoding=None,
                       val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Loads sequence-labeled data from text file in the  Groningen
    Meaning Bank  (GMB) format.
    &#34;&#34;&#34;


    return entities_from_txt(train_filepath=train_filepath,
                             val_filepath=val_filepath,
                             use_char=use_char,
                             word_column=word_column,
                             tag_column=tag_column,
                             sentence_column=sentence_column,
                             data_format=&#39;gmb&#39;,
                             encoding=encoding,
                             val_pct=val_pct, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.ner.data.entities_from_txt"><code class="name flex">
<span>def <span class="ident">entities_from_txt</span></span>(<span>train_filepath, val_filepath=None, use_char=False, word_column='Word', tag_column='Tag', sentence_column='SentenceID', data_format='conll2003', encoding=None, val_pct=0.1, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads sequence-labeled data from comma or tab-delmited text file.
Format of file is either the CoNLL2003 format or Groningen Meaning
Bank (GMB) format - specified with data_format parameter.</p>
<p>In both formats, each word appars on a separate line along with
its associated tag (or label).<br>
The last item on each line should be the tag or label assigned to word.</p>
<p>In the CoNLL2003 format, there is an empty line after
each sentence.
In the GMB format, sentences are deliniated
with a third column denoting the Sentence ID.</p>
<p>More information on CoNLL2003 format:
<a href="https://www.aclweb.org/anthology/W03-0419">https://www.aclweb.org/anthology/W03-0419</a></p>
<p>CoNLL Example (each column is typically separated by space or tab)
and
no column headings:</p>
<p>Paul
B-PER
Newman
I-PER
is
O
a
O
great
O
actor
O
!
O</p>
<p>More information on GMB format:
Refer to ner_dataset.csv on Kaggle here:
<a href="https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/version/2">https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/version/2</a></p>
<p>GMB example (each column separated by comma or tab)
with column headings:</p>
<p>SentenceID
Word
Tag
<br>
1
Paul
B-PER
1
Newman
I-PER
1
is
O
1
a
O
1
great
O
1
actor
O
1
!
O</p>
<h2 id="args">Args</h2>
<dl>
<dt>train_filepath(str): file path to training CSV</dt>
<dt><strong><code>val_filepath</code></strong> :&ensp;<code>str</code></dt>
<dd>file path to validation dataset</dd>
<dt>use_char(bool):
If True, data will be preprocessed to use character embeddings in addition to word embeddings</dt>
<dt>word_column(str): name of column containing the text</dt>
<dt>tag_column(str): name of column containing lael</dt>
<dt>sentence_column(str): name of column containing Sentence IDs</dt>
<dt>data_format(str): one of colnll2003 or gmb</dt>
<dt>word_column, tag_column, and sentence_column</dt>
<dt>ignored if 'conll2003'</dt>
<dt>encoding(str): the encoding to use.
If None, encoding is discovered automatically</dt>
<dt>val_pct(float): Proportion of training to use for validation.</dt>
<dt><strong><code>verbose</code></strong> :&ensp;<code>boolean</code></dt>
<dd>verbosity</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entities_from_txt(train_filepath, 
                      val_filepath=None,
                      use_char=False,
                      word_column=WORD_COL,
                      tag_column=TAG_COL,
                      sentence_column=SENT_COL,
                      data_format=&#39;conll2003&#39;,
                      encoding=None,
                      val_pct=0.1, verbose=1):
    &#34;&#34;&#34;
    Loads sequence-labeled data from comma or tab-delmited text file.
    Format of file is either the CoNLL2003 format or Groningen Meaning
    Bank (GMB) format - specified with data_format parameter.

    In both formats, each word appars on a separate line along with
    its associated tag (or label).  
    The last item on each line should be the tag or label assigned to word.
    
    In the CoNLL2003 format, there is an empty line after
    each sentence.  In the GMB format, sentences are deliniated
    with a third column denoting the Sentence ID.


    
    More information on CoNLL2003 format: 
       https://www.aclweb.org/anthology/W03-0419

    CoNLL Example (each column is typically separated by space or tab)
    and  no column headings:

       Paul     B-PER
       Newman   I-PER
       is       O
       a        O
       great    O
       actor    O
       !        O

    More information on GMB format:
    Refer to ner_dataset.csv on Kaggle here:
       https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/version/2

    GMB example (each column separated by comma or tab)
    with column headings:

      SentenceID   Word     Tag    
      1            Paul     B-PER
      1            Newman   I-PER
      1            is       O
      1            a        O
      1            great    O
      1            actor    O
      1            !        O
    

    Args:
        train_filepath(str): file path to training CSV
        val_filepath (str): file path to validation dataset
        use_char(bool):    If True, data will be preprocessed to use character embeddings in addition to word embeddings
        word_column(str): name of column containing the text
        tag_column(str): name of column containing lael
        sentence_column(str): name of column containing Sentence IDs
        data_format(str): one of colnll2003 or gmb
                          word_column, tag_column, and sentence_column
                          ignored if &#39;conll2003&#39;
        encoding(str): the encoding to use.  If None, encoding is discovered automatically
        val_pct(float): Proportion of training to use for validation.
        verbose (boolean): verbosity
    &#34;&#34;&#34;



    # set dataframe converter
    if data_format == &#39;gmb&#39;:
        data_to_df = pp.gmb_to_df
    else:
        data_to_df = pp.conll2003_to_df
        word_column, tag_column, sentence_column = WORD_COL, TAG_COL, SENT_COL

    # detect encoding
    if encoding is None:
        with open(train_filepath, &#39;rb&#39;) as f:
            encoding = TU.detect_encoding(f.read())
            U.vprint(&#39;detected encoding: %s (if wrong, set manually)&#39; % (encoding), verbose=verbose)

    # create dataframe
    train_df = data_to_df(train_filepath, encoding=encoding)


    val_df = None if val_filepath is None else data_to_df(val_filepath, encoding=encoding)
    return entities_from_df(train_df,
                            val_df=val_df,
                            word_column=word_column,
                            tag_column=tag_column,
                            sentence_column=sentence_column,
                            use_char=use_char,
                            val_pct=val_pct, verbose=verbose)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ktrain.text.ner" href="index.html">ktrain.text.ner</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ktrain.text.ner.data.entities_from_array" href="#ktrain.text.ner.data.entities_from_array">entities_from_array</a></code></li>
<li><code><a title="ktrain.text.ner.data.entities_from_conll2003" href="#ktrain.text.ner.data.entities_from_conll2003">entities_from_conll2003</a></code></li>
<li><code><a title="ktrain.text.ner.data.entities_from_df" href="#ktrain.text.ner.data.entities_from_df">entities_from_df</a></code></li>
<li><code><a title="ktrain.text.ner.data.entities_from_gmb" href="#ktrain.text.ner.data.entities_from_gmb">entities_from_gmb</a></code></li>
<li><code><a title="ktrain.text.ner.data.entities_from_txt" href="#ktrain.text.ner.data.entities_from_txt">entities_from_txt</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>