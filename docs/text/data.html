<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ktrain.text.data API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ktrain.text.data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ..imports import *
from .. import utils as U
from  . import preprocessor as tpp
from . import textutils as TU



MAX_FEATURES = 20000
MAXLEN = 400



def texts_from_folder(datadir, classes=None, 
                      max_features=MAX_FEATURES, maxlen=MAXLEN,
                      ngram_range=1,
                      train_test_names=[&#39;train&#39;, &#39;test&#39;],
                      preprocess_mode=&#39;standard&#39;,
                      encoding=None, # detected automatically
                      lang=None, # detected automatically
                      val_pct=0.1, random_state=None,
                      verbose=1):
    &#34;&#34;&#34;
    ```
    Returns corpus as sequence of word IDs.
    Assumes corpus is in the following folder structure:
    ├── datadir
    │   ├── train
    │   │   ├── class0       # folder containing documents of class 0
    │   │   ├── class1       # folder containing documents of class 1
    │   │   ├── class2       # folder containing documents of class 2
    │   │   └── classN       # folder containing documents of class N
    │   └── test 
    │       ├── class0       # folder containing documents of class 0
    │       ├── class1       # folder containing documents of class 1
    │       ├── class2       # folder containing documents of class 2
    │       └── classN       # folder containing documents of class N

    Each subfolder should contain documents in plain text format.
    If train and test contain additional subfolders that do not represent
    classes, they can be ignored by explicitly listing the subfolders of
    interest using the classes argument.
    Args:
        datadir (str): path to folder
        classes (list): list of classes (subfolders to consider).
                        This is simply supplied as the categories argument
                        to sklearn&#39;s load_files function.
        max_features (int):  maximum number of unigrams to consider
                             Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen (int):  maximum length of tokens in document
        ngram_range (int):  If &gt; 1, will include 2=bigrams, 3=trigrams and bigrams
        train_test_names (list):  list of strings represnting the subfolder
                                 name for train and validation sets
                                 if test name is missing, &lt;val_pct&gt; of training
                                 will be used for validation
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        encoding (str):        character encoding to use. Auto-detected if None
        lang (str):            language.  Auto-detected if None.
        val_pct(float):        Onlyl used if train_test_names  has 1 and not 2 names
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                IF None, train/test split will be random
        verbose (bool):         verbosity
        
    ```
    &#34;&#34;&#34;

    # check train_test_names
    if len(train_test_names) &lt; 1 or len(train_test_names) &gt; 2:
        raise ValueError(&#39;train_test_names must have 1 or two elements for train and optionally validation&#39;)

    # read in training and test corpora
    train_str = train_test_names[0]
    train_b = load_files(os.path.join(datadir, train_str), shuffle=True, categories=classes)
    if len(train_test_names) &gt; 1:
        test_str = train_test_names[1]
        test_b = load_files(os.path.join(datadir,  test_str), shuffle=False, categories=classes)
        x_train = train_b.data
        y_train = train_b.target
        x_test = test_b.data
        y_test = test_b.target
    else:
        x_train, x_test, y_train, y_test = train_test_split(train_b.data, 
                                                            train_b.target, 
                                                            test_size=val_pct,
                                                            random_state=random_state)

    # decode based on supplied encoding
    if encoding is None:
        encoding = TU.detect_encoding(x_train)
        U.vprint(&#39;detected encoding: %s&#39; % (encoding), verbose=verbose)
    
    try:
        x_train = [x.decode(encoding) for x in x_train]
        x_test = [x.decode(encoding) for x in x_test]
    except:
        U.vprint(&#39;Decoding with %s failed 1st attempt - using %s with skips&#39; % (encoding, 
                                                                                encoding),
                                                                                verbose=verbose)
        x_train = TU.decode_by_line(x_train, encoding=encoding, verbose=verbose)
        x_test = TU.decode_by_line(x_test, encoding=encoding, verbose=verbose)


    # detect language
    if lang is None: lang = TU.detect_lang(x_train)
    check_unsupported_lang(lang, preprocess_mode)



    # return preprocessed the texts
    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)
    if None: raise ValueError(&#39;unsupported preprocess_mode&#39;)
    preproc = preproc_type(maxlen,
                           max_features,
                           class_names = train_b.target_names,
                           lang=lang, ngram_range=ngram_range)
    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    return (trn, val, preproc)





def texts_from_csv(train_filepath, 
                   text_column,
                   label_columns = [],
                   val_filepath=None,
                   max_features=MAX_FEATURES, maxlen=MAXLEN, 
                   val_pct=0.1, ngram_range=1, preprocess_mode=&#39;standard&#39;, 
                   encoding=None,  # auto-detected
                   lang=None,      # auto-detected
                   sep=&#39;,&#39;, 
                   is_regression=False,
                   random_state=None,       
                   verbose=1):
    &#34;&#34;&#34;
    ```
    Loads text data from CSV or TSV file. Class labels are assumed to be
    one of the following formats:
        1. one-hot-encoded or multi-hot-encoded arrays representing classes:
              Example with label_columns=[&#39;positive&#39;, &#39;negative&#39;] and text_column=&#39;text&#39;:
                text|positive|negative
                I like this movie.|1|0
                I hated this movie.|0|1
            Classification will have a single one in each row: [[1,0,0], [0,1,0]]]
            Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]
        2. labels are in a single column of string or integer values representing classs labels
               Example with label_columns=[&#39;label&#39;] and text_column=&#39;text&#39;:
                 text|label
                 I like this movie.|positive
                 I hated this movie.|negative
       3. labels are a single column of numerical values for text regression
          NOTE: Must supply is_regression=True for labels to be treated as numerical targets
                 wine_description|wine_price
                 Exquisite wine!|100
                 Wine for budget shoppers|8

    Args:
        train_filepath(str): file path to training CSV
        text_column(str): name of column containing the text
        label_column(list): list of columns that are to be treated as labels
        val_filepath(string): file path to test CSV.  If not supplied,
                               10% of documents in training CSV will be
                               used for testing/validation.
        max_features(int): max num of words to consider in vocabulary
                           Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
        ngram_range(int): size of multi-word phrases to consider
                          e.g., 2 will consider both 1-word phrases and 2-word phrases
                               limited by max_features
        val_pct(float): Proportion of training to use for validation.
                        Has no effect if val_filepath is supplied.
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        encoding (str):        character encoding to use. Auto-detected if None
        lang (str):            language.  Auto-detected if None.
        sep(str):              delimiter for CSV (comma is default)
        is_regression(bool):  If True, integer targets will be treated as numerical targets instead of class IDs
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                If None, train/test split will be random
        verbose (boolean): verbosity
    ```
    &#34;&#34;&#34;
    if encoding is None:
        with open(train_filepath, &#39;rb&#39;) as f:
            #encoding = chardet.detect(f.read())[&#39;encoding&#39;]
            #encoding = &#39;utf-8&#39; if encoding.lower() in [&#39;ascii&#39;, &#39;utf8&#39;, &#39;utf-8&#39;] else encoding
            encoding = TU.detect_encoding(f.read())
            U.vprint(&#39;detected encoding: %s (if wrong, set manually)&#39; % (encoding), verbose=verbose)

    train_df = pd.read_csv(train_filepath, encoding=encoding,sep=sep)
    val_df = pd.read_csv(val_filepath, encoding=encoding,sep=sep) if val_filepath is not None else None
    return texts_from_df(train_df,
                         text_column,
                         label_columns=label_columns,
                         val_df = val_df,
                         max_features=max_features,
                         maxlen=maxlen,
                         val_pct=val_pct,
                         ngram_range=ngram_range, 
                         preprocess_mode=preprocess_mode,
                         lang=lang, is_regression=is_regression, random_state=random_state,
                         verbose=verbose)


def texts_from_df(train_df, 
                   text_column,
                   label_columns = [],
                   val_df=None,
                   max_features=MAX_FEATURES, maxlen=MAXLEN, 
                   val_pct=0.1, ngram_range=1, preprocess_mode=&#39;standard&#39;, 
                   lang=None, # auto-detected
                   is_regression=False,
                   random_state=None,
                   verbose=1):
    &#34;&#34;&#34;
    ```
    Loads text data from Pandas dataframe file. Class labels are assumed to be
    one of the following formats:
        1. one-hot-encoded or multi-hot-encoded arrays representing classes:
              Example with label_columns=[&#39;positive&#39;, &#39;negative&#39;] and text_column=&#39;text&#39;:
                text|positive|negative
                I like this movie.|1|0
                I hated this movie.|0|1
            Classification will have a single one in each row: [[1,0,0], [0,1,0]]]
            Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]
        2. labels are in a single column of string or integer values representing class labels
               Example with label_columns=[&#39;label&#39;] and text_column=&#39;text&#39;:
                 text|label
                 I like this movie.|positive
                 I hated this movie.|negative
       3. labels are a single column of numerical values for text regression
          NOTE: Must supply is_regression=True for integer labels to be treated as numerical targets
                 wine_description|wine_price
                 Exquisite wine!|100
                 Wine for budget shoppers|8

    Args:
        train_df(dataframe): Pandas dataframe
        text_column(str): name of column containing the text
        label_columns(list): list of columns that are to be treated as labels
        val_df(dataframe): file path to test dataframe.  If not supplied,
                               10% of documents in training df will be
                               used for testing/validation.
        max_features(int): max num of words to consider in vocabulary.
                           Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
        ngram_range(int): size of multi-word phrases to consider
                          e.g., 2 will consider both 1-word phrases and 2-word phrases
                               limited by max_features
        val_pct(float): Proportion of training to use for validation.
                        Has no effect if val_filepath is supplied.
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        lang (str):            language.  Auto-detected if None.
        is_regression(bool):  If True, integer targets will be treated as numerical targets instead of class IDs
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                If None, train/test split will be random
        verbose (boolean): verbosity
    ```
    &#34;&#34;&#34;

    # read in train and test data
    train_df = train_df.copy()
    train_df[text_column].fillna(&#39;fillna&#39;, inplace=True)
    if val_df is not None:
        val_df = val_df.copy()
        val_df[text_column].fillna(&#39;fillna&#39;, inplace=True)
    else:
        train_df, val_df = train_test_split(train_df, test_size=val_pct, random_state=random_state)

    # transform labels
    ytransdf = U.YTransformDataFrame(label_columns, is_regression=is_regression)
    t_df = ytransdf.apply_train(train_df)
    v_df = ytransdf.apply_test(val_df)
    class_names = ytransdf.get_classes()
    new_lab_cols = ytransdf.get_label_columns(squeeze=True)
    x_train = t_df[text_column].values
    y_train = t_df[new_lab_cols].values
    x_test = v_df[text_column].values
    y_test = v_df[new_lab_cols].values

    # detect language
    if lang is None: lang = TU.detect_lang(x_train)
    check_unsupported_lang(lang, preprocess_mode)


    # return preprocessed the texts
    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)
    if None: raise ValueError(&#39;unsupported preprocess_mode&#39;)
    preproc = preproc_type(maxlen,
                           max_features,
                           class_names = class_names,
                           lang=lang, ngram_range=ngram_range)
    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    # QUICKFIX for #314
    preproc.ytransform.le = ytransdf.le
    return (trn, val, preproc)



def texts_from_array(x_train, y_train, x_test=None, y_test=None, 
                   class_names = [],
                   max_features=MAX_FEATURES, maxlen=MAXLEN, 
                   val_pct=0.1, ngram_range=1, preprocess_mode=&#39;standard&#39;, 
                   lang=None, # auto-detected
                   random_state=None,
                   verbose=1):
    &#34;&#34;&#34;
    ```
    Loads and preprocesses text data from arrays.
    texts_from_array can handle data for both text classification
    and text regression.  If class_names is empty, a regression task is assumed.
    Args:
        x_train(list): list of training texts 
        y_train(list): labels in one of the following forms:
                       1. list of integers representing classes (class_names is required)
                       2. list of strings representing classes (class_names is not needed and ignored.)
                       3. a one or multi hot encoded array representing classes (class_names is required)
                       4. numerical values for text regresssion (class_names should be left empty)
        x_test(list): list of training texts 
        y_test(list): labels in one of the following forms:
                       1. list of integers representing classes (class_names is required)
                       2. list of strings representing classes (class_names is not needed and ignored.)
                       3. a one or multi hot encoded array representing classes (class_names is required)
                       4. numerical values for text regresssion (class_names should be left empty)
        class_names (list): list of strings representing class labels
                            shape should be (num_examples,1) or (num_examples,)
        max_features(int): max num of words to consider in vocabulary
                           Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
        ngram_range(int): size of multi-word phrases to consider
                          e.g., 2 will consider both 1-word phrases and 2-word phrases
                               limited by max_features
        val_pct(float): Proportion of training to use for validation.
                        Has no effect if x_val and  y_val is supplied.
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        lang (str):            language.  Auto-detected if None.
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                If None, train/test split will be random.
        verbose (boolean): verbosity
    ```
    &#34;&#34;&#34;
    U.check_array(x_train,  y=y_train, X_name=&#39;x_train&#39;, y_name=&#39;y_train&#39;)

    if x_test is None or y_test is None:
        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, 
                                                            test_size=val_pct,
                                                            random_state=random_state)
    else:
        U.check_array(x_test,  y=y_test, X_name=&#39;x_test&#39;, y_name=&#39;y_test&#39;)


    # removed as TextPreprocessor now handles this.
    #if isinstance(y_train[0], str):
        #if not isinstance(y_test[0], str): 
            #raise ValueError(&#39;y_train contains strings, but y_test does not&#39;)
        #encoder = LabelEncoder()
        #encoder.fit(y_train)
        #y_train = encoder.transform(y_train)
        #y_test = encoder.transform(y_test)


    # detect language
    if lang is None: lang = TU.detect_lang(x_train)
    check_unsupported_lang(lang, preprocess_mode)

    # return preprocessed the texts
    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)
    if None: raise ValueError(&#39;unsupported preprocess_mode&#39;)
    preproc = preproc_type(maxlen,
                           max_features,
                           class_names = class_names,
                           lang=lang, ngram_range=ngram_range)
    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    if not preproc.get_classes() and verbose:
        print(&#39;task: text regression (supply class_names argument if this is supposed to be classification task)&#39;)
    else:
        print(&#39;task: text classification&#39;)
    return (trn, val, preproc)



def check_unsupported_lang(lang, preprocess_mode):
    &#34;&#34;&#34;
    ```
    check for unsupported language (e.g., nospace langs not supported by Jieba)
    ```
    &#34;&#34;&#34;
    unsupported = preprocess_mode==&#39;standard&#39; and TU.is_nospace_lang(lang) and not TU.is_chinese(lang)
    if unsupported:
        raise ValueError(&#39;language %s is currently only supported by the BERT model. &#39; % (lang) +
                         &#39;Please select preprocess_mode=&#34;bert&#34;&#39;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ktrain.text.data.check_unsupported_lang"><code class="name flex">
<span>def <span class="ident">check_unsupported_lang</span></span>(<span>lang, preprocess_mode)</span>
</code></dt>
<dd>
<div class="desc"><pre><code>check for unsupported language (e.g., nospace langs not supported by Jieba)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_unsupported_lang(lang, preprocess_mode):
    &#34;&#34;&#34;
    ```
    check for unsupported language (e.g., nospace langs not supported by Jieba)
    ```
    &#34;&#34;&#34;
    unsupported = preprocess_mode==&#39;standard&#39; and TU.is_nospace_lang(lang) and not TU.is_chinese(lang)
    if unsupported:
        raise ValueError(&#39;language %s is currently only supported by the BERT model. &#39; % (lang) +
                         &#39;Please select preprocess_mode=&#34;bert&#34;&#39;)</code></pre>
</details>
</dd>
<dt id="ktrain.text.data.texts_from_array"><code class="name flex">
<span>def <span class="ident">texts_from_array</span></span>(<span>x_train, y_train, x_test=None, y_test=None, class_names=[], max_features=20000, maxlen=400, val_pct=0.1, ngram_range=1, preprocess_mode='standard', lang=None, random_state=None, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><pre><code>Loads and preprocesses text data from arrays.
texts_from_array can handle data for both text classification
and text regression.  If class_names is empty, a regression task is assumed.
Args:
    x_train(list): list of training texts 
    y_train(list): labels in one of the following forms:
                   1. list of integers representing classes (class_names is required)
                   2. list of strings representing classes (class_names is not needed and ignored.)
                   3. a one or multi hot encoded array representing classes (class_names is required)
                   4. numerical values for text regresssion (class_names should be left empty)
    x_test(list): list of training texts 
    y_test(list): labels in one of the following forms:
                   1. list of integers representing classes (class_names is required)
                   2. list of strings representing classes (class_names is not needed and ignored.)
                   3. a one or multi hot encoded array representing classes (class_names is required)
                   4. numerical values for text regresssion (class_names should be left empty)
    class_names (list): list of strings representing class labels
                        shape should be (num_examples,1) or (num_examples,)
    max_features(int): max num of words to consider in vocabulary
                       Note: This is only used for preprocess_mode='standard'.
    maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
    ngram_range(int): size of multi-word phrases to consider
                      e.g., 2 will consider both 1-word phrases and 2-word phrases
                           limited by max_features
    val_pct(float): Proportion of training to use for validation.
                    Has no effect if x_val and  y_val is supplied.
    preprocess_mode (str):  Either 'standard' (normal tokenization) or one of {'bert', 'distilbert'}
                            tokenization and preprocessing for use with 
                            BERT/DistilBert text classification model.
    lang (str):            language.  Auto-detected if None.
    random_state(int):      If integer is supplied, train/test split is reproducible.
                            If None, train/test split will be random.
    verbose (boolean): verbosity
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def texts_from_array(x_train, y_train, x_test=None, y_test=None, 
                   class_names = [],
                   max_features=MAX_FEATURES, maxlen=MAXLEN, 
                   val_pct=0.1, ngram_range=1, preprocess_mode=&#39;standard&#39;, 
                   lang=None, # auto-detected
                   random_state=None,
                   verbose=1):
    &#34;&#34;&#34;
    ```
    Loads and preprocesses text data from arrays.
    texts_from_array can handle data for both text classification
    and text regression.  If class_names is empty, a regression task is assumed.
    Args:
        x_train(list): list of training texts 
        y_train(list): labels in one of the following forms:
                       1. list of integers representing classes (class_names is required)
                       2. list of strings representing classes (class_names is not needed and ignored.)
                       3. a one or multi hot encoded array representing classes (class_names is required)
                       4. numerical values for text regresssion (class_names should be left empty)
        x_test(list): list of training texts 
        y_test(list): labels in one of the following forms:
                       1. list of integers representing classes (class_names is required)
                       2. list of strings representing classes (class_names is not needed and ignored.)
                       3. a one or multi hot encoded array representing classes (class_names is required)
                       4. numerical values for text regresssion (class_names should be left empty)
        class_names (list): list of strings representing class labels
                            shape should be (num_examples,1) or (num_examples,)
        max_features(int): max num of words to consider in vocabulary
                           Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
        ngram_range(int): size of multi-word phrases to consider
                          e.g., 2 will consider both 1-word phrases and 2-word phrases
                               limited by max_features
        val_pct(float): Proportion of training to use for validation.
                        Has no effect if x_val and  y_val is supplied.
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        lang (str):            language.  Auto-detected if None.
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                If None, train/test split will be random.
        verbose (boolean): verbosity
    ```
    &#34;&#34;&#34;
    U.check_array(x_train,  y=y_train, X_name=&#39;x_train&#39;, y_name=&#39;y_train&#39;)

    if x_test is None or y_test is None:
        x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, 
                                                            test_size=val_pct,
                                                            random_state=random_state)
    else:
        U.check_array(x_test,  y=y_test, X_name=&#39;x_test&#39;, y_name=&#39;y_test&#39;)


    # removed as TextPreprocessor now handles this.
    #if isinstance(y_train[0], str):
        #if not isinstance(y_test[0], str): 
            #raise ValueError(&#39;y_train contains strings, but y_test does not&#39;)
        #encoder = LabelEncoder()
        #encoder.fit(y_train)
        #y_train = encoder.transform(y_train)
        #y_test = encoder.transform(y_test)


    # detect language
    if lang is None: lang = TU.detect_lang(x_train)
    check_unsupported_lang(lang, preprocess_mode)

    # return preprocessed the texts
    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)
    if None: raise ValueError(&#39;unsupported preprocess_mode&#39;)
    preproc = preproc_type(maxlen,
                           max_features,
                           class_names = class_names,
                           lang=lang, ngram_range=ngram_range)
    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    if not preproc.get_classes() and verbose:
        print(&#39;task: text regression (supply class_names argument if this is supposed to be classification task)&#39;)
    else:
        print(&#39;task: text classification&#39;)
    return (trn, val, preproc)</code></pre>
</details>
</dd>
<dt id="ktrain.text.data.texts_from_csv"><code class="name flex">
<span>def <span class="ident">texts_from_csv</span></span>(<span>train_filepath, text_column, label_columns=[], val_filepath=None, max_features=20000, maxlen=400, val_pct=0.1, ngram_range=1, preprocess_mode='standard', encoding=None, lang=None, sep=',', is_regression=False, random_state=None, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><pre><code>Loads text data from CSV or TSV file. Class labels are assumed to be
one of the following formats:
    1. one-hot-encoded or multi-hot-encoded arrays representing classes:
          Example with label_columns=['positive', 'negative'] and text_column='text':
            text|positive|negative
            I like this movie.|1|0
            I hated this movie.|0|1
        Classification will have a single one in each row: [[1,0,0], [0,1,0]]]
        Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]
    2. labels are in a single column of string or integer values representing classs labels
           Example with label_columns=['label'] and text_column='text':
             text|label
             I like this movie.|positive
             I hated this movie.|negative
   3. labels are a single column of numerical values for text regression
      NOTE: Must supply is_regression=True for labels to be treated as numerical targets
             wine_description|wine_price
             Exquisite wine!|100
             Wine for budget shoppers|8

Args:
    train_filepath(str): file path to training CSV
    text_column(str): name of column containing the text
    label_column(list): list of columns that are to be treated as labels
    val_filepath(string): file path to test CSV.  If not supplied,
                           10% of documents in training CSV will be
                           used for testing/validation.
    max_features(int): max num of words to consider in vocabulary
                       Note: This is only used for preprocess_mode='standard'.
    maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
    ngram_range(int): size of multi-word phrases to consider
                      e.g., 2 will consider both 1-word phrases and 2-word phrases
                           limited by max_features
    val_pct(float): Proportion of training to use for validation.
                    Has no effect if val_filepath is supplied.
    preprocess_mode (str):  Either 'standard' (normal tokenization) or one of {'bert', 'distilbert'}
                            tokenization and preprocessing for use with 
                            BERT/DistilBert text classification model.
    encoding (str):        character encoding to use. Auto-detected if None
    lang (str):            language.  Auto-detected if None.
    sep(str):              delimiter for CSV (comma is default)
    is_regression(bool):  If True, integer targets will be treated as numerical targets instead of class IDs
    random_state(int):      If integer is supplied, train/test split is reproducible.
                            If None, train/test split will be random
    verbose (boolean): verbosity
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def texts_from_csv(train_filepath, 
                   text_column,
                   label_columns = [],
                   val_filepath=None,
                   max_features=MAX_FEATURES, maxlen=MAXLEN, 
                   val_pct=0.1, ngram_range=1, preprocess_mode=&#39;standard&#39;, 
                   encoding=None,  # auto-detected
                   lang=None,      # auto-detected
                   sep=&#39;,&#39;, 
                   is_regression=False,
                   random_state=None,       
                   verbose=1):
    &#34;&#34;&#34;
    ```
    Loads text data from CSV or TSV file. Class labels are assumed to be
    one of the following formats:
        1. one-hot-encoded or multi-hot-encoded arrays representing classes:
              Example with label_columns=[&#39;positive&#39;, &#39;negative&#39;] and text_column=&#39;text&#39;:
                text|positive|negative
                I like this movie.|1|0
                I hated this movie.|0|1
            Classification will have a single one in each row: [[1,0,0], [0,1,0]]]
            Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]
        2. labels are in a single column of string or integer values representing classs labels
               Example with label_columns=[&#39;label&#39;] and text_column=&#39;text&#39;:
                 text|label
                 I like this movie.|positive
                 I hated this movie.|negative
       3. labels are a single column of numerical values for text regression
          NOTE: Must supply is_regression=True for labels to be treated as numerical targets
                 wine_description|wine_price
                 Exquisite wine!|100
                 Wine for budget shoppers|8

    Args:
        train_filepath(str): file path to training CSV
        text_column(str): name of column containing the text
        label_column(list): list of columns that are to be treated as labels
        val_filepath(string): file path to test CSV.  If not supplied,
                               10% of documents in training CSV will be
                               used for testing/validation.
        max_features(int): max num of words to consider in vocabulary
                           Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
        ngram_range(int): size of multi-word phrases to consider
                          e.g., 2 will consider both 1-word phrases and 2-word phrases
                               limited by max_features
        val_pct(float): Proportion of training to use for validation.
                        Has no effect if val_filepath is supplied.
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        encoding (str):        character encoding to use. Auto-detected if None
        lang (str):            language.  Auto-detected if None.
        sep(str):              delimiter for CSV (comma is default)
        is_regression(bool):  If True, integer targets will be treated as numerical targets instead of class IDs
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                If None, train/test split will be random
        verbose (boolean): verbosity
    ```
    &#34;&#34;&#34;
    if encoding is None:
        with open(train_filepath, &#39;rb&#39;) as f:
            #encoding = chardet.detect(f.read())[&#39;encoding&#39;]
            #encoding = &#39;utf-8&#39; if encoding.lower() in [&#39;ascii&#39;, &#39;utf8&#39;, &#39;utf-8&#39;] else encoding
            encoding = TU.detect_encoding(f.read())
            U.vprint(&#39;detected encoding: %s (if wrong, set manually)&#39; % (encoding), verbose=verbose)

    train_df = pd.read_csv(train_filepath, encoding=encoding,sep=sep)
    val_df = pd.read_csv(val_filepath, encoding=encoding,sep=sep) if val_filepath is not None else None
    return texts_from_df(train_df,
                         text_column,
                         label_columns=label_columns,
                         val_df = val_df,
                         max_features=max_features,
                         maxlen=maxlen,
                         val_pct=val_pct,
                         ngram_range=ngram_range, 
                         preprocess_mode=preprocess_mode,
                         lang=lang, is_regression=is_regression, random_state=random_state,
                         verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.data.texts_from_df"><code class="name flex">
<span>def <span class="ident">texts_from_df</span></span>(<span>train_df, text_column, label_columns=[], val_df=None, max_features=20000, maxlen=400, val_pct=0.1, ngram_range=1, preprocess_mode='standard', lang=None, is_regression=False, random_state=None, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><pre><code>Loads text data from Pandas dataframe file. Class labels are assumed to be
one of the following formats:
    1. one-hot-encoded or multi-hot-encoded arrays representing classes:
          Example with label_columns=['positive', 'negative'] and text_column='text':
            text|positive|negative
            I like this movie.|1|0
            I hated this movie.|0|1
        Classification will have a single one in each row: [[1,0,0], [0,1,0]]]
        Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]
    2. labels are in a single column of string or integer values representing class labels
           Example with label_columns=['label'] and text_column='text':
             text|label
             I like this movie.|positive
             I hated this movie.|negative
   3. labels are a single column of numerical values for text regression
      NOTE: Must supply is_regression=True for integer labels to be treated as numerical targets
             wine_description|wine_price
             Exquisite wine!|100
             Wine for budget shoppers|8

Args:
    train_df(dataframe): Pandas dataframe
    text_column(str): name of column containing the text
    label_columns(list): list of columns that are to be treated as labels
    val_df(dataframe): file path to test dataframe.  If not supplied,
                           10% of documents in training df will be
                           used for testing/validation.
    max_features(int): max num of words to consider in vocabulary.
                       Note: This is only used for preprocess_mode='standard'.
    maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
    ngram_range(int): size of multi-word phrases to consider
                      e.g., 2 will consider both 1-word phrases and 2-word phrases
                           limited by max_features
    val_pct(float): Proportion of training to use for validation.
                    Has no effect if val_filepath is supplied.
    preprocess_mode (str):  Either 'standard' (normal tokenization) or one of {'bert', 'distilbert'}
                            tokenization and preprocessing for use with 
                            BERT/DistilBert text classification model.
    lang (str):            language.  Auto-detected if None.
    is_regression(bool):  If True, integer targets will be treated as numerical targets instead of class IDs
    random_state(int):      If integer is supplied, train/test split is reproducible.
                            If None, train/test split will be random
    verbose (boolean): verbosity
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def texts_from_df(train_df, 
                   text_column,
                   label_columns = [],
                   val_df=None,
                   max_features=MAX_FEATURES, maxlen=MAXLEN, 
                   val_pct=0.1, ngram_range=1, preprocess_mode=&#39;standard&#39;, 
                   lang=None, # auto-detected
                   is_regression=False,
                   random_state=None,
                   verbose=1):
    &#34;&#34;&#34;
    ```
    Loads text data from Pandas dataframe file. Class labels are assumed to be
    one of the following formats:
        1. one-hot-encoded or multi-hot-encoded arrays representing classes:
              Example with label_columns=[&#39;positive&#39;, &#39;negative&#39;] and text_column=&#39;text&#39;:
                text|positive|negative
                I like this movie.|1|0
                I hated this movie.|0|1
            Classification will have a single one in each row: [[1,0,0], [0,1,0]]]
            Multi-label classification will have one more ones in each row: [[1,1,0], [0,1,1]]
        2. labels are in a single column of string or integer values representing class labels
               Example with label_columns=[&#39;label&#39;] and text_column=&#39;text&#39;:
                 text|label
                 I like this movie.|positive
                 I hated this movie.|negative
       3. labels are a single column of numerical values for text regression
          NOTE: Must supply is_regression=True for integer labels to be treated as numerical targets
                 wine_description|wine_price
                 Exquisite wine!|100
                 Wine for budget shoppers|8

    Args:
        train_df(dataframe): Pandas dataframe
        text_column(str): name of column containing the text
        label_columns(list): list of columns that are to be treated as labels
        val_df(dataframe): file path to test dataframe.  If not supplied,
                               10% of documents in training df will be
                               used for testing/validation.
        max_features(int): max num of words to consider in vocabulary.
                           Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen(int): each document can be of most &lt;maxlen&gt; words. 0 is used as padding ID.
        ngram_range(int): size of multi-word phrases to consider
                          e.g., 2 will consider both 1-word phrases and 2-word phrases
                               limited by max_features
        val_pct(float): Proportion of training to use for validation.
                        Has no effect if val_filepath is supplied.
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        lang (str):            language.  Auto-detected if None.
        is_regression(bool):  If True, integer targets will be treated as numerical targets instead of class IDs
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                If None, train/test split will be random
        verbose (boolean): verbosity
    ```
    &#34;&#34;&#34;

    # read in train and test data
    train_df = train_df.copy()
    train_df[text_column].fillna(&#39;fillna&#39;, inplace=True)
    if val_df is not None:
        val_df = val_df.copy()
        val_df[text_column].fillna(&#39;fillna&#39;, inplace=True)
    else:
        train_df, val_df = train_test_split(train_df, test_size=val_pct, random_state=random_state)

    # transform labels
    ytransdf = U.YTransformDataFrame(label_columns, is_regression=is_regression)
    t_df = ytransdf.apply_train(train_df)
    v_df = ytransdf.apply_test(val_df)
    class_names = ytransdf.get_classes()
    new_lab_cols = ytransdf.get_label_columns(squeeze=True)
    x_train = t_df[text_column].values
    y_train = t_df[new_lab_cols].values
    x_test = v_df[text_column].values
    y_test = v_df[new_lab_cols].values

    # detect language
    if lang is None: lang = TU.detect_lang(x_train)
    check_unsupported_lang(lang, preprocess_mode)


    # return preprocessed the texts
    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)
    if None: raise ValueError(&#39;unsupported preprocess_mode&#39;)
    preproc = preproc_type(maxlen,
                           max_features,
                           class_names = class_names,
                           lang=lang, ngram_range=ngram_range)
    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    # QUICKFIX for #314
    preproc.ytransform.le = ytransdf.le
    return (trn, val, preproc)</code></pre>
</details>
</dd>
<dt id="ktrain.text.data.texts_from_folder"><code class="name flex">
<span>def <span class="ident">texts_from_folder</span></span>(<span>datadir, classes=None, max_features=20000, maxlen=400, ngram_range=1, train_test_names=['train', 'test'], preprocess_mode='standard', encoding=None, lang=None, val_pct=0.1, random_state=None, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><pre><code>Returns corpus as sequence of word IDs.
Assumes corpus is in the following folder structure:
├── datadir
│   ├── train
│   │   ├── class0       # folder containing documents of class 0
│   │   ├── class1       # folder containing documents of class 1
│   │   ├── class2       # folder containing documents of class 2
│   │   └── classN       # folder containing documents of class N
│   └── test 
│       ├── class0       # folder containing documents of class 0
│       ├── class1       # folder containing documents of class 1
│       ├── class2       # folder containing documents of class 2
│       └── classN       # folder containing documents of class N

Each subfolder should contain documents in plain text format.
If train and test contain additional subfolders that do not represent
classes, they can be ignored by explicitly listing the subfolders of
interest using the classes argument.
Args:
    datadir (str): path to folder
    classes (list): list of classes (subfolders to consider).
                    This is simply supplied as the categories argument
                    to sklearn's load_files function.
    max_features (int):  maximum number of unigrams to consider
                         Note: This is only used for preprocess_mode='standard'.
    maxlen (int):  maximum length of tokens in document
    ngram_range (int):  If &gt; 1, will include 2=bigrams, 3=trigrams and bigrams
    train_test_names (list):  list of strings represnting the subfolder
                             name for train and validation sets
                             if test name is missing, &lt;val_pct&gt; of training
                             will be used for validation
    preprocess_mode (str):  Either 'standard' (normal tokenization) or one of {'bert', 'distilbert'}
                            tokenization and preprocessing for use with 
                            BERT/DistilBert text classification model.
    encoding (str):        character encoding to use. Auto-detected if None
    lang (str):            language.  Auto-detected if None.
    val_pct(float):        Onlyl used if train_test_names  has 1 and not 2 names
    random_state(int):      If integer is supplied, train/test split is reproducible.
                            IF None, train/test split will be random
    verbose (bool):         verbosity

</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def texts_from_folder(datadir, classes=None, 
                      max_features=MAX_FEATURES, maxlen=MAXLEN,
                      ngram_range=1,
                      train_test_names=[&#39;train&#39;, &#39;test&#39;],
                      preprocess_mode=&#39;standard&#39;,
                      encoding=None, # detected automatically
                      lang=None, # detected automatically
                      val_pct=0.1, random_state=None,
                      verbose=1):
    &#34;&#34;&#34;
    ```
    Returns corpus as sequence of word IDs.
    Assumes corpus is in the following folder structure:
    ├── datadir
    │   ├── train
    │   │   ├── class0       # folder containing documents of class 0
    │   │   ├── class1       # folder containing documents of class 1
    │   │   ├── class2       # folder containing documents of class 2
    │   │   └── classN       # folder containing documents of class N
    │   └── test 
    │       ├── class0       # folder containing documents of class 0
    │       ├── class1       # folder containing documents of class 1
    │       ├── class2       # folder containing documents of class 2
    │       └── classN       # folder containing documents of class N

    Each subfolder should contain documents in plain text format.
    If train and test contain additional subfolders that do not represent
    classes, they can be ignored by explicitly listing the subfolders of
    interest using the classes argument.
    Args:
        datadir (str): path to folder
        classes (list): list of classes (subfolders to consider).
                        This is simply supplied as the categories argument
                        to sklearn&#39;s load_files function.
        max_features (int):  maximum number of unigrams to consider
                             Note: This is only used for preprocess_mode=&#39;standard&#39;.
        maxlen (int):  maximum length of tokens in document
        ngram_range (int):  If &gt; 1, will include 2=bigrams, 3=trigrams and bigrams
        train_test_names (list):  list of strings represnting the subfolder
                                 name for train and validation sets
                                 if test name is missing, &lt;val_pct&gt; of training
                                 will be used for validation
        preprocess_mode (str):  Either &#39;standard&#39; (normal tokenization) or one of {&#39;bert&#39;, &#39;distilbert&#39;}
                                tokenization and preprocessing for use with 
                                BERT/DistilBert text classification model.
        encoding (str):        character encoding to use. Auto-detected if None
        lang (str):            language.  Auto-detected if None.
        val_pct(float):        Onlyl used if train_test_names  has 1 and not 2 names
        random_state(int):      If integer is supplied, train/test split is reproducible.
                                IF None, train/test split will be random
        verbose (bool):         verbosity
        
    ```
    &#34;&#34;&#34;

    # check train_test_names
    if len(train_test_names) &lt; 1 or len(train_test_names) &gt; 2:
        raise ValueError(&#39;train_test_names must have 1 or two elements for train and optionally validation&#39;)

    # read in training and test corpora
    train_str = train_test_names[0]
    train_b = load_files(os.path.join(datadir, train_str), shuffle=True, categories=classes)
    if len(train_test_names) &gt; 1:
        test_str = train_test_names[1]
        test_b = load_files(os.path.join(datadir,  test_str), shuffle=False, categories=classes)
        x_train = train_b.data
        y_train = train_b.target
        x_test = test_b.data
        y_test = test_b.target
    else:
        x_train, x_test, y_train, y_test = train_test_split(train_b.data, 
                                                            train_b.target, 
                                                            test_size=val_pct,
                                                            random_state=random_state)

    # decode based on supplied encoding
    if encoding is None:
        encoding = TU.detect_encoding(x_train)
        U.vprint(&#39;detected encoding: %s&#39; % (encoding), verbose=verbose)
    
    try:
        x_train = [x.decode(encoding) for x in x_train]
        x_test = [x.decode(encoding) for x in x_test]
    except:
        U.vprint(&#39;Decoding with %s failed 1st attempt - using %s with skips&#39; % (encoding, 
                                                                                encoding),
                                                                                verbose=verbose)
        x_train = TU.decode_by_line(x_train, encoding=encoding, verbose=verbose)
        x_test = TU.decode_by_line(x_test, encoding=encoding, verbose=verbose)


    # detect language
    if lang is None: lang = TU.detect_lang(x_train)
    check_unsupported_lang(lang, preprocess_mode)



    # return preprocessed the texts
    preproc_type = tpp.TEXT_PREPROCESSORS.get(preprocess_mode, None)
    if None: raise ValueError(&#39;unsupported preprocess_mode&#39;)
    preproc = preproc_type(maxlen,
                           max_features,
                           class_names = train_b.target_names,
                           lang=lang, ngram_range=ngram_range)
    trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    return (trn, val, preproc)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ktrain.text" href="index.html">ktrain.text</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ktrain.text.data.check_unsupported_lang" href="#ktrain.text.data.check_unsupported_lang">check_unsupported_lang</a></code></li>
<li><code><a title="ktrain.text.data.texts_from_array" href="#ktrain.text.data.texts_from_array">texts_from_array</a></code></li>
<li><code><a title="ktrain.text.data.texts_from_csv" href="#ktrain.text.data.texts_from_csv">texts_from_csv</a></code></li>
<li><code><a title="ktrain.text.data.texts_from_df" href="#ktrain.text.data.texts_from_df">texts_from_df</a></code></li>
<li><code><a title="ktrain.text.data.texts_from_folder" href="#ktrain.text.data.texts_from_folder">texts_from_folder</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>