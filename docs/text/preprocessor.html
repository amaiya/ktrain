<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ktrain.text.preprocessor API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ktrain.text.preprocessor</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ..imports import *
from .. import utils as U
from ..preprocessor import Preprocessor
from ..data import SequenceDataset
from . import textutils as TU

from transformers import AutoConfig, TFAutoModelForSequenceClassification, AutoTokenizer, TFAutoModel


DISTILBERT= &#39;distilbert&#39;

NOSPACE_LANGS = [&#39;zh-cn&#39;, &#39;zh-tw&#39;, &#39;ja&#39;]


def is_nospace_lang(lang):
    return lang in NOSPACE_LANGS


def fname_from_url(url):
    return os.path.split(url)[-1]


#------------------------------------------------------------------------------
# Word Vectors
#------------------------------------------------------------------------------
WV_URL = &#39;https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip&#39;
#WV_URL = &#39;http://nlp.stanford.edu/data/glove.6B.zip


def get_wv_path(wv_path_or_url=WV_URL):
    # process if file path given
    if os.path.isfile(wv_path_or_url) and wv_path_or_url.endswith(&#39;vec&#39;): return wv_path_or_url
    elif os.path.isfile(wv_path_or_url):
        raise ValueError(&#34;wv_path_or_url must either be URL .vec.zip or .vec.gz file or file path to .vec file&#34;)

    # process if URL is given
    fasttext_url = &#39;https://dl.fbaipublicfiles.com/fasttext&#39;
    if not wv_path_or_url.startswith(fasttext_url):
        raise ValueError(&#39;selected word vector file must be from %s&#39;% (fasttext_url))
    if not wv_path_or_url.endswith(&#39;.vec.zip&#39;) and not wv_path_or_url.endswith(&#39;vec.gz&#39;):
        raise ValueError(&#39;If wv_path_or_url is URL, must be .vec.zip filea from Facebook fasttext site.&#39;)

    ktrain_data = U.get_ktrain_data()
    zip_fpath = os.path.join(ktrain_data, fname_from_url(wv_path_or_url))
    wv_path =  os.path.join(ktrain_data, os.path.splitext(fname_from_url(wv_path_or_url))[0])
    if not os.path.isfile(wv_path):
        # download zip
        print(&#39;downloading pretrained word vectors to %s ...&#39; % (ktrain_data))
        U.download(wv_path_or_url, zip_fpath)

        # unzip
        print(&#39;\nextracting pretrained word vectors...&#39;)
        if wv_path_or_url.endswith(&#39;.vec.zip&#39;):
            with zipfile.ZipFile(zip_fpath, &#39;r&#39;) as zip_ref:
                zip_ref.extractall(ktrain_data)
        else: # .vec.gz
            with gzip.open(zip_fpath, &#39;rb&#39;) as f_in:
                with open(wv_path, &#39;wb&#39;) as f_out:
                    shutil.copyfileobj(f_in, f_out)
        print(&#39;done.\n&#39;)

        # cleanup
        print(&#39;cleanup downloaded zip...&#39;)
        try:
            os.remove(zip_fpath)
            print(&#39;done.\n&#39;)
        except OSError:
            print(&#39;failed to cleanup/remove %s&#39; % (zip_fpath))
    return wv_path


def get_coefs(word, *arr): return word, np.asarray(arr, dtype=&#39;float32&#39;)



#def load_wv(wv_path=None, verbose=1):
    #if verbose: print(&#39;Loading pretrained word vectors...this may take a few moments...&#39;)
    #if wv_path is None: wv_path = get_wv_path()
    #embeddings_index = dict(get_coefs(*o.rstrip().rsplit(&#39; &#39;)) for o in open(wv_path, encoding=&#39;utf-8&#39;))
    #if verbose: print(&#39;Done.&#39;)
    #return embeddings_index


def file_len(fname):
    with open(fname, encoding=&#39;utf-8&#39;) as f:
        for i, l in enumerate(f):
            pass
    return i + 1


def load_wv(wv_path_or_url=WV_URL, verbose=1):
    wv_path = get_wv_path(wv_path_or_url)
    if verbose: print(&#39;loading pretrained word vectors...this may take a few moments...&#39;)
    length = file_len(wv_path)
    tups = []
    mb = master_bar(range(1))
    for i in mb:
        f = open(wv_path, encoding=&#39;utf-8&#39;)
        for o in progress_bar(range(length), parent=mb):
            o = f.readline()
            tups.append(get_coefs(*o.rstrip().rsplit(&#39; &#39;)))
        f.close()
        #if verbose: mb.write(&#39;done.&#39;)
    return dict(tups)



#------------------------------------------------------------------------------
# BERT
#------------------------------------------------------------------------------

#BERT_PATH = os.path.join(os.path.dirname(os.path.abspath(localbert.__file__)), &#39;uncased_L-12_H-768_A-12&#39;)
BERT_URL = &#39;https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip&#39;
BERT_URL_MULTI = &#39;https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip&#39;
BERT_URL_CN = &#39;https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip&#39;

def get_bert_path(lang=&#39;en&#39;):
    if lang == &#39;en&#39;:
        bert_url = BERT_URL
    elif lang.startswith(&#39;zh-&#39;):
        bert_url = BERT_URL_CN
    else:
        bert_url = BERT_URL_MULTI
    ktrain_data = U.get_ktrain_data()
    zip_fpath = os.path.join(ktrain_data, fname_from_url(bert_url))
    bert_path = os.path.join( ktrain_data, os.path.splitext(fname_from_url(bert_url))[0] )
    if not os.path.isdir(bert_path) or \
       not os.path.isfile(os.path.join(bert_path, &#39;bert_config.json&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;bert_model.ckpt.data-00000-of-00001&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;bert_model.ckpt.index&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;bert_model.ckpt.meta&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;vocab.txt&#39;)):
        # download zip
        print(&#39;downloading pretrained BERT model (%s)...&#39; % (fname_from_url(bert_url)))
        U.download(bert_url, zip_fpath)

        # unzip
        print(&#39;\nextracting pretrained BERT model...&#39;)
        with zipfile.ZipFile(zip_fpath, &#39;r&#39;) as zip_ref:
            zip_ref.extractall(ktrain_data)
        print(&#39;done.\n&#39;)

        # cleanup
        print(&#39;cleanup downloaded zip...&#39;)
        try:
            os.remove(zip_fpath)
            print(&#39;done.\n&#39;)
        except OSError:
            print(&#39;failed to cleanup/remove %s&#39; % (zip_fpath))
    return bert_path



def bert_tokenize(docs, tokenizer, max_length, verbose=1):
    if verbose:
        mb = master_bar(range(1))
        pb = progress_bar(docs, parent=mb)
    else:
        mb = range(1)
        pb = docs


    indices = []
    for i in mb:
        for doc in pb:
            ids, segments = tokenizer.encode(doc, max_len=max_length)
            indices.append(ids)
        if verbose: mb.write(&#39;done.&#39;)
    zeros = np.zeros_like(indices)
    return [np.array(indices), np.array(zeros)]

#------------------------------------------------------------------------------
# Transformers UTILITIES
#------------------------------------------------------------------------------

#def convert_to_tfdataset(csv):
    #def gen():
        #for ex in csv:
            #yield  {&#39;idx&#39;: ex[0],
                     #&#39;sentence&#39;: ex[1],
                     #&#39;label&#39;: str(ex[2])}
    #return tf.data.Dataset.from_generator(gen,
        #{&#39;idx&#39;: tf.int64,
          #&#39;sentence&#39;: tf.string,
          #&#39;label&#39;: tf.int64})


#def features_to_tfdataset(features):

#    def gen():
#        for ex in features:
#            yield ({&#39;input_ids&#39;: ex.input_ids,
#                     &#39;attention_mask&#39;: ex.attention_mask,
#                     &#39;token_type_ids&#39;: ex.token_type_ids},
#                    ex.label)

#    return tf.data.Dataset.from_generator(gen,
#        ({&#39;input_ids&#39;: tf.int32,
#          &#39;attention_mask&#39;: tf.int32,
#          &#39;token_type_ids&#39;: tf.int32},
#         tf.int64),
#        ({&#39;input_ids&#39;: tf.TensorShape([None]),
#          &#39;attention_mask&#39;: tf.TensorShape([None]),
#          &#39;token_type_ids&#39;: tf.TensorShape([None])},
#         tf.TensorShape([None])))
#         #tf.TensorShape(])))

def _is_sentence_pair(tup):
    if isinstance(tup, (tuple)) and len(tup) == 2 and\
            isinstance(tup[0], str) and isinstance(tup[1], str):
        return True
    else:
        if isinstance(tup, (list, np.ndarray)) and len(tup) == 2 and\
                isinstance(tup[0], str) and isinstance(tup[1], str):
            warnings.warn(&#39;List or array of two texts supplied, so task being treated as text classification. &#39; +\
                          &#39;If this is a sentence pair classification task, please cast to tuple.&#39;)
        return False


def detect_text_format(texts):
    is_pair = False
    is_array = False
    err_msg = &#39;invalid text format: texts should be list of strings or list of sentence pairs in form of tuples (str, str)&#39;
    if _is_sentence_pair(texts):
        is_pair=True
        is_array = False
    elif isinstance(texts, (tuple, list, np.ndarray)):
        is_array = True
        if len(texts) == 0: raise ValueError(&#39;texts is empty&#39;)
        peek = texts[0]
        is_pair = _is_sentence_pair(peek)
        if not is_pair and not isinstance(peek, str):
            raise ValueError(err_msg)
    return is_array, is_pair



def hf_features_to_tfdataset(features_list, labels):
    features_list = np.array(features_list)
    labels = np.array(labels) if labels is not None else None
    tfdataset = tf.data.Dataset.from_tensor_slices((features_list, labels))
    tfdataset = tfdataset.map(lambda x,y: ({&#39;input_ids&#39;: x[0], 
                                            &#39;attention_mask&#39;: x[1], 
                                             &#39;token_type_ids&#39;: x[2]}, y))

    return tfdataset



def hf_convert_example(text_a, text_b=None, tokenizer=None,
                       max_length=512,
                       pad_on_left=False,
                       pad_token=0,
                       pad_token_segment_id=0,
                       mask_padding_with_zero=True):
    &#34;&#34;&#34;
    convert InputExample to InputFeature for Hugging Face transformer
    &#34;&#34;&#34;
    if tokenizer is None: raise ValueError(&#39;tokenizer is required&#39;)
    inputs = tokenizer.encode_plus(
        text_a,
        text_b,
        add_special_tokens=True,
        return_token_type_ids=True,
        max_length=max_length, 
        truncation=&#39;longest_first&#39;
    )
    input_ids, token_type_ids = inputs[&#34;input_ids&#34;], inputs[&#34;token_type_ids&#34;]

    # The mask has 1 for real tokens and 0 for padding tokens. Only real
    # tokens are attended to.
    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)

    # Zero-pad up to the sequence length.
    padding_length = max_length - len(input_ids)
    if pad_on_left:
        input_ids = ([pad_token] * padding_length) + input_ids
        attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask
        token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids
    else:
        input_ids = input_ids + ([pad_token] * padding_length)
        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)
        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)
    assert len(input_ids) == max_length, &#34;Error with input length {} vs {}&#34;.format(len(input_ids), max_length)
    assert len(attention_mask) == max_length, &#34;Error with input length {} vs {}&#34;.format(len(attention_mask), max_length)
    assert len(token_type_ids) == max_length, &#34;Error with input length {} vs {}&#34;.format(len(token_type_ids), max_length)


    #if ex_index &lt; 1:
        #print(&#34;*** Example ***&#34;)
        #print(&#34;guid: %s&#34; % (example.guid))
        #print(&#34;input_ids: %s&#34; % &#34; &#34;.join([str(x) for x in input_ids]))
        #print(&#34;attention_mask: %s&#34; % &#34; &#34;.join([str(x) for x in attention_mask]))
        #print(&#34;token_type_ids: %s&#34; % &#34; &#34;.join([str(x) for x in token_type_ids]))
        #print(&#34;label: %s (id = %d)&#34; % (example.label, label))

    return [input_ids, attention_mask, token_type_ids]




def hf_convert_examples(texts, y=None, tokenizer=None,
                        max_length=512,
                        pad_on_left=False,
                        pad_token=0,
                        pad_token_segment_id=0,
                        mask_padding_with_zero=True,
                        use_dynamic_shape=False,
                        verbose=1):
    &#34;&#34;&#34;
    Loads a data file into a list of ``InputFeatures``
    Args:
        texts: texts of documents or sentence pairs
        y:  labels for documents
        tokenizer: Instance of a tokenizer that will tokenize the examples
        max_length: Maximum example length
        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)
        pad_token: Padding token
        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)
        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values
            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for
            actual values)
        use_dynamic_shape(bool):  If True, supplied max_length will be ignored and will be computed
                                  based on provided texts instead.
        verbose(bool): verbosity
    Returns:
        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``
        containing the task-specific features. If the input is a list of ``InputExamples``, will return
        a list of task-specific ``InputFeatures`` which can be fed to the model.
    &#34;&#34;&#34;

    is_array, is_pair = detect_text_format(texts)

    if use_dynamic_shape:
        sentences = []
        for text in texts:
            if is_pair:
                text_a = text[0]
                text_b = text[1]
            else:
                text_a = text
                text_b = None
            sentences.append( tokenizer.convert_ids_to_tokens(tokenizer.encode(text_a, text_b)) )
            #sentences.append(tokenizer.tokenize(text_a, text_b)) # only works for Fast tokenizers
        maxlen = len(max([tokens for tokens in sentences], key=len,)) + 2

        if maxlen &lt; max_length: max_length = maxlen


    data = []
    features_list = []
    labels = []
    if verbose:
        mb = master_bar(range(1))
        pb = progress_bar(texts, parent=mb)
    else:
        mb = range(1)
        pb = texts
    for i in mb:
        #for (idx, text) in enumerate(progress_bar(texts, parent=mb)):
        for (idx, text) in enumerate(pb):
            if is_pair:
                text_a = text[0]
                text_b = text[1]
            else:
                text_a = text
                text_b = None
            features = hf_convert_example(text_a, text_b=text_b, tokenizer=tokenizer,
                                          max_length=max_length,
                                          pad_on_left=pad_on_left,
                                          pad_token=pad_token,
                                          pad_token_segment_id=pad_token_segment_id,
                                          mask_padding_with_zero=mask_padding_with_zero)
            features_list.append(features)
            labels.append(y[idx] if y is not None else None)
    #tfdataset = hf_features_to_tfdataset(features_list, labels)
    #return tfdataset
    #return (features_list, labels)
    # HF_EXCEPTION
    # due to issues in transormers library and TF2 tf.Datasets, arrays are converted
    # to iterators on-the-fly
    #return  TransformerSequence(np.array(features_list), np.array(labels))
    return  TransformerDataset(np.array(features_list), np.array(labels))


#------------------------------------------------------------------------------


class TextPreprocessor(Preprocessor):
    &#34;&#34;&#34;
    Text preprocessing base class
    &#34;&#34;&#34;

    def __init__(self, maxlen, class_names, lang=&#39;en&#39;, multilabel=None):

        self.set_classes(class_names) # converts to list of necessary
        self.maxlen = maxlen
        self.lang = lang
        self.multilabel = multilabel # currently, this is always initially set None until set by set_multilabel
        self.preprocess_train_called = False
        #self.label_encoder = None # only set if y is in string format
        self.ytransform = None
        self.c = self.c.tolist() if isinstance(self.c, np.ndarray) else self.c


    def migrate_classes(self, class_names, classes):
        # NOTE: this method transforms to np.ndarray to list.
        # If removed and &#34;if class_names&#34; is issued prior to set_classes(), an error will occur.
        class_names = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names
        classes = classes.tolist() if isinstance(classes, np.ndarray) else classes

        if not class_names and classes:
            class_names = classes
            warnings.warn(&#39;The class_names argument is replacing the classes argument. Please update your code.&#39;)
        return class_names


    def get_tokenizer(self):
        raise NotImplementedError(&#39;This method was not overridden in subclass&#39;)

    def check_trained(self):
        if not self.preprocess_train_called:
            warnings.warn(&#39;The method preprocess_train was never called. You can disable this warning by setting preprocess_train_called=True.&#39;)
            #raise Exception(&#39;preprocess_train must be called&#39;)


    def get_preprocessor(self):
        raise NotImplementedError


    def get_classes(self):
        return self.c


    def set_classes(self, class_names):
        self.c = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names


    def preprocess(self, texts):
        raise NotImplementedError


    def set_multilabel(self, data, mode, verbose=1):
        if mode == &#39;train&#39; and self.get_classes():
            original_multilabel = self.multilabel
            discovered_multilabel = U.is_multilabel(data)
            if original_multilabel is None:
                self.multilabel = discovered_multilabel
            elif original_multilabel is True and discovered_multilabel is False:
                warnings.warn(&#39;The multilabel=True argument was supplied, but labels do not indicate &#39;+\
                              &#39;a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.&#39;)
            elif original_multilabel is False and discovered_multilabel is True:
                warnings.warn(&#39;The multilabel=False argument was supplied, but labels inidcate that  &#39;+\
                              &#39;this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.&#39;)
            U.vprint(&#34;Is Multi-Label? %s&#34; % (self.multilabel), verbose=verbose)


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        raise NotImplementedError


    def is_chinese(self):
        return TU.is_chinese(self.lang)


    def is_nospace_lang(self):
        return TU.is_nospace_lang(self.lang)


    def process_chinese(self, texts, lang=None):
        #if lang is None: lang = langdetect.detect(texts[0])
        if lang is None: lang = TU.detect_lang(texts)
        if not TU.is_nospace_lang(lang): return texts
        return TU.split_chinese(texts)


    @classmethod
    def seqlen_stats(cls, list_of_texts):
        &#34;&#34;&#34;
        compute sequence length stats from
        list of texts in any spaces-segmented language
        Args:
            list_of_texts: list of strings
        Returns:
            dict: dictionary with keys: mean, 95percentile, 99percentile
        &#34;&#34;&#34;
        counts = []
        for text in list_of_texts:
            if isinstance(text, (list, np.ndarray)):
                lst = text
            else:
                lst = text.split()
            counts.append(len(lst))
        p95 = np.percentile(counts, 95)
        p99 = np.percentile(counts, 99)
        avg = sum(counts)/len(counts)
        return {&#39;mean&#39;:avg, &#39;95percentile&#39;: p95, &#39;99percentile&#39;:p99}


    def print_seqlen_stats(self, texts, mode, verbose=1):
        &#34;&#34;&#34;
        prints stats about sequence lengths
        &#34;&#34;&#34;
        if verbose and not self.is_nospace_lang():
            stat_dict = TextPreprocessor.seqlen_stats(texts)
            print( &#34;%s sequence lengths:&#34; % mode)
            for k in stat_dict:
                print(&#34;\t%s : %s&#34; % (k, int(round(stat_dict[k]))))


    def _transform_y(self, y_data, train=False, verbose=1):
        &#34;&#34;&#34;
        preprocess y
        If shape of y is 1, then task is considered classification if self.c exists
        or regression if not.
        &#34;&#34;&#34;
        if self.ytransform is None:
            self.ytransform = U.YTransform(class_names=self.get_classes())
        y = self.ytransform.apply(y_data, train=train)
        if train: self.c = self.ytransform.get_classes()
        return y


class StandardTextPreprocessor(TextPreprocessor):
    &#34;&#34;&#34;
    Standard text preprocessing
    &#34;&#34;&#34;

    def __init__(self, maxlen, max_features, class_names=[], classes=[], 
                 lang=&#39;en&#39;, ngram_range=1, multilabel=None):
        class_names = self.migrate_classes(class_names, classes)
        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)
        self.tok = None
        self.tok_dct = {}
        self.max_features = max_features
        self.ngram_range = ngram_range

    def get_tokenizer(self):
        return self.tok


    def __getstate__(self):
        return {k: v for k, v in self.__dict__.items()}


    def __setstate__(self, state):
        &#34;&#34;&#34;
        For backwards compatibility with pre-ytransform versions
        &#34;&#34;&#34;
        self.__dict__.update(state)
        if not hasattr(self, &#39;ytransform&#39;):
            le = self.label_encoder if hasattr(self, &#39;label_encoder&#39;) else None
            self.ytransform = U.YTransform(class_names=self.get_classes(), label_encoder=le)



    def get_preprocessor(self):
        return (self.tok, self.tok_dct)


    def preprocess(self, texts):
        return self.preprocess_test(texts, verbose=0)[0]


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        dct = self.tok.index_word
        return &#34; &#34;.join([dct[wid] for wid in doc if wid != 0 and wid in dct])


    def preprocess_train(self, train_text, y_train, verbose=1):
        &#34;&#34;&#34;
        preprocess training set
        &#34;&#34;&#34;
        if self.lang is None: self.lang = TU.detect_lang(train_text)


        U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

        # special processing if Chinese
        train_text = self.process_chinese(train_text, lang=self.lang)

        # extract vocabulary
        self.tok = Tokenizer(num_words=self.max_features)
        self.tok.fit_on_texts(train_text)
        U.vprint(&#39;Word Counts: {}&#39;.format(len(self.tok.word_counts)), verbose=verbose)
        U.vprint(&#39;Nrows: {}&#39;.format(len(train_text)), verbose=verbose)

        # convert to word IDs
        x_train = self.tok.texts_to_sequences(train_text)
        U.vprint(&#39;{} train sequences&#39;.format(len(x_train)), verbose=verbose)
        self.print_seqlen_stats(x_train, &#39;train&#39;, verbose=verbose)

        # add ngrams
        x_train = self._fit_ngrams(x_train, verbose=verbose)

        # pad sequences
        x_train = sequence.pad_sequences(x_train, maxlen=self.maxlen)
        U.vprint(&#39;x_train shape: ({},{})&#39;.format(x_train.shape[0], x_train.shape[1]), verbose=verbose)

        # transform y
        y_train = self._transform_y(y_train, train=True, verbose=verbose)
        if y_train is not None and verbose:
            print(&#39;y_train shape: %s&#39; % (y_train.shape,))

        # return
        result =  (x_train, y_train)
        self.set_multilabel(result, &#39;train&#39;)
        self.preprocess_train_called = True
        return result


    def preprocess_test(self, test_text, y_test=None, verbose=1):
        &#34;&#34;&#34;
        preprocess validation or test dataset
        &#34;&#34;&#34;
        self.check_trained()
        if self.tok is None or self.lang is None:
            raise Exception(&#39;Unfitted tokenizer or missing language. Did you run preprocess_train first?&#39;)

        # check for and process chinese
        test_text = self.process_chinese(test_text, self.lang)

        # convert to word IDs
        x_test = self.tok.texts_to_sequences(test_text)
        U.vprint(&#39;{} test sequences&#39;.format(len(x_test)), verbose=verbose)
        self.print_seqlen_stats(x_test, &#39;test&#39;, verbose=verbose)

        # add n-grams
        x_test = self._add_ngrams(x_test, mode=&#39;test&#39;, verbose=verbose)


        # pad sequences
        x_test = sequence.pad_sequences(x_test, maxlen=self.maxlen)
        U.vprint(&#39;x_test shape: ({},{})&#39;.format(x_test.shape[0], x_test.shape[1]), verbose=verbose)

        # transform y
        y_test = self._transform_y(y_test, train=False, verbose=verbose)
        if y_test is not None and verbose:
            print(&#39;y_test shape: %s&#39; % (y_test.shape,))


        # return
        return (x_test, y_test)



    def _fit_ngrams(self, x_train, verbose=1):
        self.tok_dct = {}
        if self.ngram_range &lt; 2: return x_train
        U.vprint(&#39;Adding {}-gram features&#39;.format(self.ngram_range), verbose=verbose)
        # Create set of unique n-gram from the training set.
        ngram_set = set()
        for input_list in x_train:
            for i in range(2, self.ngram_range + 1):
                set_of_ngram = self._create_ngram_set(input_list, ngram_value=i)
                ngram_set.update(set_of_ngram)

        # Dictionary mapping n-gram token to a unique integer.
        # Integer values are greater than max_features in order
        # to avoid collision with existing features.
        start_index = self.max_features + 1
        token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}
        indice_token = {token_indice[k]: k for k in token_indice}
        self.tok_dct = token_indice

        # max_features is the highest integer that could be found in the dataset.
        self.max_features = np.max(list(indice_token.keys())) + 1
        U.vprint(&#39;max_features changed to %s with addition of ngrams&#39; % (self.max_features), verbose=verbose)

        # Augmenting x_train with n-grams features
        x_train = self._add_ngrams(x_train, verbose=verbose, mode=&#39;train&#39;)
        return x_train


    def _add_ngrams(self, sequences, verbose=1, mode=&#39;test&#39;):
        &#34;&#34;&#34;
        Augment the input list of list (sequences) by appending n-grams values.
        Example: adding bi-gram
        &#34;&#34;&#34;
        token_indice = self.tok_dct
        if self.ngram_range &lt; 2: return sequences
        new_sequences = []
        for input_list in sequences:
            new_list = input_list[:]
            for ngram_value in range(2, self.ngram_range + 1):
                for i in range(len(new_list) - ngram_value + 1):
                    ngram = tuple(new_list[i:i + ngram_value])
                    if ngram in token_indice:
                        new_list.append(token_indice[ngram])
            new_sequences.append(new_list)
        U.vprint(&#39;Average {} sequence length with ngrams: {}&#39;.format(mode,
            np.mean(list(map(len, new_sequences)), dtype=int)), verbose=verbose)    
        self.print_seqlen_stats(new_sequences, &#39;%s (w/ngrams)&#39; % mode, verbose=verbose)
        return new_sequences



    def _create_ngram_set(self, input_list, ngram_value=2):
        &#34;&#34;&#34;
        Extract a set of n-grams from a list of integers.
        &gt;&gt;&gt; create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)
        {(4, 9), (4, 1), (1, 4), (9, 4)}
        &gt;&gt;&gt; create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)
        [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]
        &#34;&#34;&#34;
        return set(zip(*[input_list[i:] for i in range(ngram_value)]))


    def ngram_count(self):
        if not self.tok_dct: return 1
        s = set()
        for k in self.tok_dct.keys():
            s.add(len(k))
        return max(list(s))


class BERTPreprocessor(TextPreprocessor):
    &#34;&#34;&#34;
    text preprocessing for BERT model
    &#34;&#34;&#34;

    def __init__(self, maxlen, max_features, class_names=[], classes=[], 
                lang=&#39;en&#39;, ngram_range=1, multilabel=None):
        class_names = self.migrate_classes(class_names, classes)


        if maxlen &gt; 512: raise ValueError(&#39;BERT only supports maxlen &lt;= 512&#39;)

        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)
        vocab_path = os.path.join(get_bert_path(lang=lang), &#39;vocab.txt&#39;)
        token_dict = {}
        with codecs.open(vocab_path, &#39;r&#39;, &#39;utf8&#39;) as reader:
            for line in reader:
                token = line.strip()
                token_dict[token] = len(token_dict)
        tokenizer = BERT_Tokenizer(token_dict)
        self.tok = tokenizer
        self.tok_dct = dict((v,k) for k,v in token_dict.items())
        self.max_features = max_features # ignored
        self.ngram_range = 1 # ignored


    def get_tokenizer(self):
        return self.tok


    def __getstate__(self):
        return {k: v for k, v in self.__dict__.items()}


    def __setstate__(self, state):
        &#34;&#34;&#34;
        For backwards compatibility with pre-ytransform versions
        &#34;&#34;&#34;
        self.__dict__.update(state)
        if not hasattr(self, &#39;ytransform&#39;):
            le = self.label_encoder if hasattr(self, &#39;label_encoder&#39;) else None
            self.ytransform = U.YTransform(class_names=self.get_classes(), label_encoder=le)


    def get_preprocessor(self):
        return (self.tok, self.tok_dct)



    def preprocess(self, texts):
        return self.preprocess_test(texts, verbose=0)[0]


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        dct = self.tok_dct
        return &#34; &#34;.join([dct[wid] for wid in doc if wid != 0 and wid in dct])


    def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
        &#34;&#34;&#34;
        preprocess training set
        &#34;&#34;&#34;
        if mode == &#39;train&#39; and y is None:
            raise ValueError(&#39;y is required when mode=train&#39;)
        if self.lang is None and mode==&#39;train&#39;: self.lang = TU.detect_lang(texts)
        U.vprint(&#39;preprocessing %s...&#39; % (mode), verbose=verbose)
        U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

        x = bert_tokenize(texts, self.tok, self.maxlen, verbose=verbose)

        # transform y
        y = self._transform_y(y, train=mode==&#39;train&#39;, verbose=verbose)
        result = (x, y)
        self.set_multilabel(result, mode)
        if mode == &#39;train&#39;: self.preprocess_train_called = True
        return result



    def preprocess_test(self, texts, y=None, mode=&#39;test&#39;, verbose=1):
        self.check_trained()
        return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)


class TransformersPreprocessor(TextPreprocessor):
    &#34;&#34;&#34;
    text preprocessing for Hugging Face Transformer models
    &#34;&#34;&#34;

    def __init__(self,  model_name,
                maxlen, max_features, class_names=[], classes=[], 
                lang=&#39;en&#39;, ngram_range=1, multilabel=None):
        class_names = self.migrate_classes(class_names, classes)

        if maxlen &gt; 512: raise ValueError(&#39;Transformer models only supports maxlen &lt;= 512&#39;)

        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)

        self.model_name = model_name
        self.name = model_name.split(&#39;-&#39;)[0]
        if model_name.startswith(&#39;xlm-roberta&#39;): 
            self.name = &#39;xlm_roberta&#39;
            self.model_name = &#39;jplu/tf-&#39; + self.model_name
        else:
            self.name = model_name.split(&#39;-&#39;)[0]
        self.config = AutoConfig.from_pretrained(model_name)
        self.model_type = TFAutoModelForSequenceClassification
        self.tokenizer_type = AutoTokenizer

        if &#34;bert-base-japanese&#34; in model_name:
            self.tokenizer_type = transformers.BertJapaneseTokenizer

        # NOTE: As of v0.16.1, do not unnecessarily instantiate tokenizer
        # as it will be saved/pickled along with Preprocessor, which causes
        # problems for some community-uploaded models like bert-base-japanse-whole-word-masking.
        #tokenizer = self.tokenizer_type.from_pretrained(model_name)
        #self.tok = tokenizer
        self.tok = None # not pickled,  see __getstate__ 

        self.tok_dct = None
        self.max_features = max_features # ignored
        self.ngram_range = 1 # ignored


    def __getstate__(self):
        return {k: v for k, v in self.__dict__.items() if k not in [&#39;tok&#39;]}


    def __setstate__(self, state):
        &#34;&#34;&#34;
        For backwards compatibility with previous versions of ktrain
        that saved tokenizer and did not use ytransform
        &#34;&#34;&#34;
        self.__dict__.update(state)
        if not hasattr(self, &#39;tok&#39;): self.tok = None
        if not hasattr(self, &#39;ytransform&#39;): 
            le = self.label_encoder if hasattr(self, &#39;label_encoder&#39;) else None
            self.ytransform = U.YTransform(class_names=self.get_classes(), label_encoder=le)

    def set_config(self, config):
        self.config = config

    def get_config(self):
        return self.config

    def set_tokenizer(self, tokenizer):
        self.tok = tokenizer

    def get_tokenizer(self, fpath=None):
        model_name = self.model_name if fpath is None else fpath
        if self.tok is None:
            try:
                # use fast tokenizer if possible
                if self.name == &#39;bert&#39; and &#39;japanese&#39; not in model_name:
                    from transformers import BertTokenizerFast
                    self.tok = BertTokenizerFast.from_pretrained(model_name)
                elif self.name == &#39;distilbert&#39;:
                    from transformers import DistilBertTokenizerFast
                    self.tok = DistilBertTokenizerFast.from_pretrained(model_name)
                elif self.name == &#39;roberta&#39;:
                    from transformers import RobertaTokenizerFast
                    self.tok = RobertaTokenizerFast.from_pretrained(model_name)
                else:
                    self.tok = self.tokenizer_type.from_pretrained(model_name)
            except:
                error_msg = f&#34;Could not load tokenizer from model_name: {model_name}. &#34; +\
                            f&#34;If {model_name} is a local path, please make sure it exists and contains tokenizer files from Hugging Face. &#34; +\
                            f&#34;You can also reset model_name with preproc.model_name = &#39;/your/new/path&#39;.&#34;
                raise ValueError(error_msg)
        return self.tok


    def save_tokenizer(self, fpath):
        if os.path.isfile(fpath):
            raise ValueError(f&#39;There is an existing file named {fpath}. &#39; +\
                              &#39;Please use dfferent value for fpath.&#39;)
        elif os.path.exists(fpath):
            pass
        elif not os.path.exists(fpath):
            os.makedirs(fpath)
        tok =self.get_tokenizer()
        tok.save_pretrained(fpath)
        return



    def get_preprocessor(self):
        return (self.get_tokenizer(), self.tok_dct)



    def preprocess(self, texts):
        tseq = self.preprocess_test(texts, verbose=0)
        return tseq.to_tfdataset(train=False)


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        tok, _ = self.get_preprocessor()
        return self.tok.convert_ids_to_tokens(doc)
        #raise Exception(&#39;currently_unsupported: Transformers.Preprocessor.undo is not yet supported&#39;)


    def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
        &#34;&#34;&#34;
        preprocess training set
        &#34;&#34;&#34;

        U.vprint(&#39;preprocessing %s...&#39; % (mode), verbose=verbose)
        U.check_array(texts, y=y, X_name=&#39;texts&#39;)

        # detect sentence pairs
        is_array, is_pair = detect_text_format(texts)
        if not is_array: raise ValueError(&#39;texts must be a list of strings or a list of sentence pairs&#39;)

        # detect language
        if self.lang is None and mode==&#39;train&#39;: self.lang = TU.detect_lang(texts)
        U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

        # print stats
        if not is_pair: self.print_seqlen_stats(texts, mode, verbose=verbose)
        if is_pair: U.vprint(&#39;sentence pairs detected&#39;, verbose=verbose)

        # transform y
        if y is None and mode == &#39;train&#39;:
            raise ValueError(&#39;y is required for training sets&#39;)
        elif y is None:
            y = np.array([1] * len(texts))
        y = self._transform_y(y, train=mode==&#39;train&#39;, verbose=verbose)

        # convert examples
        tok, _ = self.get_preprocessor()
        dataset = hf_convert_examples(texts, y=y, tokenizer=tok, max_length=self.maxlen,
                                      pad_on_left=bool(self.name in [&#39;xlnet&#39;]),
                                      pad_token=tok.convert_tokens_to_ids([tok.pad_token][0]),
                                      pad_token_segment_id=4 if self.name in [&#39;xlnet&#39;] else 0,
                                      use_dynamic_shape=False if mode == &#39;train&#39; else True,
                                      verbose=verbose)
        self.set_multilabel(dataset, mode, verbose=verbose)
        if mode == &#39;train&#39;:  self.preprocess_train_called = True
        return dataset



    def preprocess_test(self, texts, y=None, mode=&#39;test&#39;, verbose=1):
        self.check_trained()
        return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)


    @classmethod
    def load_model_and_configure_from_data(cls, fpath, transformer_ds):
        &#34;&#34;&#34;
        loads model from file path and configures loss function and metrics automatically
        based on inspecting data
        Args:
          fpath(str): path to model folder
          transformer_ds(TransformerDataset): an instance of TransformerDataset
        &#34;&#34;&#34;
        is_regression = U.is_regression_from_data(transformer_ds)
        multilabel = U.is_multilabel(transformer_ds)
        model = TFAutoModelForSequenceClassification.from_pretrained(fpath)
        if is_regression:
            metrics = [&#39;mae&#39;]
            loss_fn = &#39;mse&#39;
        else:
            metrics = [&#39;accuracy&#39;]
            if multilabel:
                loss_fn =  keras.losses.BinaryCrossentropy(from_logits=True)
            else:
                loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)
        model.compile(loss=loss_fn,
                      optimizer=U.DEFAULT_OPT,
                      metrics=metrics)
        return model



    def _load_pretrained(self, mname, num_labels):
        &#34;&#34;&#34;
        load pretrained model
        &#34;&#34;&#34;
        if self.config is not None:
            self.config.num_labels = num_labels
            try:
                model = self.model_type.from_pretrained(mname, config=self.config)
            except:
                warnings.warn(&#39;Could not find Tensorflow version of model.  Attempting to download/load PyTorch version as TensorFlow model using from_pt=True. &#39; +\
                              &#39;You will need PyTorch installed for this.&#39;)
                try:
                    model = self.model_type.from_pretrained(mname, config=self.config, from_pt=True)
                except:
                    # load model as normal to expose error to user
                    model = self.model_type.from_pretrained(mname, config=self.config)
                    #raise ValueError(&#39;could not load pretrained model %s using both from_pt=False and from_pt=True&#39; % (mname))
        else:
            model = self.model_type.from_pretrained(mname, num_labels=num_labels)
        return model



    def get_classifier(self, fpath=None, multilabel=None, metrics=[&#39;accuracy&#39;]):
        &#34;&#34;&#34;
        creates a model for text classification
        Args:
          fpath(str): optional path to saved pretrained model. Typically left as None.
          multilabel(bool): If None, multilabel status is discovered from data [recommended].
                            If True, model will be forcibly configured for multilabel task.
                            If False, model will be forcibly configured for non-multilabel task.
                            It is recommended to leave this as None.
          metrics(list): metrics to use
        &#34;&#34;&#34;
        self.check_trained()
        if not self.get_classes():
            warnings.warn(&#39;no class labels were provided - treating as regression&#39;)
            return self.get_regression_model()

        # process multilabel task
        multilabel = self.multilabel if multilabel is None else multilabel
        if multilabel is True and self.multilabel is False:
            warnings.warn(&#39;The multilabel=True argument was supplied, but labels do not indicate &#39;+\
                          &#39;a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.&#39;)
        elif multilabel is False and self.multilabel is True:
                warnings.warn(&#39;The multilabel=False argument was supplied, but labels inidcate that  &#39;+\
                              &#39;this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.&#39;)

        # setup model
        num_labels = len(self.get_classes())
        mname = fpath if fpath is not None else self.model_name
        model = self._load_pretrained(mname, num_labels)
        if multilabel:
            loss_fn =  keras.losses.BinaryCrossentropy(from_logits=True)
        else:
            loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)
        model.compile(loss=loss_fn,
                      optimizer=U.DEFAULT_OPT,
                      metrics=metrics)
        return model


    def get_regression_model(self, fpath=None, metrics=[&#39;mae&#39;]):
        &#34;&#34;&#34;
        creates a model for text regression
        Args:
          fpath(str): optional path to saved pretrained model. Typically left as None.
          metrics(list): metrics to use
        &#34;&#34;&#34;
        self.check_trained()
        if self.get_classes():
            warnings.warn(&#39;class labels were provided - treating as classification problem&#39;)
            return self.get_classifier()
        num_labels = 1
        mname = fpath if fpath is not None else self.model_name
        model = self._load_pretrained(mname, num_labels)
        loss_fn = &#39;mse&#39;
        model.compile(loss=loss_fn,
                      optimizer=U.DEFAULT_OPT,
                      metrics=metrics)
        return model


    def get_model(self, fpath=None):
        self.check_trained()
        if not self.get_classes():
            return self.get_regression_model(fpath=fpath)
        else:
            return self.get_classifier(fpath=fpath)



class DistilBertPreprocessor(TransformersPreprocessor):
    &#34;&#34;&#34;
    text preprocessing for Hugging Face DistlBert model
    &#34;&#34;&#34;

    def __init__(self, maxlen, max_features, class_names=[], classes=[], 
                lang=&#39;en&#39;, ngram_range=1):
        class_names = self.migrate_classes(class_names, classes)
        name = DISTILBERT
        if lang == &#39;en&#39;:
            model_name = &#39;distilbert-base-uncased&#39;
        else:
            model_name = &#39;distilbert-base-multilingual-cased&#39;

        super().__init__(model_name,
                         maxlen, max_features, class_names=class_names, 
                         lang=lang, ngram_range=ngram_range)


class Transformer(TransformersPreprocessor):
    &#34;&#34;&#34;
    convenience class for text classification Hugging Face transformers 
    Usage:
       t = Transformer(&#39;distilbert-base-uncased&#39;, maxlen=128, classes=[&#39;neg&#39;, &#39;pos&#39;], batch_size=16)
       train_dataset = t.preprocess_train(train_texts, train_labels)
       model = t.get_classifier()
       model.fit(train_dataset)
    &#34;&#34;&#34;

    def __init__(self, model_name, maxlen=128, class_names=[], classes=[],
                 batch_size=None, use_with_learner=True):
        &#34;&#34;&#34;
        Args:
            model_name (str):  name of Hugging Face pretrained model
            maxlen (int):  sequence length
            class_names(list):  list of strings of class names (e.g., &#39;positive&#39;, &#39;negative&#39;).
                                The index position of string is the class ID.
                                Not required for:
                                  - regression problems
                                  - binary/multi classification problems where
                                    labels in y_train/y_test are in string format.
                                    In this case, classes will be populated automatically.
                                    get_classes() can be called to view discovered class labels.
                                The class_names argument replaces the old classes argument.
            classes(list):  alias for class_names.  Included for backwards-compatiblity.

            use_with_learner(bool):  If False, preprocess_train and preprocess_test
                                     will return tf.Datasets for direct use with model.fit
                                     in tf.Keras.
                                     If True, preprocess_train and preprocess_test will
                                     return a ktrain TransformerDataset object for use with
                                     ktrain.get_learner.
            batch_size (int): batch_size - only required if use_with_learner=False




        &#34;&#34;&#34;
        multilabel = None # force discovery of multilabel task from data in preprocess_train-&gt;set_multilabel
        class_names = self.migrate_classes(class_names, classes)
        if not use_with_learner and batch_size is None:
            raise ValueError(&#39;batch_size is required when use_with_learner=False&#39;)
        if multilabel and (class_names is None or not class_names):
            raise ValueError(&#39;classes argument is required when multilabel=True&#39;)
        super().__init__(model_name,
                         maxlen, max_features=10000, class_names=class_names, multilabel=multilabel)
        self.batch_size = batch_size
        self.use_with_learner = use_with_learner
        self.lang = None


    def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
        &#34;&#34;&#34;
        Preprocess training set for A Transformer model

        Y values can be in one of the following forms:
        1) integers representing the class (index into array returned by get_classes)
           for binary and multiclass text classification.
           If labels are integers, class_names argument to Transformer constructor is required.
        2) strings representing the class (e.g., &#39;negative&#39;, &#39;positive&#39;).
           If labels are strings, class_names argument to Transformer constructor is ignored,
           as class labels will be extracted from y.
        3) multi-hot-encoded vector for multilabel text classification problems
           If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
        4) Numerical values for regression problems.
           &lt;class_names&gt; argument to Transformer constructor should NOT be supplied

        Args:
            texts (list of strings): text of documents
            y: labels
            mode (str):  If &#39;train&#39; and prepare_for_learner=False,
                         a tf.Dataset will be returned with repeat enabled
                         for training with fit_generator
            verbose(bool): verbosity
        Returns:
          TransformerDataset if self.use_with_learner = True else tf.Dataset
        &#34;&#34;&#34;
        tseq = super().preprocess_train(texts, y=y, mode=mode, verbose=verbose)
        if self.use_with_learner: return tseq
        tseq.batch_size = self.batch_size
        train = (mode == &#39;train&#39;)
        return tseq.to_tfdataset(train=train)


    def preprocess_test(self, texts, y=None,  verbose=1):
        &#34;&#34;&#34;
        Preprocess the validation or test set for a Transformer model
        Y values can be in one of the following forms:
        1) integers representing the class (index into array returned by get_classes)
           for binary and multiclass text classification.
           If labels are integers, class_names argument to Transformer constructor is required.
        2) strings representing the class (e.g., &#39;negative&#39;, &#39;positive&#39;).
           If labels are strings, class_names argument to Transformer constructor is ignored,
           as class labels will be extracted from y.
        3) multi-hot-encoded vector for multilabel text classification problems
           If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
        4) Numerical values for regression problems.
           &lt;class_names&gt; argument to Transformer constructor should NOT be supplied

        Args:
            texts (list of strings): text of documents
            y: labels
            verbose(bool): verbosity
        Returns:
            TransformerDataset if self.use_with_learner = True else tf.Dataset
        &#34;&#34;&#34;
        self.check_trained()
        return self.preprocess_train(texts, y=y, mode=&#39;test&#39;, verbose=verbose)


class TransformerEmbedding():
    def __init__(self, model_name, layers=U.DEFAULT_TRANSFORMER_LAYERS):
        &#34;&#34;&#34;
        Args:
            model_name (str):  name of Hugging Face pretrained model.
                               Choose from here: https://huggingface.co/transformers/pretrained_models.html
            layers(list): list of indexes indicating which hidden layers to use when
                          constructing the embedding (e.g., last=[-1])
                               
        &#34;&#34;&#34;
        self.layers = layers
        self.model_name = model_name
        if model_name.startswith(&#39;xlm-roberta&#39;):
            self.name = &#39;xlm_roberta&#39;
        else:
            self.name = model_name.split(&#39;-&#39;)[0]

        self.config = AutoConfig.from_pretrained(model_name)
        self.model_type = TFAutoModel
        self.tokenizer_type = AutoTokenizer

        if &#34;bert-base-japanese&#34; in model_name:
            self.tokenizer_type = transformers.BertJapaneseTokenizer

        self.tokenizer = self.tokenizer_type.from_pretrained(model_name)
        self.model = self._load_pretrained(model_name)
        try:
            self.embsize = self.embed(&#39;ktrain&#39;, word_level=False).shape[1] # (batch_size, embsize)
        except:
            warnings.warn(&#39;could not determine Embedding size&#39;)
        if type(self.model).__name__ not in [&#39;TFBertModel&#39;, &#39;TFDistilBertModel&#39;, &#39;TFAlbertModel&#39;]:
            raise ValueError(&#39;TransformerEmbedding class currently only supports BERT-style models: &#39; +\
                             &#39;Bert, DistilBert, and Albert and variants like BioBERT and SciBERT\n\n&#39; +\
                             &#39;model received: %s (%s))&#39; % (type(self.model).__name__, model_name))


    def _load_pretrained(self, model_name):
        &#34;&#34;&#34;
        load pretrained model
        &#34;&#34;&#34;
        if self.config is not None:
            self.config.output_hidden_states = True
            try:
                model = self.model_type.from_pretrained(model_name, config=self.config)
            except:
                warnings.warn(&#39;Could not find Tensorflow version of model.  Attempting to download/load PyTorch version as TensorFlow model using from_pt=True. &#39; +\
                              &#39;You will need PyTorch installed for this.&#39;)
                try:
                    model = self.model_type.from_pretrained(model_name, config=self.config, from_pt=True)
                except:
                    raise ValueError(&#39;could not load pretrained model %s using both from_pt=False and from_pt=True&#39; % (model_name))
        else:
            model = self.model_type.from_pretrained(model_name, output_hidden_states=True)
        return model



    def embed(self, texts, word_level=True, max_length=512):
        &#34;&#34;&#34;
        get embedding for word, phrase, or sentence
        Args:
          text(str|list): word, phrase, or sentence or list of them representing a batch
          word_level(bool): If True, returns embedding for each token in supplied texts.
                            If False, returns embedding for each text in texts
          max_length(int): max length of tokens
        Returns:
            np.ndarray : embeddings
        &#34;&#34;&#34;
        if isinstance(texts, str): texts = [texts]
        if not isinstance(texts[0], str): texts = [&#34; &#34;.join(text) for text in texts]

        sentences = []
        for text in texts:
            sentences.append(self.tokenizer.tokenize(text))
        maxlen = len(max([tokens for tokens in sentences], key=len,)) + 2
        if max_length is not None and maxlen &gt; max_length: maxlen = max_length # added due to issue #270
        sentences = []

        all_input_ids = []
        all_input_masks = []
        for text in texts:
            tokens = self.tokenizer.tokenize(text)
            if len(tokens) &gt; maxlen - 2:
                tokens = tokens[0 : (maxlen - 2)]
            sentences.append(tokens)
            tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]
            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
            input_mask = [1] * len(input_ids)
            while len(input_ids) &lt; maxlen:
                input_ids.append(0)
                input_mask.append(0)
            all_input_ids.append(input_ids)
            all_input_masks.append(input_mask)

        all_input_ids = np.array(all_input_ids)
        all_input_masks = np.array(all_input_masks)
        outputs = self.model(all_input_ids, attention_mask=all_input_masks)
        hidden_states = outputs[-1] # output_hidden_states=True

        # compile raw embeddings
        if len(self.layers) == 1:
            #raw_embeddings = hidden_states[-1].numpy()
            raw_embeddings = hidden_states[self.layers[0]].numpy()
        else:
            raw_embeddings = []
            for batch_id in range(hidden_states[0].shape[0]):
                token_embeddings = []
                for token_id in range(hidden_states[0].shape[1]):
                    all_layers = []
                    for layer_id in self.layers:
                        all_layers.append(hidden_states[layer_id][batch_id][token_id].numpy())
                    token_embeddings.append(np.concatenate(all_layers) )  
                raw_embeddings.append(token_embeddings)
            raw_embeddings = np.array(raw_embeddings)

        if not word_level: # sentence-level embedding
            return np.mean(raw_embeddings, axis=1)
            #return np.squeeze(raw_embeddings[:,0:1,:], axis=1)

        # filter-out extra subword tokens and special tokens 
        # (using first subword of each token as embedding representations)
        filtered_embeddings = []
        for batch_idx, tokens in enumerate(sentences):
            embedding = []
            for token_idx, token in enumerate(tokens):
                if token in [self.tokenizer.cls_token, self.tokenizer.sep_token] or token.startswith(&#39;##&#39;): continue
                embedding.append(raw_embeddings[batch_idx][token_idx])
            filtered_embeddings.append(embedding)

        # pad embeddings with zeros
        max_length = max([len(e) for e in filtered_embeddings])
        embeddings = []
        for e in filtered_embeddings:
            for i in range(max_length-len(e)):
                e.append(np.zeros((self.embsize,)))
            embeddings.append(np.array(e))
        return np.array(embeddings)


class TransformerDataset(SequenceDataset):
    &#34;&#34;&#34;
    Wrapper for Transformer datasets.
    &#34;&#34;&#34;

    def __init__(self, x, y, batch_size=1):
        if type(x) not in [list, np.ndarray]: raise ValueError(&#39;x must be list or np.ndarray&#39;)
        if type(y) not in [list, np.ndarray]: raise ValueError(&#39;y must be list or np.ndarray&#39;)
        if type(x) == list: x = np.array(x)
        if type(y) == list: y = np.array(y)
        self.x = x
        self.y = y
        self.batch_size = batch_size


    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]
        batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]
        return (batch_x, batch_y)


    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)


    def to_tfdataset(self, train=True):
        &#34;&#34;&#34;
        convert transformer features to tf.Dataset
        &#34;&#34;&#34;
        if train:
            shuffle=True
            repeat = True
        else:
            shuffle=False
            repeat=False

        if len(self.y.shape) == 1:
            yshape = []
            ytype = tf.float32
        else:
            yshape = [None]
            ytype = tf.int64

        def gen():
            for idx, data in enumerate(self.x):
                yield ({&#39;input_ids&#39;: data[0],
                         &#39;attention_mask&#39;: data[1],
                         &#39;token_type_ids&#39;: data[2]},
                        self.y[idx])

        tfdataset= tf.data.Dataset.from_generator(gen,
            ({&#39;input_ids&#39;: tf.int32,
              &#39;attention_mask&#39;: tf.int32,
              &#39;token_type_ids&#39;: tf.int32},
             ytype),
            ({&#39;input_ids&#39;: tf.TensorShape([None]),
              &#39;attention_mask&#39;: tf.TensorShape([None]),
              &#39;token_type_ids&#39;: tf.TensorShape([None])},
             tf.TensorShape(yshape)))

        if shuffle:
            tfdataset = tfdataset.shuffle(self.x.shape[0])
        tfdataset = tfdataset.batch(self.batch_size)
        if repeat:
            tfdataset = tfdataset.repeat(-1)
        return tfdataset


    def get_y(self):
        return self.y

    def nsamples(self):
        return len(self.x)

    def nclasses(self):
        return self.y.shape[1]

    def xshape(self):
        return (len(self.x), self.x[0].shape[1])


# preprocessors
TEXT_PREPROCESSORS = {&#39;standard&#39;: StandardTextPreprocessor,
                      &#39;bert&#39;: BERTPreprocessor,
                      &#39;distilbert&#39;: DistilBertPreprocessor}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ktrain.text.preprocessor.bert_tokenize"><code class="name flex">
<span>def <span class="ident">bert_tokenize</span></span>(<span>docs, tokenizer, max_length, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bert_tokenize(docs, tokenizer, max_length, verbose=1):
    if verbose:
        mb = master_bar(range(1))
        pb = progress_bar(docs, parent=mb)
    else:
        mb = range(1)
        pb = docs


    indices = []
    for i in mb:
        for doc in pb:
            ids, segments = tokenizer.encode(doc, max_len=max_length)
            indices.append(ids)
        if verbose: mb.write(&#39;done.&#39;)
    zeros = np.zeros_like(indices)
    return [np.array(indices), np.array(zeros)]</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.detect_text_format"><code class="name flex">
<span>def <span class="ident">detect_text_format</span></span>(<span>texts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_text_format(texts):
    is_pair = False
    is_array = False
    err_msg = &#39;invalid text format: texts should be list of strings or list of sentence pairs in form of tuples (str, str)&#39;
    if _is_sentence_pair(texts):
        is_pair=True
        is_array = False
    elif isinstance(texts, (tuple, list, np.ndarray)):
        is_array = True
        if len(texts) == 0: raise ValueError(&#39;texts is empty&#39;)
        peek = texts[0]
        is_pair = _is_sentence_pair(peek)
        if not is_pair and not isinstance(peek, str):
            raise ValueError(err_msg)
    return is_array, is_pair</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.file_len"><code class="name flex">
<span>def <span class="ident">file_len</span></span>(<span>fname)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def file_len(fname):
    with open(fname, encoding=&#39;utf-8&#39;) as f:
        for i, l in enumerate(f):
            pass
    return i + 1</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.fname_from_url"><code class="name flex">
<span>def <span class="ident">fname_from_url</span></span>(<span>url)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fname_from_url(url):
    return os.path.split(url)[-1]</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.get_bert_path"><code class="name flex">
<span>def <span class="ident">get_bert_path</span></span>(<span>lang='en')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_bert_path(lang=&#39;en&#39;):
    if lang == &#39;en&#39;:
        bert_url = BERT_URL
    elif lang.startswith(&#39;zh-&#39;):
        bert_url = BERT_URL_CN
    else:
        bert_url = BERT_URL_MULTI
    ktrain_data = U.get_ktrain_data()
    zip_fpath = os.path.join(ktrain_data, fname_from_url(bert_url))
    bert_path = os.path.join( ktrain_data, os.path.splitext(fname_from_url(bert_url))[0] )
    if not os.path.isdir(bert_path) or \
       not os.path.isfile(os.path.join(bert_path, &#39;bert_config.json&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;bert_model.ckpt.data-00000-of-00001&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;bert_model.ckpt.index&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;bert_model.ckpt.meta&#39;)) or\
       not os.path.isfile(os.path.join(bert_path, &#39;vocab.txt&#39;)):
        # download zip
        print(&#39;downloading pretrained BERT model (%s)...&#39; % (fname_from_url(bert_url)))
        U.download(bert_url, zip_fpath)

        # unzip
        print(&#39;\nextracting pretrained BERT model...&#39;)
        with zipfile.ZipFile(zip_fpath, &#39;r&#39;) as zip_ref:
            zip_ref.extractall(ktrain_data)
        print(&#39;done.\n&#39;)

        # cleanup
        print(&#39;cleanup downloaded zip...&#39;)
        try:
            os.remove(zip_fpath)
            print(&#39;done.\n&#39;)
        except OSError:
            print(&#39;failed to cleanup/remove %s&#39; % (zip_fpath))
    return bert_path</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.get_coefs"><code class="name flex">
<span>def <span class="ident">get_coefs</span></span>(<span>word, *arr)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_coefs(word, *arr): return word, np.asarray(arr, dtype=&#39;float32&#39;)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.get_wv_path"><code class="name flex">
<span>def <span class="ident">get_wv_path</span></span>(<span>wv_path_or_url='https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_wv_path(wv_path_or_url=WV_URL):
    # process if file path given
    if os.path.isfile(wv_path_or_url) and wv_path_or_url.endswith(&#39;vec&#39;): return wv_path_or_url
    elif os.path.isfile(wv_path_or_url):
        raise ValueError(&#34;wv_path_or_url must either be URL .vec.zip or .vec.gz file or file path to .vec file&#34;)

    # process if URL is given
    fasttext_url = &#39;https://dl.fbaipublicfiles.com/fasttext&#39;
    if not wv_path_or_url.startswith(fasttext_url):
        raise ValueError(&#39;selected word vector file must be from %s&#39;% (fasttext_url))
    if not wv_path_or_url.endswith(&#39;.vec.zip&#39;) and not wv_path_or_url.endswith(&#39;vec.gz&#39;):
        raise ValueError(&#39;If wv_path_or_url is URL, must be .vec.zip filea from Facebook fasttext site.&#39;)

    ktrain_data = U.get_ktrain_data()
    zip_fpath = os.path.join(ktrain_data, fname_from_url(wv_path_or_url))
    wv_path =  os.path.join(ktrain_data, os.path.splitext(fname_from_url(wv_path_or_url))[0])
    if not os.path.isfile(wv_path):
        # download zip
        print(&#39;downloading pretrained word vectors to %s ...&#39; % (ktrain_data))
        U.download(wv_path_or_url, zip_fpath)

        # unzip
        print(&#39;\nextracting pretrained word vectors...&#39;)
        if wv_path_or_url.endswith(&#39;.vec.zip&#39;):
            with zipfile.ZipFile(zip_fpath, &#39;r&#39;) as zip_ref:
                zip_ref.extractall(ktrain_data)
        else: # .vec.gz
            with gzip.open(zip_fpath, &#39;rb&#39;) as f_in:
                with open(wv_path, &#39;wb&#39;) as f_out:
                    shutil.copyfileobj(f_in, f_out)
        print(&#39;done.\n&#39;)

        # cleanup
        print(&#39;cleanup downloaded zip...&#39;)
        try:
            os.remove(zip_fpath)
            print(&#39;done.\n&#39;)
        except OSError:
            print(&#39;failed to cleanup/remove %s&#39; % (zip_fpath))
    return wv_path</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.hf_convert_example"><code class="name flex">
<span>def <span class="ident">hf_convert_example</span></span>(<span>text_a, text_b=None, tokenizer=None, max_length=512, pad_on_left=False, pad_token=0, pad_token_segment_id=0, mask_padding_with_zero=True)</span>
</code></dt>
<dd>
<div class="desc"><p>convert InputExample to InputFeature for Hugging Face transformer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hf_convert_example(text_a, text_b=None, tokenizer=None,
                       max_length=512,
                       pad_on_left=False,
                       pad_token=0,
                       pad_token_segment_id=0,
                       mask_padding_with_zero=True):
    &#34;&#34;&#34;
    convert InputExample to InputFeature for Hugging Face transformer
    &#34;&#34;&#34;
    if tokenizer is None: raise ValueError(&#39;tokenizer is required&#39;)
    inputs = tokenizer.encode_plus(
        text_a,
        text_b,
        add_special_tokens=True,
        return_token_type_ids=True,
        max_length=max_length, 
        truncation=&#39;longest_first&#39;
    )
    input_ids, token_type_ids = inputs[&#34;input_ids&#34;], inputs[&#34;token_type_ids&#34;]

    # The mask has 1 for real tokens and 0 for padding tokens. Only real
    # tokens are attended to.
    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)

    # Zero-pad up to the sequence length.
    padding_length = max_length - len(input_ids)
    if pad_on_left:
        input_ids = ([pad_token] * padding_length) + input_ids
        attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask
        token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids
    else:
        input_ids = input_ids + ([pad_token] * padding_length)
        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)
        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)
    assert len(input_ids) == max_length, &#34;Error with input length {} vs {}&#34;.format(len(input_ids), max_length)
    assert len(attention_mask) == max_length, &#34;Error with input length {} vs {}&#34;.format(len(attention_mask), max_length)
    assert len(token_type_ids) == max_length, &#34;Error with input length {} vs {}&#34;.format(len(token_type_ids), max_length)


    #if ex_index &lt; 1:
        #print(&#34;*** Example ***&#34;)
        #print(&#34;guid: %s&#34; % (example.guid))
        #print(&#34;input_ids: %s&#34; % &#34; &#34;.join([str(x) for x in input_ids]))
        #print(&#34;attention_mask: %s&#34; % &#34; &#34;.join([str(x) for x in attention_mask]))
        #print(&#34;token_type_ids: %s&#34; % &#34; &#34;.join([str(x) for x in token_type_ids]))
        #print(&#34;label: %s (id = %d)&#34; % (example.label, label))

    return [input_ids, attention_mask, token_type_ids]</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.hf_convert_examples"><code class="name flex">
<span>def <span class="ident">hf_convert_examples</span></span>(<span>texts, y=None, tokenizer=None, max_length=512, pad_on_left=False, pad_token=0, pad_token_segment_id=0, mask_padding_with_zero=True, use_dynamic_shape=False, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a data file into a list of <code>InputFeatures</code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>texts</code></strong></dt>
<dd>texts of documents or sentence pairs</dd>
<dt><strong><code>y</code></strong></dt>
<dd>labels for documents</dd>
<dt><strong><code>tokenizer</code></strong></dt>
<dd>Instance of a tokenizer that will tokenize the examples</dd>
<dt><strong><code>max_length</code></strong></dt>
<dd>Maximum example length</dd>
<dt><strong><code>pad_on_left</code></strong></dt>
<dd>If set to <code>True</code>, the examples will be padded on the left rather than on the right (default)</dd>
<dt><strong><code>pad_token</code></strong></dt>
<dd>Padding token</dd>
<dt><strong><code>pad_token_segment_id</code></strong></dt>
<dd>The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)</dd>
<dt><strong><code>mask_padding_with_zero</code></strong></dt>
<dd>If set to <code>True</code>, the attention mask will be filled by <code>1</code> for actual values
and by <code>0</code> for padded values. If set to <code>False</code>, inverts it (<code>1</code> for padded values, <code>0</code> for
actual values)</dd>
</dl>
<p>use_dynamic_shape(bool):
If True, supplied max_length will be ignored and will be computed
based on provided texts instead.
verbose(bool): verbosity</p>
<h2 id="returns">Returns</h2>
<p>If the <code>examples</code> input is a <code>tf.data.Dataset</code>, will return a <code>tf.data.Dataset</code>
containing the task-specific features. If the input is a list of <code>InputExamples</code>, will return
a list of task-specific <code>InputFeatures</code> which can be fed to the model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hf_convert_examples(texts, y=None, tokenizer=None,
                        max_length=512,
                        pad_on_left=False,
                        pad_token=0,
                        pad_token_segment_id=0,
                        mask_padding_with_zero=True,
                        use_dynamic_shape=False,
                        verbose=1):
    &#34;&#34;&#34;
    Loads a data file into a list of ``InputFeatures``
    Args:
        texts: texts of documents or sentence pairs
        y:  labels for documents
        tokenizer: Instance of a tokenizer that will tokenize the examples
        max_length: Maximum example length
        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)
        pad_token: Padding token
        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)
        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values
            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for
            actual values)
        use_dynamic_shape(bool):  If True, supplied max_length will be ignored and will be computed
                                  based on provided texts instead.
        verbose(bool): verbosity
    Returns:
        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``
        containing the task-specific features. If the input is a list of ``InputExamples``, will return
        a list of task-specific ``InputFeatures`` which can be fed to the model.
    &#34;&#34;&#34;

    is_array, is_pair = detect_text_format(texts)

    if use_dynamic_shape:
        sentences = []
        for text in texts:
            if is_pair:
                text_a = text[0]
                text_b = text[1]
            else:
                text_a = text
                text_b = None
            sentences.append( tokenizer.convert_ids_to_tokens(tokenizer.encode(text_a, text_b)) )
            #sentences.append(tokenizer.tokenize(text_a, text_b)) # only works for Fast tokenizers
        maxlen = len(max([tokens for tokens in sentences], key=len,)) + 2

        if maxlen &lt; max_length: max_length = maxlen


    data = []
    features_list = []
    labels = []
    if verbose:
        mb = master_bar(range(1))
        pb = progress_bar(texts, parent=mb)
    else:
        mb = range(1)
        pb = texts
    for i in mb:
        #for (idx, text) in enumerate(progress_bar(texts, parent=mb)):
        for (idx, text) in enumerate(pb):
            if is_pair:
                text_a = text[0]
                text_b = text[1]
            else:
                text_a = text
                text_b = None
            features = hf_convert_example(text_a, text_b=text_b, tokenizer=tokenizer,
                                          max_length=max_length,
                                          pad_on_left=pad_on_left,
                                          pad_token=pad_token,
                                          pad_token_segment_id=pad_token_segment_id,
                                          mask_padding_with_zero=mask_padding_with_zero)
            features_list.append(features)
            labels.append(y[idx] if y is not None else None)
    #tfdataset = hf_features_to_tfdataset(features_list, labels)
    #return tfdataset
    #return (features_list, labels)
    # HF_EXCEPTION
    # due to issues in transormers library and TF2 tf.Datasets, arrays are converted
    # to iterators on-the-fly
    #return  TransformerSequence(np.array(features_list), np.array(labels))
    return  TransformerDataset(np.array(features_list), np.array(labels))</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.hf_features_to_tfdataset"><code class="name flex">
<span>def <span class="ident">hf_features_to_tfdataset</span></span>(<span>features_list, labels)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hf_features_to_tfdataset(features_list, labels):
    features_list = np.array(features_list)
    labels = np.array(labels) if labels is not None else None
    tfdataset = tf.data.Dataset.from_tensor_slices((features_list, labels))
    tfdataset = tfdataset.map(lambda x,y: ({&#39;input_ids&#39;: x[0], 
                                            &#39;attention_mask&#39;: x[1], 
                                             &#39;token_type_ids&#39;: x[2]}, y))

    return tfdataset</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.is_nospace_lang"><code class="name flex">
<span>def <span class="ident">is_nospace_lang</span></span>(<span>lang)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_nospace_lang(lang):
    return lang in NOSPACE_LANGS</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.load_wv"><code class="name flex">
<span>def <span class="ident">load_wv</span></span>(<span>wv_path_or_url='https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip', verbose=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_wv(wv_path_or_url=WV_URL, verbose=1):
    wv_path = get_wv_path(wv_path_or_url)
    if verbose: print(&#39;loading pretrained word vectors...this may take a few moments...&#39;)
    length = file_len(wv_path)
    tups = []
    mb = master_bar(range(1))
    for i in mb:
        f = open(wv_path, encoding=&#39;utf-8&#39;)
        for o in progress_bar(range(length), parent=mb):
            o = f.readline()
            tups.append(get_coefs(*o.rstrip().rsplit(&#39; &#39;)))
        f.close()
        #if verbose: mb.write(&#39;done.&#39;)
    return dict(tups)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ktrain.text.preprocessor.BERTPreprocessor"><code class="flex name class">
<span>class <span class="ident">BERTPreprocessor</span></span>
<span>(</span><span>maxlen, max_features, class_names=[], classes=[], lang='en', ngram_range=1, multilabel=None)</span>
</code></dt>
<dd>
<div class="desc"><p>text preprocessing for BERT model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BERTPreprocessor(TextPreprocessor):
    &#34;&#34;&#34;
    text preprocessing for BERT model
    &#34;&#34;&#34;

    def __init__(self, maxlen, max_features, class_names=[], classes=[], 
                lang=&#39;en&#39;, ngram_range=1, multilabel=None):
        class_names = self.migrate_classes(class_names, classes)


        if maxlen &gt; 512: raise ValueError(&#39;BERT only supports maxlen &lt;= 512&#39;)

        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)
        vocab_path = os.path.join(get_bert_path(lang=lang), &#39;vocab.txt&#39;)
        token_dict = {}
        with codecs.open(vocab_path, &#39;r&#39;, &#39;utf8&#39;) as reader:
            for line in reader:
                token = line.strip()
                token_dict[token] = len(token_dict)
        tokenizer = BERT_Tokenizer(token_dict)
        self.tok = tokenizer
        self.tok_dct = dict((v,k) for k,v in token_dict.items())
        self.max_features = max_features # ignored
        self.ngram_range = 1 # ignored


    def get_tokenizer(self):
        return self.tok


    def __getstate__(self):
        return {k: v for k, v in self.__dict__.items()}


    def __setstate__(self, state):
        &#34;&#34;&#34;
        For backwards compatibility with pre-ytransform versions
        &#34;&#34;&#34;
        self.__dict__.update(state)
        if not hasattr(self, &#39;ytransform&#39;):
            le = self.label_encoder if hasattr(self, &#39;label_encoder&#39;) else None
            self.ytransform = U.YTransform(class_names=self.get_classes(), label_encoder=le)


    def get_preprocessor(self):
        return (self.tok, self.tok_dct)



    def preprocess(self, texts):
        return self.preprocess_test(texts, verbose=0)[0]


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        dct = self.tok_dct
        return &#34; &#34;.join([dct[wid] for wid in doc if wid != 0 and wid in dct])


    def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
        &#34;&#34;&#34;
        preprocess training set
        &#34;&#34;&#34;
        if mode == &#39;train&#39; and y is None:
            raise ValueError(&#39;y is required when mode=train&#39;)
        if self.lang is None and mode==&#39;train&#39;: self.lang = TU.detect_lang(texts)
        U.vprint(&#39;preprocessing %s...&#39; % (mode), verbose=verbose)
        U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

        x = bert_tokenize(texts, self.tok, self.maxlen, verbose=verbose)

        # transform y
        y = self._transform_y(y, train=mode==&#39;train&#39;, verbose=verbose)
        result = (x, y)
        self.set_multilabel(result, mode)
        if mode == &#39;train&#39;: self.preprocess_train_called = True
        return result



    def preprocess_test(self, texts, y=None, mode=&#39;test&#39;, verbose=1):
        self.check_trained()
        return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></li>
<li><a title="ktrain.preprocessor.Preprocessor" href="../preprocessor.html#ktrain.preprocessor.Preprocessor">Preprocessor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.BERTPreprocessor.get_preprocessor"><code class="name flex">
<span>def <span class="ident">get_preprocessor</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_preprocessor(self):
    return (self.tok, self.tok_dct)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.BERTPreprocessor.get_tokenizer"><code class="name flex">
<span>def <span class="ident">get_tokenizer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tokenizer(self):
    return self.tok</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.BERTPreprocessor.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>self, texts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess(self, texts):
    return self.preprocess_test(texts, verbose=0)[0]</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.BERTPreprocessor.preprocess_test"><code class="name flex">
<span>def <span class="ident">preprocess_test</span></span>(<span>self, texts, y=None, mode='test', verbose=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_test(self, texts, y=None, mode=&#39;test&#39;, verbose=1):
    self.check_trained()
    return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.BERTPreprocessor.preprocess_train"><code class="name flex">
<span>def <span class="ident">preprocess_train</span></span>(<span>self, texts, y=None, mode='train', verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>preprocess training set</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
    &#34;&#34;&#34;
    preprocess training set
    &#34;&#34;&#34;
    if mode == &#39;train&#39; and y is None:
        raise ValueError(&#39;y is required when mode=train&#39;)
    if self.lang is None and mode==&#39;train&#39;: self.lang = TU.detect_lang(texts)
    U.vprint(&#39;preprocessing %s...&#39; % (mode), verbose=verbose)
    U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

    x = bert_tokenize(texts, self.tok, self.maxlen, verbose=verbose)

    # transform y
    y = self._transform_y(y, train=mode==&#39;train&#39;, verbose=verbose)
    result = (x, y)
    self.set_multilabel(result, mode)
    if mode == &#39;train&#39;: self.preprocess_train_called = True
    return result</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></b></code>:
<ul class="hlist">
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats">print_seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.seqlen_stats">seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.undo" href="#ktrain.text.preprocessor.TextPreprocessor.undo">undo</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ktrain.text.preprocessor.DistilBertPreprocessor"><code class="flex name class">
<span>class <span class="ident">DistilBertPreprocessor</span></span>
<span>(</span><span>maxlen, max_features, class_names=[], classes=[], lang='en', ngram_range=1)</span>
</code></dt>
<dd>
<div class="desc"><p>text preprocessing for Hugging Face DistlBert model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DistilBertPreprocessor(TransformersPreprocessor):
    &#34;&#34;&#34;
    text preprocessing for Hugging Face DistlBert model
    &#34;&#34;&#34;

    def __init__(self, maxlen, max_features, class_names=[], classes=[], 
                lang=&#39;en&#39;, ngram_range=1):
        class_names = self.migrate_classes(class_names, classes)
        name = DISTILBERT
        if lang == &#39;en&#39;:
            model_name = &#39;distilbert-base-uncased&#39;
        else:
            model_name = &#39;distilbert-base-multilingual-cased&#39;

        super().__init__(model_name,
                         maxlen, max_features, class_names=class_names, 
                         lang=lang, ngram_range=ngram_range)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ktrain.text.preprocessor.TransformersPreprocessor" href="#ktrain.text.preprocessor.TransformersPreprocessor">TransformersPreprocessor</a></li>
<li><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></li>
<li><a title="ktrain.preprocessor.Preprocessor" href="../preprocessor.html#ktrain.preprocessor.Preprocessor">Preprocessor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ktrain.text.preprocessor.TransformersPreprocessor" href="#ktrain.text.preprocessor.TransformersPreprocessor">TransformersPreprocessor</a></b></code>:
<ul class="hlist">
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_classifier" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_classifier">get_classifier</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_regression_model" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_regression_model">get_regression_model</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.load_model_and_configure_from_data" href="#ktrain.text.preprocessor.TransformersPreprocessor.load_model_and_configure_from_data">load_model_and_configure_from_data</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.preprocess_train" href="#ktrain.text.preprocessor.TransformersPreprocessor.preprocess_train">preprocess_train</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.print_seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats">print_seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.seqlen_stats">seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.undo" href="#ktrain.text.preprocessor.TextPreprocessor.undo">undo</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ktrain.text.preprocessor.StandardTextPreprocessor"><code class="flex name class">
<span>class <span class="ident">StandardTextPreprocessor</span></span>
<span>(</span><span>maxlen, max_features, class_names=[], classes=[], lang='en', ngram_range=1, multilabel=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Standard text preprocessing</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StandardTextPreprocessor(TextPreprocessor):
    &#34;&#34;&#34;
    Standard text preprocessing
    &#34;&#34;&#34;

    def __init__(self, maxlen, max_features, class_names=[], classes=[], 
                 lang=&#39;en&#39;, ngram_range=1, multilabel=None):
        class_names = self.migrate_classes(class_names, classes)
        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)
        self.tok = None
        self.tok_dct = {}
        self.max_features = max_features
        self.ngram_range = ngram_range

    def get_tokenizer(self):
        return self.tok


    def __getstate__(self):
        return {k: v for k, v in self.__dict__.items()}


    def __setstate__(self, state):
        &#34;&#34;&#34;
        For backwards compatibility with pre-ytransform versions
        &#34;&#34;&#34;
        self.__dict__.update(state)
        if not hasattr(self, &#39;ytransform&#39;):
            le = self.label_encoder if hasattr(self, &#39;label_encoder&#39;) else None
            self.ytransform = U.YTransform(class_names=self.get_classes(), label_encoder=le)



    def get_preprocessor(self):
        return (self.tok, self.tok_dct)


    def preprocess(self, texts):
        return self.preprocess_test(texts, verbose=0)[0]


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        dct = self.tok.index_word
        return &#34; &#34;.join([dct[wid] for wid in doc if wid != 0 and wid in dct])


    def preprocess_train(self, train_text, y_train, verbose=1):
        &#34;&#34;&#34;
        preprocess training set
        &#34;&#34;&#34;
        if self.lang is None: self.lang = TU.detect_lang(train_text)


        U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

        # special processing if Chinese
        train_text = self.process_chinese(train_text, lang=self.lang)

        # extract vocabulary
        self.tok = Tokenizer(num_words=self.max_features)
        self.tok.fit_on_texts(train_text)
        U.vprint(&#39;Word Counts: {}&#39;.format(len(self.tok.word_counts)), verbose=verbose)
        U.vprint(&#39;Nrows: {}&#39;.format(len(train_text)), verbose=verbose)

        # convert to word IDs
        x_train = self.tok.texts_to_sequences(train_text)
        U.vprint(&#39;{} train sequences&#39;.format(len(x_train)), verbose=verbose)
        self.print_seqlen_stats(x_train, &#39;train&#39;, verbose=verbose)

        # add ngrams
        x_train = self._fit_ngrams(x_train, verbose=verbose)

        # pad sequences
        x_train = sequence.pad_sequences(x_train, maxlen=self.maxlen)
        U.vprint(&#39;x_train shape: ({},{})&#39;.format(x_train.shape[0], x_train.shape[1]), verbose=verbose)

        # transform y
        y_train = self._transform_y(y_train, train=True, verbose=verbose)
        if y_train is not None and verbose:
            print(&#39;y_train shape: %s&#39; % (y_train.shape,))

        # return
        result =  (x_train, y_train)
        self.set_multilabel(result, &#39;train&#39;)
        self.preprocess_train_called = True
        return result


    def preprocess_test(self, test_text, y_test=None, verbose=1):
        &#34;&#34;&#34;
        preprocess validation or test dataset
        &#34;&#34;&#34;
        self.check_trained()
        if self.tok is None or self.lang is None:
            raise Exception(&#39;Unfitted tokenizer or missing language. Did you run preprocess_train first?&#39;)

        # check for and process chinese
        test_text = self.process_chinese(test_text, self.lang)

        # convert to word IDs
        x_test = self.tok.texts_to_sequences(test_text)
        U.vprint(&#39;{} test sequences&#39;.format(len(x_test)), verbose=verbose)
        self.print_seqlen_stats(x_test, &#39;test&#39;, verbose=verbose)

        # add n-grams
        x_test = self._add_ngrams(x_test, mode=&#39;test&#39;, verbose=verbose)


        # pad sequences
        x_test = sequence.pad_sequences(x_test, maxlen=self.maxlen)
        U.vprint(&#39;x_test shape: ({},{})&#39;.format(x_test.shape[0], x_test.shape[1]), verbose=verbose)

        # transform y
        y_test = self._transform_y(y_test, train=False, verbose=verbose)
        if y_test is not None and verbose:
            print(&#39;y_test shape: %s&#39; % (y_test.shape,))


        # return
        return (x_test, y_test)



    def _fit_ngrams(self, x_train, verbose=1):
        self.tok_dct = {}
        if self.ngram_range &lt; 2: return x_train
        U.vprint(&#39;Adding {}-gram features&#39;.format(self.ngram_range), verbose=verbose)
        # Create set of unique n-gram from the training set.
        ngram_set = set()
        for input_list in x_train:
            for i in range(2, self.ngram_range + 1):
                set_of_ngram = self._create_ngram_set(input_list, ngram_value=i)
                ngram_set.update(set_of_ngram)

        # Dictionary mapping n-gram token to a unique integer.
        # Integer values are greater than max_features in order
        # to avoid collision with existing features.
        start_index = self.max_features + 1
        token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}
        indice_token = {token_indice[k]: k for k in token_indice}
        self.tok_dct = token_indice

        # max_features is the highest integer that could be found in the dataset.
        self.max_features = np.max(list(indice_token.keys())) + 1
        U.vprint(&#39;max_features changed to %s with addition of ngrams&#39; % (self.max_features), verbose=verbose)

        # Augmenting x_train with n-grams features
        x_train = self._add_ngrams(x_train, verbose=verbose, mode=&#39;train&#39;)
        return x_train


    def _add_ngrams(self, sequences, verbose=1, mode=&#39;test&#39;):
        &#34;&#34;&#34;
        Augment the input list of list (sequences) by appending n-grams values.
        Example: adding bi-gram
        &#34;&#34;&#34;
        token_indice = self.tok_dct
        if self.ngram_range &lt; 2: return sequences
        new_sequences = []
        for input_list in sequences:
            new_list = input_list[:]
            for ngram_value in range(2, self.ngram_range + 1):
                for i in range(len(new_list) - ngram_value + 1):
                    ngram = tuple(new_list[i:i + ngram_value])
                    if ngram in token_indice:
                        new_list.append(token_indice[ngram])
            new_sequences.append(new_list)
        U.vprint(&#39;Average {} sequence length with ngrams: {}&#39;.format(mode,
            np.mean(list(map(len, new_sequences)), dtype=int)), verbose=verbose)    
        self.print_seqlen_stats(new_sequences, &#39;%s (w/ngrams)&#39; % mode, verbose=verbose)
        return new_sequences



    def _create_ngram_set(self, input_list, ngram_value=2):
        &#34;&#34;&#34;
        Extract a set of n-grams from a list of integers.
        &gt;&gt;&gt; create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)
        {(4, 9), (4, 1), (1, 4), (9, 4)}
        &gt;&gt;&gt; create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)
        [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]
        &#34;&#34;&#34;
        return set(zip(*[input_list[i:] for i in range(ngram_value)]))


    def ngram_count(self):
        if not self.tok_dct: return 1
        s = set()
        for k in self.tok_dct.keys():
            s.add(len(k))
        return max(list(s))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></li>
<li><a title="ktrain.preprocessor.Preprocessor" href="../preprocessor.html#ktrain.preprocessor.Preprocessor">Preprocessor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.StandardTextPreprocessor.get_preprocessor"><code class="name flex">
<span>def <span class="ident">get_preprocessor</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_preprocessor(self):
    return (self.tok, self.tok_dct)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.StandardTextPreprocessor.get_tokenizer"><code class="name flex">
<span>def <span class="ident">get_tokenizer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tokenizer(self):
    return self.tok</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.StandardTextPreprocessor.ngram_count"><code class="name flex">
<span>def <span class="ident">ngram_count</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ngram_count(self):
    if not self.tok_dct: return 1
    s = set()
    for k in self.tok_dct.keys():
        s.add(len(k))
    return max(list(s))</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.StandardTextPreprocessor.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>self, texts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess(self, texts):
    return self.preprocess_test(texts, verbose=0)[0]</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.StandardTextPreprocessor.preprocess_test"><code class="name flex">
<span>def <span class="ident">preprocess_test</span></span>(<span>self, test_text, y_test=None, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>preprocess validation or test dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_test(self, test_text, y_test=None, verbose=1):
    &#34;&#34;&#34;
    preprocess validation or test dataset
    &#34;&#34;&#34;
    self.check_trained()
    if self.tok is None or self.lang is None:
        raise Exception(&#39;Unfitted tokenizer or missing language. Did you run preprocess_train first?&#39;)

    # check for and process chinese
    test_text = self.process_chinese(test_text, self.lang)

    # convert to word IDs
    x_test = self.tok.texts_to_sequences(test_text)
    U.vprint(&#39;{} test sequences&#39;.format(len(x_test)), verbose=verbose)
    self.print_seqlen_stats(x_test, &#39;test&#39;, verbose=verbose)

    # add n-grams
    x_test = self._add_ngrams(x_test, mode=&#39;test&#39;, verbose=verbose)


    # pad sequences
    x_test = sequence.pad_sequences(x_test, maxlen=self.maxlen)
    U.vprint(&#39;x_test shape: ({},{})&#39;.format(x_test.shape[0], x_test.shape[1]), verbose=verbose)

    # transform y
    y_test = self._transform_y(y_test, train=False, verbose=verbose)
    if y_test is not None and verbose:
        print(&#39;y_test shape: %s&#39; % (y_test.shape,))


    # return
    return (x_test, y_test)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.StandardTextPreprocessor.preprocess_train"><code class="name flex">
<span>def <span class="ident">preprocess_train</span></span>(<span>self, train_text, y_train, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>preprocess training set</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_train(self, train_text, y_train, verbose=1):
    &#34;&#34;&#34;
    preprocess training set
    &#34;&#34;&#34;
    if self.lang is None: self.lang = TU.detect_lang(train_text)


    U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

    # special processing if Chinese
    train_text = self.process_chinese(train_text, lang=self.lang)

    # extract vocabulary
    self.tok = Tokenizer(num_words=self.max_features)
    self.tok.fit_on_texts(train_text)
    U.vprint(&#39;Word Counts: {}&#39;.format(len(self.tok.word_counts)), verbose=verbose)
    U.vprint(&#39;Nrows: {}&#39;.format(len(train_text)), verbose=verbose)

    # convert to word IDs
    x_train = self.tok.texts_to_sequences(train_text)
    U.vprint(&#39;{} train sequences&#39;.format(len(x_train)), verbose=verbose)
    self.print_seqlen_stats(x_train, &#39;train&#39;, verbose=verbose)

    # add ngrams
    x_train = self._fit_ngrams(x_train, verbose=verbose)

    # pad sequences
    x_train = sequence.pad_sequences(x_train, maxlen=self.maxlen)
    U.vprint(&#39;x_train shape: ({},{})&#39;.format(x_train.shape[0], x_train.shape[1]), verbose=verbose)

    # transform y
    y_train = self._transform_y(y_train, train=True, verbose=verbose)
    if y_train is not None and verbose:
        print(&#39;y_train shape: %s&#39; % (y_train.shape,))

    # return
    result =  (x_train, y_train)
    self.set_multilabel(result, &#39;train&#39;)
    self.preprocess_train_called = True
    return result</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></b></code>:
<ul class="hlist">
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats">print_seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.seqlen_stats">seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.undo" href="#ktrain.text.preprocessor.TextPreprocessor.undo">undo</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor"><code class="flex name class">
<span>class <span class="ident">TextPreprocessor</span></span>
<span>(</span><span>maxlen, class_names, lang='en', multilabel=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Text preprocessing base class</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TextPreprocessor(Preprocessor):
    &#34;&#34;&#34;
    Text preprocessing base class
    &#34;&#34;&#34;

    def __init__(self, maxlen, class_names, lang=&#39;en&#39;, multilabel=None):

        self.set_classes(class_names) # converts to list of necessary
        self.maxlen = maxlen
        self.lang = lang
        self.multilabel = multilabel # currently, this is always initially set None until set by set_multilabel
        self.preprocess_train_called = False
        #self.label_encoder = None # only set if y is in string format
        self.ytransform = None
        self.c = self.c.tolist() if isinstance(self.c, np.ndarray) else self.c


    def migrate_classes(self, class_names, classes):
        # NOTE: this method transforms to np.ndarray to list.
        # If removed and &#34;if class_names&#34; is issued prior to set_classes(), an error will occur.
        class_names = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names
        classes = classes.tolist() if isinstance(classes, np.ndarray) else classes

        if not class_names and classes:
            class_names = classes
            warnings.warn(&#39;The class_names argument is replacing the classes argument. Please update your code.&#39;)
        return class_names


    def get_tokenizer(self):
        raise NotImplementedError(&#39;This method was not overridden in subclass&#39;)

    def check_trained(self):
        if not self.preprocess_train_called:
            warnings.warn(&#39;The method preprocess_train was never called. You can disable this warning by setting preprocess_train_called=True.&#39;)
            #raise Exception(&#39;preprocess_train must be called&#39;)


    def get_preprocessor(self):
        raise NotImplementedError


    def get_classes(self):
        return self.c


    def set_classes(self, class_names):
        self.c = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names


    def preprocess(self, texts):
        raise NotImplementedError


    def set_multilabel(self, data, mode, verbose=1):
        if mode == &#39;train&#39; and self.get_classes():
            original_multilabel = self.multilabel
            discovered_multilabel = U.is_multilabel(data)
            if original_multilabel is None:
                self.multilabel = discovered_multilabel
            elif original_multilabel is True and discovered_multilabel is False:
                warnings.warn(&#39;The multilabel=True argument was supplied, but labels do not indicate &#39;+\
                              &#39;a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.&#39;)
            elif original_multilabel is False and discovered_multilabel is True:
                warnings.warn(&#39;The multilabel=False argument was supplied, but labels inidcate that  &#39;+\
                              &#39;this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.&#39;)
            U.vprint(&#34;Is Multi-Label? %s&#34; % (self.multilabel), verbose=verbose)


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        raise NotImplementedError


    def is_chinese(self):
        return TU.is_chinese(self.lang)


    def is_nospace_lang(self):
        return TU.is_nospace_lang(self.lang)


    def process_chinese(self, texts, lang=None):
        #if lang is None: lang = langdetect.detect(texts[0])
        if lang is None: lang = TU.detect_lang(texts)
        if not TU.is_nospace_lang(lang): return texts
        return TU.split_chinese(texts)


    @classmethod
    def seqlen_stats(cls, list_of_texts):
        &#34;&#34;&#34;
        compute sequence length stats from
        list of texts in any spaces-segmented language
        Args:
            list_of_texts: list of strings
        Returns:
            dict: dictionary with keys: mean, 95percentile, 99percentile
        &#34;&#34;&#34;
        counts = []
        for text in list_of_texts:
            if isinstance(text, (list, np.ndarray)):
                lst = text
            else:
                lst = text.split()
            counts.append(len(lst))
        p95 = np.percentile(counts, 95)
        p99 = np.percentile(counts, 99)
        avg = sum(counts)/len(counts)
        return {&#39;mean&#39;:avg, &#39;95percentile&#39;: p95, &#39;99percentile&#39;:p99}


    def print_seqlen_stats(self, texts, mode, verbose=1):
        &#34;&#34;&#34;
        prints stats about sequence lengths
        &#34;&#34;&#34;
        if verbose and not self.is_nospace_lang():
            stat_dict = TextPreprocessor.seqlen_stats(texts)
            print( &#34;%s sequence lengths:&#34; % mode)
            for k in stat_dict:
                print(&#34;\t%s : %s&#34; % (k, int(round(stat_dict[k]))))


    def _transform_y(self, y_data, train=False, verbose=1):
        &#34;&#34;&#34;
        preprocess y
        If shape of y is 1, then task is considered classification if self.c exists
        or regression if not.
        &#34;&#34;&#34;
        if self.ytransform is None:
            self.ytransform = U.YTransform(class_names=self.get_classes())
        y = self.ytransform.apply(y_data, train=train)
        if train: self.c = self.ytransform.get_classes()
        return y</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ktrain.preprocessor.Preprocessor" href="../preprocessor.html#ktrain.preprocessor.Preprocessor">Preprocessor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ktrain.text.preprocessor.BERTPreprocessor" href="#ktrain.text.preprocessor.BERTPreprocessor">BERTPreprocessor</a></li>
<li><a title="ktrain.text.preprocessor.StandardTextPreprocessor" href="#ktrain.text.preprocessor.StandardTextPreprocessor">StandardTextPreprocessor</a></li>
<li><a title="ktrain.text.preprocessor.TransformersPreprocessor" href="#ktrain.text.preprocessor.TransformersPreprocessor">TransformersPreprocessor</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.TextPreprocessor.seqlen_stats"><code class="name flex">
<span>def <span class="ident">seqlen_stats</span></span>(<span>list_of_texts)</span>
</code></dt>
<dd>
<div class="desc"><p>compute sequence length stats from
list of texts in any spaces-segmented language</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>list_of_texts</code></strong></dt>
<dd>list of strings</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>dictionary with keys: mean, 95percentile, 99percentile</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def seqlen_stats(cls, list_of_texts):
    &#34;&#34;&#34;
    compute sequence length stats from
    list of texts in any spaces-segmented language
    Args:
        list_of_texts: list of strings
    Returns:
        dict: dictionary with keys: mean, 95percentile, 99percentile
    &#34;&#34;&#34;
    counts = []
    for text in list_of_texts:
        if isinstance(text, (list, np.ndarray)):
            lst = text
        else:
            lst = text.split()
        counts.append(len(lst))
    p95 = np.percentile(counts, 95)
    p99 = np.percentile(counts, 99)
    avg = sum(counts)/len(counts)
    return {&#39;mean&#39;:avg, &#39;95percentile&#39;: p95, &#39;99percentile&#39;:p99}</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.TextPreprocessor.check_trained"><code class="name flex">
<span>def <span class="ident">check_trained</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_trained(self):
    if not self.preprocess_train_called:
        warnings.warn(&#39;The method preprocess_train was never called. You can disable this warning by setting preprocess_train_called=True.&#39;)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.get_classes"><code class="name flex">
<span>def <span class="ident">get_classes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_classes(self):
    return self.c</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.get_preprocessor"><code class="name flex">
<span>def <span class="ident">get_preprocessor</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_preprocessor(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.get_tokenizer"><code class="name flex">
<span>def <span class="ident">get_tokenizer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tokenizer(self):
    raise NotImplementedError(&#39;This method was not overridden in subclass&#39;)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.is_chinese"><code class="name flex">
<span>def <span class="ident">is_chinese</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_chinese(self):
    return TU.is_chinese(self.lang)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.is_nospace_lang"><code class="name flex">
<span>def <span class="ident">is_nospace_lang</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_nospace_lang(self):
    return TU.is_nospace_lang(self.lang)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.migrate_classes"><code class="name flex">
<span>def <span class="ident">migrate_classes</span></span>(<span>self, class_names, classes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def migrate_classes(self, class_names, classes):
    # NOTE: this method transforms to np.ndarray to list.
    # If removed and &#34;if class_names&#34; is issued prior to set_classes(), an error will occur.
    class_names = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names
    classes = classes.tolist() if isinstance(classes, np.ndarray) else classes

    if not class_names and classes:
        class_names = classes
        warnings.warn(&#39;The class_names argument is replacing the classes argument. Please update your code.&#39;)
    return class_names</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>self, texts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess(self, texts):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats"><code class="name flex">
<span>def <span class="ident">print_seqlen_stats</span></span>(<span>self, texts, mode, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>prints stats about sequence lengths</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_seqlen_stats(self, texts, mode, verbose=1):
    &#34;&#34;&#34;
    prints stats about sequence lengths
    &#34;&#34;&#34;
    if verbose and not self.is_nospace_lang():
        stat_dict = TextPreprocessor.seqlen_stats(texts)
        print( &#34;%s sequence lengths:&#34; % mode)
        for k in stat_dict:
            print(&#34;\t%s : %s&#34; % (k, int(round(stat_dict[k]))))</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.process_chinese"><code class="name flex">
<span>def <span class="ident">process_chinese</span></span>(<span>self, texts, lang=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_chinese(self, texts, lang=None):
    #if lang is None: lang = langdetect.detect(texts[0])
    if lang is None: lang = TU.detect_lang(texts)
    if not TU.is_nospace_lang(lang): return texts
    return TU.split_chinese(texts)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.set_classes"><code class="name flex">
<span>def <span class="ident">set_classes</span></span>(<span>self, class_names)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_classes(self, class_names):
    self.c = class_names.tolist() if isinstance(class_names, np.ndarray) else class_names</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.set_multilabel"><code class="name flex">
<span>def <span class="ident">set_multilabel</span></span>(<span>self, data, mode, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_multilabel(self, data, mode, verbose=1):
    if mode == &#39;train&#39; and self.get_classes():
        original_multilabel = self.multilabel
        discovered_multilabel = U.is_multilabel(data)
        if original_multilabel is None:
            self.multilabel = discovered_multilabel
        elif original_multilabel is True and discovered_multilabel is False:
            warnings.warn(&#39;The multilabel=True argument was supplied, but labels do not indicate &#39;+\
                          &#39;a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.&#39;)
        elif original_multilabel is False and discovered_multilabel is True:
            warnings.warn(&#39;The multilabel=False argument was supplied, but labels inidcate that  &#39;+\
                          &#39;this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.&#39;)
        U.vprint(&#34;Is Multi-Label? %s&#34; % (self.multilabel), verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TextPreprocessor.undo"><code class="name flex">
<span>def <span class="ident">undo</span></span>(<span>self, doc)</span>
</code></dt>
<dd>
<div class="desc"><p>undoes preprocessing and returns raw data by:
converting a list or array of Word IDs back to words</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def undo(self, doc):
    &#34;&#34;&#34;
    undoes preprocessing and returns raw data by:
    converting a list or array of Word IDs back to words
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ktrain.text.preprocessor.Transformer"><code class="flex name class">
<span>class <span class="ident">Transformer</span></span>
<span>(</span><span>model_name, maxlen=128, class_names=[], classes=[], batch_size=None, use_with_learner=True)</span>
</code></dt>
<dd>
<div class="desc"><p>convenience class for text classification Hugging Face transformers </p>
<h2 id="usage">Usage</h2>
<p>t = Transformer('distilbert-base-uncased', maxlen=128, classes=['neg', 'pos'], batch_size=16)
train_dataset = t.preprocess_train(train_texts, train_labels)
model = t.get_classifier()
model.fit(train_dataset)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>name of Hugging Face pretrained model</dd>
<dt><strong><code>maxlen</code></strong> :&ensp;<code>int</code></dt>
<dd>sequence length</dd>
</dl>
<p>class_names(list):
list of strings of class names (e.g., 'positive', 'negative').
The index position of string is the class ID.
Not required for:
- regression problems
- binary/multi classification problems where
labels in y_train/y_test are in string format.
In this case, classes will be populated automatically.
get_classes() can be called to view discovered class labels.
The class_names argument replaces the old classes argument.
classes(list):
alias for class_names.
Included for backwards-compatiblity.</p>
<dl>
<dt>use_with_learner(bool):
If False, preprocess_train and preprocess_test</dt>
<dt>will return tf.Datasets for direct use with model.fit</dt>
<dt>in tf.Keras.</dt>
<dt>If True, preprocess_train and preprocess_test will</dt>
<dt>return a ktrain TransformerDataset object for use with</dt>
<dt>ktrain.get_learner.</dt>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>batch_size - only required if use_with_learner=False</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Transformer(TransformersPreprocessor):
    &#34;&#34;&#34;
    convenience class for text classification Hugging Face transformers 
    Usage:
       t = Transformer(&#39;distilbert-base-uncased&#39;, maxlen=128, classes=[&#39;neg&#39;, &#39;pos&#39;], batch_size=16)
       train_dataset = t.preprocess_train(train_texts, train_labels)
       model = t.get_classifier()
       model.fit(train_dataset)
    &#34;&#34;&#34;

    def __init__(self, model_name, maxlen=128, class_names=[], classes=[],
                 batch_size=None, use_with_learner=True):
        &#34;&#34;&#34;
        Args:
            model_name (str):  name of Hugging Face pretrained model
            maxlen (int):  sequence length
            class_names(list):  list of strings of class names (e.g., &#39;positive&#39;, &#39;negative&#39;).
                                The index position of string is the class ID.
                                Not required for:
                                  - regression problems
                                  - binary/multi classification problems where
                                    labels in y_train/y_test are in string format.
                                    In this case, classes will be populated automatically.
                                    get_classes() can be called to view discovered class labels.
                                The class_names argument replaces the old classes argument.
            classes(list):  alias for class_names.  Included for backwards-compatiblity.

            use_with_learner(bool):  If False, preprocess_train and preprocess_test
                                     will return tf.Datasets for direct use with model.fit
                                     in tf.Keras.
                                     If True, preprocess_train and preprocess_test will
                                     return a ktrain TransformerDataset object for use with
                                     ktrain.get_learner.
            batch_size (int): batch_size - only required if use_with_learner=False




        &#34;&#34;&#34;
        multilabel = None # force discovery of multilabel task from data in preprocess_train-&gt;set_multilabel
        class_names = self.migrate_classes(class_names, classes)
        if not use_with_learner and batch_size is None:
            raise ValueError(&#39;batch_size is required when use_with_learner=False&#39;)
        if multilabel and (class_names is None or not class_names):
            raise ValueError(&#39;classes argument is required when multilabel=True&#39;)
        super().__init__(model_name,
                         maxlen, max_features=10000, class_names=class_names, multilabel=multilabel)
        self.batch_size = batch_size
        self.use_with_learner = use_with_learner
        self.lang = None


    def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
        &#34;&#34;&#34;
        Preprocess training set for A Transformer model

        Y values can be in one of the following forms:
        1) integers representing the class (index into array returned by get_classes)
           for binary and multiclass text classification.
           If labels are integers, class_names argument to Transformer constructor is required.
        2) strings representing the class (e.g., &#39;negative&#39;, &#39;positive&#39;).
           If labels are strings, class_names argument to Transformer constructor is ignored,
           as class labels will be extracted from y.
        3) multi-hot-encoded vector for multilabel text classification problems
           If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
        4) Numerical values for regression problems.
           &lt;class_names&gt; argument to Transformer constructor should NOT be supplied

        Args:
            texts (list of strings): text of documents
            y: labels
            mode (str):  If &#39;train&#39; and prepare_for_learner=False,
                         a tf.Dataset will be returned with repeat enabled
                         for training with fit_generator
            verbose(bool): verbosity
        Returns:
          TransformerDataset if self.use_with_learner = True else tf.Dataset
        &#34;&#34;&#34;
        tseq = super().preprocess_train(texts, y=y, mode=mode, verbose=verbose)
        if self.use_with_learner: return tseq
        tseq.batch_size = self.batch_size
        train = (mode == &#39;train&#39;)
        return tseq.to_tfdataset(train=train)


    def preprocess_test(self, texts, y=None,  verbose=1):
        &#34;&#34;&#34;
        Preprocess the validation or test set for a Transformer model
        Y values can be in one of the following forms:
        1) integers representing the class (index into array returned by get_classes)
           for binary and multiclass text classification.
           If labels are integers, class_names argument to Transformer constructor is required.
        2) strings representing the class (e.g., &#39;negative&#39;, &#39;positive&#39;).
           If labels are strings, class_names argument to Transformer constructor is ignored,
           as class labels will be extracted from y.
        3) multi-hot-encoded vector for multilabel text classification problems
           If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
        4) Numerical values for regression problems.
           &lt;class_names&gt; argument to Transformer constructor should NOT be supplied

        Args:
            texts (list of strings): text of documents
            y: labels
            verbose(bool): verbosity
        Returns:
            TransformerDataset if self.use_with_learner = True else tf.Dataset
        &#34;&#34;&#34;
        self.check_trained()
        return self.preprocess_train(texts, y=y, mode=&#39;test&#39;, verbose=verbose)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ktrain.text.preprocessor.TransformersPreprocessor" href="#ktrain.text.preprocessor.TransformersPreprocessor">TransformersPreprocessor</a></li>
<li><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></li>
<li><a title="ktrain.preprocessor.Preprocessor" href="../preprocessor.html#ktrain.preprocessor.Preprocessor">Preprocessor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.Transformer.preprocess_test"><code class="name flex">
<span>def <span class="ident">preprocess_test</span></span>(<span>self, texts, y=None, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocess the validation or test set for a Transformer model
Y values can be in one of the following forms:
1) integers representing the class (index into array returned by get_classes)
for binary and multiclass text classification.
If labels are integers, class_names argument to Transformer constructor is required.
2) strings representing the class (e.g., 'negative', 'positive').
If labels are strings, class_names argument to Transformer constructor is ignored,
as class labels will be extracted from y.
3) multi-hot-encoded vector for multilabel text classification problems
If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
4) Numerical values for regression problems.
<class_names> argument to Transformer constructor should NOT be supplied</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>texts</code></strong> :&ensp;<code>list</code> of <code>strings</code></dt>
<dd>text of documents</dd>
<dt><strong><code>y</code></strong></dt>
<dd>labels</dd>
</dl>
<p>verbose(bool): verbosity</p>
<h2 id="returns">Returns</h2>
<p>TransformerDataset if self.use_with_learner = True else tf.Dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_test(self, texts, y=None,  verbose=1):
    &#34;&#34;&#34;
    Preprocess the validation or test set for a Transformer model
    Y values can be in one of the following forms:
    1) integers representing the class (index into array returned by get_classes)
       for binary and multiclass text classification.
       If labels are integers, class_names argument to Transformer constructor is required.
    2) strings representing the class (e.g., &#39;negative&#39;, &#39;positive&#39;).
       If labels are strings, class_names argument to Transformer constructor is ignored,
       as class labels will be extracted from y.
    3) multi-hot-encoded vector for multilabel text classification problems
       If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
    4) Numerical values for regression problems.
       &lt;class_names&gt; argument to Transformer constructor should NOT be supplied

    Args:
        texts (list of strings): text of documents
        y: labels
        verbose(bool): verbosity
    Returns:
        TransformerDataset if self.use_with_learner = True else tf.Dataset
    &#34;&#34;&#34;
    self.check_trained()
    return self.preprocess_train(texts, y=y, mode=&#39;test&#39;, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.Transformer.preprocess_train"><code class="name flex">
<span>def <span class="ident">preprocess_train</span></span>(<span>self, texts, y=None, mode='train', verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocess training set for A Transformer model</p>
<p>Y values can be in one of the following forms:
1) integers representing the class (index into array returned by get_classes)
for binary and multiclass text classification.
If labels are integers, class_names argument to Transformer constructor is required.
2) strings representing the class (e.g., 'negative', 'positive').
If labels are strings, class_names argument to Transformer constructor is ignored,
as class labels will be extracted from y.
3) multi-hot-encoded vector for multilabel text classification problems
If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
4) Numerical values for regression problems.
<class_names> argument to Transformer constructor should NOT be supplied</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>texts</code></strong> :&ensp;<code>list</code> of <code>strings</code></dt>
<dd>text of documents</dd>
<dt><strong><code>y</code></strong></dt>
<dd>labels</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>str</code></dt>
<dd>If 'train' and prepare_for_learner=False,
a tf.Dataset will be returned with repeat enabled
for training with fit_generator</dd>
</dl>
<p>verbose(bool): verbosity</p>
<h2 id="returns">Returns</h2>
<p>TransformerDataset if self.use_with_learner = True else tf.Dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
    &#34;&#34;&#34;
    Preprocess training set for A Transformer model

    Y values can be in one of the following forms:
    1) integers representing the class (index into array returned by get_classes)
       for binary and multiclass text classification.
       If labels are integers, class_names argument to Transformer constructor is required.
    2) strings representing the class (e.g., &#39;negative&#39;, &#39;positive&#39;).
       If labels are strings, class_names argument to Transformer constructor is ignored,
       as class labels will be extracted from y.
    3) multi-hot-encoded vector for multilabel text classification problems
       If labels are multi-hot-encoded, class_names argument to Transformer constructor is requird.
    4) Numerical values for regression problems.
       &lt;class_names&gt; argument to Transformer constructor should NOT be supplied

    Args:
        texts (list of strings): text of documents
        y: labels
        mode (str):  If &#39;train&#39; and prepare_for_learner=False,
                     a tf.Dataset will be returned with repeat enabled
                     for training with fit_generator
        verbose(bool): verbosity
    Returns:
      TransformerDataset if self.use_with_learner = True else tf.Dataset
    &#34;&#34;&#34;
    tseq = super().preprocess_train(texts, y=y, mode=mode, verbose=verbose)
    if self.use_with_learner: return tseq
    tseq.batch_size = self.batch_size
    train = (mode == &#39;train&#39;)
    return tseq.to_tfdataset(train=train)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ktrain.text.preprocessor.TransformersPreprocessor" href="#ktrain.text.preprocessor.TransformersPreprocessor">TransformersPreprocessor</a></b></code>:
<ul class="hlist">
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_classifier" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_classifier">get_classifier</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_regression_model" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_regression_model">get_regression_model</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.load_model_and_configure_from_data" href="#ktrain.text.preprocessor.TransformersPreprocessor.load_model_and_configure_from_data">load_model_and_configure_from_data</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.print_seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats">print_seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.seqlen_stats">seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.undo" href="#ktrain.text.preprocessor.TextPreprocessor.undo">undo</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ktrain.text.preprocessor.TransformerDataset"><code class="flex name class">
<span>class <span class="ident">TransformerDataset</span></span>
<span>(</span><span>x, y, batch_size=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for Transformer datasets.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformerDataset(SequenceDataset):
    &#34;&#34;&#34;
    Wrapper for Transformer datasets.
    &#34;&#34;&#34;

    def __init__(self, x, y, batch_size=1):
        if type(x) not in [list, np.ndarray]: raise ValueError(&#39;x must be list or np.ndarray&#39;)
        if type(y) not in [list, np.ndarray]: raise ValueError(&#39;y must be list or np.ndarray&#39;)
        if type(x) == list: x = np.array(x)
        if type(y) == list: y = np.array(y)
        self.x = x
        self.y = y
        self.batch_size = batch_size


    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]
        batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]
        return (batch_x, batch_y)


    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)


    def to_tfdataset(self, train=True):
        &#34;&#34;&#34;
        convert transformer features to tf.Dataset
        &#34;&#34;&#34;
        if train:
            shuffle=True
            repeat = True
        else:
            shuffle=False
            repeat=False

        if len(self.y.shape) == 1:
            yshape = []
            ytype = tf.float32
        else:
            yshape = [None]
            ytype = tf.int64

        def gen():
            for idx, data in enumerate(self.x):
                yield ({&#39;input_ids&#39;: data[0],
                         &#39;attention_mask&#39;: data[1],
                         &#39;token_type_ids&#39;: data[2]},
                        self.y[idx])

        tfdataset= tf.data.Dataset.from_generator(gen,
            ({&#39;input_ids&#39;: tf.int32,
              &#39;attention_mask&#39;: tf.int32,
              &#39;token_type_ids&#39;: tf.int32},
             ytype),
            ({&#39;input_ids&#39;: tf.TensorShape([None]),
              &#39;attention_mask&#39;: tf.TensorShape([None]),
              &#39;token_type_ids&#39;: tf.TensorShape([None])},
             tf.TensorShape(yshape)))

        if shuffle:
            tfdataset = tfdataset.shuffle(self.x.shape[0])
        tfdataset = tfdataset.batch(self.batch_size)
        if repeat:
            tfdataset = tfdataset.repeat(-1)
        return tfdataset


    def get_y(self):
        return self.y

    def nsamples(self):
        return len(self.x)

    def nclasses(self):
        return self.y.shape[1]

    def xshape(self):
        return (len(self.x), self.x[0].shape[1])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ktrain.data.SequenceDataset" href="../data.html#ktrain.data.SequenceDataset">SequenceDataset</a></li>
<li><a title="ktrain.data.Dataset" href="../data.html#ktrain.data.Dataset">Dataset</a></li>
<li>tensorflow.python.keras.utils.data_utils.Sequence</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.TransformerDataset.get_y"><code class="name flex">
<span>def <span class="ident">get_y</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_y(self):
    return self.y</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformerDataset.nsamples"><code class="name flex">
<span>def <span class="ident">nsamples</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nsamples(self):
    return len(self.x)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformerDataset.to_tfdataset"><code class="name flex">
<span>def <span class="ident">to_tfdataset</span></span>(<span>self, train=True)</span>
</code></dt>
<dd>
<div class="desc"><p>convert transformer features to tf.Dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_tfdataset(self, train=True):
    &#34;&#34;&#34;
    convert transformer features to tf.Dataset
    &#34;&#34;&#34;
    if train:
        shuffle=True
        repeat = True
    else:
        shuffle=False
        repeat=False

    if len(self.y.shape) == 1:
        yshape = []
        ytype = tf.float32
    else:
        yshape = [None]
        ytype = tf.int64

    def gen():
        for idx, data in enumerate(self.x):
            yield ({&#39;input_ids&#39;: data[0],
                     &#39;attention_mask&#39;: data[1],
                     &#39;token_type_ids&#39;: data[2]},
                    self.y[idx])

    tfdataset= tf.data.Dataset.from_generator(gen,
        ({&#39;input_ids&#39;: tf.int32,
          &#39;attention_mask&#39;: tf.int32,
          &#39;token_type_ids&#39;: tf.int32},
         ytype),
        ({&#39;input_ids&#39;: tf.TensorShape([None]),
          &#39;attention_mask&#39;: tf.TensorShape([None]),
          &#39;token_type_ids&#39;: tf.TensorShape([None])},
         tf.TensorShape(yshape)))

    if shuffle:
        tfdataset = tfdataset.shuffle(self.x.shape[0])
    tfdataset = tfdataset.batch(self.batch_size)
    if repeat:
        tfdataset = tfdataset.repeat(-1)
    return tfdataset</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ktrain.data.SequenceDataset" href="../data.html#ktrain.data.SequenceDataset">SequenceDataset</a></b></code>:
<ul class="hlist">
<li><code><a title="ktrain.data.SequenceDataset.nclasses" href="../data.html#ktrain.data.Dataset.nclasses">nclasses</a></code></li>
<li><code><a title="ktrain.data.SequenceDataset.ondisk" href="../data.html#ktrain.data.Dataset.ondisk">ondisk</a></code></li>
<li><code><a title="ktrain.data.SequenceDataset.xshape" href="../data.html#ktrain.data.Dataset.xshape">xshape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="ktrain.text.preprocessor.TransformerEmbedding"><code class="flex name class">
<span>class <span class="ident">TransformerEmbedding</span></span>
<span>(</span><span>model_name, layers=[-2])</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_name</code></strong> :&ensp;<code>str</code></dt>
<dd>name of Hugging Face pretrained model.
Choose from here: <a href="https://huggingface.co/transformers/pretrained_models.html">https://huggingface.co/transformers/pretrained_models.html</a></dd>
</dl>
<p>layers(list): list of indexes indicating which hidden layers to use when
constructing the embedding (e.g., last=[-1])</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformerEmbedding():
    def __init__(self, model_name, layers=U.DEFAULT_TRANSFORMER_LAYERS):
        &#34;&#34;&#34;
        Args:
            model_name (str):  name of Hugging Face pretrained model.
                               Choose from here: https://huggingface.co/transformers/pretrained_models.html
            layers(list): list of indexes indicating which hidden layers to use when
                          constructing the embedding (e.g., last=[-1])
                               
        &#34;&#34;&#34;
        self.layers = layers
        self.model_name = model_name
        if model_name.startswith(&#39;xlm-roberta&#39;):
            self.name = &#39;xlm_roberta&#39;
        else:
            self.name = model_name.split(&#39;-&#39;)[0]

        self.config = AutoConfig.from_pretrained(model_name)
        self.model_type = TFAutoModel
        self.tokenizer_type = AutoTokenizer

        if &#34;bert-base-japanese&#34; in model_name:
            self.tokenizer_type = transformers.BertJapaneseTokenizer

        self.tokenizer = self.tokenizer_type.from_pretrained(model_name)
        self.model = self._load_pretrained(model_name)
        try:
            self.embsize = self.embed(&#39;ktrain&#39;, word_level=False).shape[1] # (batch_size, embsize)
        except:
            warnings.warn(&#39;could not determine Embedding size&#39;)
        if type(self.model).__name__ not in [&#39;TFBertModel&#39;, &#39;TFDistilBertModel&#39;, &#39;TFAlbertModel&#39;]:
            raise ValueError(&#39;TransformerEmbedding class currently only supports BERT-style models: &#39; +\
                             &#39;Bert, DistilBert, and Albert and variants like BioBERT and SciBERT\n\n&#39; +\
                             &#39;model received: %s (%s))&#39; % (type(self.model).__name__, model_name))


    def _load_pretrained(self, model_name):
        &#34;&#34;&#34;
        load pretrained model
        &#34;&#34;&#34;
        if self.config is not None:
            self.config.output_hidden_states = True
            try:
                model = self.model_type.from_pretrained(model_name, config=self.config)
            except:
                warnings.warn(&#39;Could not find Tensorflow version of model.  Attempting to download/load PyTorch version as TensorFlow model using from_pt=True. &#39; +\
                              &#39;You will need PyTorch installed for this.&#39;)
                try:
                    model = self.model_type.from_pretrained(model_name, config=self.config, from_pt=True)
                except:
                    raise ValueError(&#39;could not load pretrained model %s using both from_pt=False and from_pt=True&#39; % (model_name))
        else:
            model = self.model_type.from_pretrained(model_name, output_hidden_states=True)
        return model



    def embed(self, texts, word_level=True, max_length=512):
        &#34;&#34;&#34;
        get embedding for word, phrase, or sentence
        Args:
          text(str|list): word, phrase, or sentence or list of them representing a batch
          word_level(bool): If True, returns embedding for each token in supplied texts.
                            If False, returns embedding for each text in texts
          max_length(int): max length of tokens
        Returns:
            np.ndarray : embeddings
        &#34;&#34;&#34;
        if isinstance(texts, str): texts = [texts]
        if not isinstance(texts[0], str): texts = [&#34; &#34;.join(text) for text in texts]

        sentences = []
        for text in texts:
            sentences.append(self.tokenizer.tokenize(text))
        maxlen = len(max([tokens for tokens in sentences], key=len,)) + 2
        if max_length is not None and maxlen &gt; max_length: maxlen = max_length # added due to issue #270
        sentences = []

        all_input_ids = []
        all_input_masks = []
        for text in texts:
            tokens = self.tokenizer.tokenize(text)
            if len(tokens) &gt; maxlen - 2:
                tokens = tokens[0 : (maxlen - 2)]
            sentences.append(tokens)
            tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]
            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
            input_mask = [1] * len(input_ids)
            while len(input_ids) &lt; maxlen:
                input_ids.append(0)
                input_mask.append(0)
            all_input_ids.append(input_ids)
            all_input_masks.append(input_mask)

        all_input_ids = np.array(all_input_ids)
        all_input_masks = np.array(all_input_masks)
        outputs = self.model(all_input_ids, attention_mask=all_input_masks)
        hidden_states = outputs[-1] # output_hidden_states=True

        # compile raw embeddings
        if len(self.layers) == 1:
            #raw_embeddings = hidden_states[-1].numpy()
            raw_embeddings = hidden_states[self.layers[0]].numpy()
        else:
            raw_embeddings = []
            for batch_id in range(hidden_states[0].shape[0]):
                token_embeddings = []
                for token_id in range(hidden_states[0].shape[1]):
                    all_layers = []
                    for layer_id in self.layers:
                        all_layers.append(hidden_states[layer_id][batch_id][token_id].numpy())
                    token_embeddings.append(np.concatenate(all_layers) )  
                raw_embeddings.append(token_embeddings)
            raw_embeddings = np.array(raw_embeddings)

        if not word_level: # sentence-level embedding
            return np.mean(raw_embeddings, axis=1)
            #return np.squeeze(raw_embeddings[:,0:1,:], axis=1)

        # filter-out extra subword tokens and special tokens 
        # (using first subword of each token as embedding representations)
        filtered_embeddings = []
        for batch_idx, tokens in enumerate(sentences):
            embedding = []
            for token_idx, token in enumerate(tokens):
                if token in [self.tokenizer.cls_token, self.tokenizer.sep_token] or token.startswith(&#39;##&#39;): continue
                embedding.append(raw_embeddings[batch_idx][token_idx])
            filtered_embeddings.append(embedding)

        # pad embeddings with zeros
        max_length = max([len(e) for e in filtered_embeddings])
        embeddings = []
        for e in filtered_embeddings:
            for i in range(max_length-len(e)):
                e.append(np.zeros((self.embsize,)))
            embeddings.append(np.array(e))
        return np.array(embeddings)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.TransformerEmbedding.embed"><code class="name flex">
<span>def <span class="ident">embed</span></span>(<span>self, texts, word_level=True, max_length=512)</span>
</code></dt>
<dd>
<div class="desc"><p>get embedding for word, phrase, or sentence</p>
<h2 id="args">Args</h2>
<p>text(str|list): word, phrase, or sentence or list of them representing a batch
word_level(bool): If True, returns embedding for each token in supplied texts.
If False, returns embedding for each text in texts
max_length(int): max length of tokens</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray </code></dt>
<dd>embeddings</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed(self, texts, word_level=True, max_length=512):
    &#34;&#34;&#34;
    get embedding for word, phrase, or sentence
    Args:
      text(str|list): word, phrase, or sentence or list of them representing a batch
      word_level(bool): If True, returns embedding for each token in supplied texts.
                        If False, returns embedding for each text in texts
      max_length(int): max length of tokens
    Returns:
        np.ndarray : embeddings
    &#34;&#34;&#34;
    if isinstance(texts, str): texts = [texts]
    if not isinstance(texts[0], str): texts = [&#34; &#34;.join(text) for text in texts]

    sentences = []
    for text in texts:
        sentences.append(self.tokenizer.tokenize(text))
    maxlen = len(max([tokens for tokens in sentences], key=len,)) + 2
    if max_length is not None and maxlen &gt; max_length: maxlen = max_length # added due to issue #270
    sentences = []

    all_input_ids = []
    all_input_masks = []
    for text in texts:
        tokens = self.tokenizer.tokenize(text)
        if len(tokens) &gt; maxlen - 2:
            tokens = tokens[0 : (maxlen - 2)]
        sentences.append(tokens)
        tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]
        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        input_mask = [1] * len(input_ids)
        while len(input_ids) &lt; maxlen:
            input_ids.append(0)
            input_mask.append(0)
        all_input_ids.append(input_ids)
        all_input_masks.append(input_mask)

    all_input_ids = np.array(all_input_ids)
    all_input_masks = np.array(all_input_masks)
    outputs = self.model(all_input_ids, attention_mask=all_input_masks)
    hidden_states = outputs[-1] # output_hidden_states=True

    # compile raw embeddings
    if len(self.layers) == 1:
        #raw_embeddings = hidden_states[-1].numpy()
        raw_embeddings = hidden_states[self.layers[0]].numpy()
    else:
        raw_embeddings = []
        for batch_id in range(hidden_states[0].shape[0]):
            token_embeddings = []
            for token_id in range(hidden_states[0].shape[1]):
                all_layers = []
                for layer_id in self.layers:
                    all_layers.append(hidden_states[layer_id][batch_id][token_id].numpy())
                token_embeddings.append(np.concatenate(all_layers) )  
            raw_embeddings.append(token_embeddings)
        raw_embeddings = np.array(raw_embeddings)

    if not word_level: # sentence-level embedding
        return np.mean(raw_embeddings, axis=1)
        #return np.squeeze(raw_embeddings[:,0:1,:], axis=1)

    # filter-out extra subword tokens and special tokens 
    # (using first subword of each token as embedding representations)
    filtered_embeddings = []
    for batch_idx, tokens in enumerate(sentences):
        embedding = []
        for token_idx, token in enumerate(tokens):
            if token in [self.tokenizer.cls_token, self.tokenizer.sep_token] or token.startswith(&#39;##&#39;): continue
            embedding.append(raw_embeddings[batch_idx][token_idx])
        filtered_embeddings.append(embedding)

    # pad embeddings with zeros
    max_length = max([len(e) for e in filtered_embeddings])
    embeddings = []
    for e in filtered_embeddings:
        for i in range(max_length-len(e)):
            e.append(np.zeros((self.embsize,)))
        embeddings.append(np.array(e))
    return np.array(embeddings)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor"><code class="flex name class">
<span>class <span class="ident">TransformersPreprocessor</span></span>
<span>(</span><span>model_name, maxlen, max_features, class_names=[], classes=[], lang='en', ngram_range=1, multilabel=None)</span>
</code></dt>
<dd>
<div class="desc"><p>text preprocessing for Hugging Face Transformer models</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformersPreprocessor(TextPreprocessor):
    &#34;&#34;&#34;
    text preprocessing for Hugging Face Transformer models
    &#34;&#34;&#34;

    def __init__(self,  model_name,
                maxlen, max_features, class_names=[], classes=[], 
                lang=&#39;en&#39;, ngram_range=1, multilabel=None):
        class_names = self.migrate_classes(class_names, classes)

        if maxlen &gt; 512: raise ValueError(&#39;Transformer models only supports maxlen &lt;= 512&#39;)

        super().__init__(maxlen, class_names, lang=lang, multilabel=multilabel)

        self.model_name = model_name
        self.name = model_name.split(&#39;-&#39;)[0]
        if model_name.startswith(&#39;xlm-roberta&#39;): 
            self.name = &#39;xlm_roberta&#39;
            self.model_name = &#39;jplu/tf-&#39; + self.model_name
        else:
            self.name = model_name.split(&#39;-&#39;)[0]
        self.config = AutoConfig.from_pretrained(model_name)
        self.model_type = TFAutoModelForSequenceClassification
        self.tokenizer_type = AutoTokenizer

        if &#34;bert-base-japanese&#34; in model_name:
            self.tokenizer_type = transformers.BertJapaneseTokenizer

        # NOTE: As of v0.16.1, do not unnecessarily instantiate tokenizer
        # as it will be saved/pickled along with Preprocessor, which causes
        # problems for some community-uploaded models like bert-base-japanse-whole-word-masking.
        #tokenizer = self.tokenizer_type.from_pretrained(model_name)
        #self.tok = tokenizer
        self.tok = None # not pickled,  see __getstate__ 

        self.tok_dct = None
        self.max_features = max_features # ignored
        self.ngram_range = 1 # ignored


    def __getstate__(self):
        return {k: v for k, v in self.__dict__.items() if k not in [&#39;tok&#39;]}


    def __setstate__(self, state):
        &#34;&#34;&#34;
        For backwards compatibility with previous versions of ktrain
        that saved tokenizer and did not use ytransform
        &#34;&#34;&#34;
        self.__dict__.update(state)
        if not hasattr(self, &#39;tok&#39;): self.tok = None
        if not hasattr(self, &#39;ytransform&#39;): 
            le = self.label_encoder if hasattr(self, &#39;label_encoder&#39;) else None
            self.ytransform = U.YTransform(class_names=self.get_classes(), label_encoder=le)

    def set_config(self, config):
        self.config = config

    def get_config(self):
        return self.config

    def set_tokenizer(self, tokenizer):
        self.tok = tokenizer

    def get_tokenizer(self, fpath=None):
        model_name = self.model_name if fpath is None else fpath
        if self.tok is None:
            try:
                # use fast tokenizer if possible
                if self.name == &#39;bert&#39; and &#39;japanese&#39; not in model_name:
                    from transformers import BertTokenizerFast
                    self.tok = BertTokenizerFast.from_pretrained(model_name)
                elif self.name == &#39;distilbert&#39;:
                    from transformers import DistilBertTokenizerFast
                    self.tok = DistilBertTokenizerFast.from_pretrained(model_name)
                elif self.name == &#39;roberta&#39;:
                    from transformers import RobertaTokenizerFast
                    self.tok = RobertaTokenizerFast.from_pretrained(model_name)
                else:
                    self.tok = self.tokenizer_type.from_pretrained(model_name)
            except:
                error_msg = f&#34;Could not load tokenizer from model_name: {model_name}. &#34; +\
                            f&#34;If {model_name} is a local path, please make sure it exists and contains tokenizer files from Hugging Face. &#34; +\
                            f&#34;You can also reset model_name with preproc.model_name = &#39;/your/new/path&#39;.&#34;
                raise ValueError(error_msg)
        return self.tok


    def save_tokenizer(self, fpath):
        if os.path.isfile(fpath):
            raise ValueError(f&#39;There is an existing file named {fpath}. &#39; +\
                              &#39;Please use dfferent value for fpath.&#39;)
        elif os.path.exists(fpath):
            pass
        elif not os.path.exists(fpath):
            os.makedirs(fpath)
        tok =self.get_tokenizer()
        tok.save_pretrained(fpath)
        return



    def get_preprocessor(self):
        return (self.get_tokenizer(), self.tok_dct)



    def preprocess(self, texts):
        tseq = self.preprocess_test(texts, verbose=0)
        return tseq.to_tfdataset(train=False)


    def undo(self, doc):
        &#34;&#34;&#34;
        undoes preprocessing and returns raw data by:
        converting a list or array of Word IDs back to words
        &#34;&#34;&#34;
        tok, _ = self.get_preprocessor()
        return self.tok.convert_ids_to_tokens(doc)
        #raise Exception(&#39;currently_unsupported: Transformers.Preprocessor.undo is not yet supported&#39;)


    def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
        &#34;&#34;&#34;
        preprocess training set
        &#34;&#34;&#34;

        U.vprint(&#39;preprocessing %s...&#39; % (mode), verbose=verbose)
        U.check_array(texts, y=y, X_name=&#39;texts&#39;)

        # detect sentence pairs
        is_array, is_pair = detect_text_format(texts)
        if not is_array: raise ValueError(&#39;texts must be a list of strings or a list of sentence pairs&#39;)

        # detect language
        if self.lang is None and mode==&#39;train&#39;: self.lang = TU.detect_lang(texts)
        U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

        # print stats
        if not is_pair: self.print_seqlen_stats(texts, mode, verbose=verbose)
        if is_pair: U.vprint(&#39;sentence pairs detected&#39;, verbose=verbose)

        # transform y
        if y is None and mode == &#39;train&#39;:
            raise ValueError(&#39;y is required for training sets&#39;)
        elif y is None:
            y = np.array([1] * len(texts))
        y = self._transform_y(y, train=mode==&#39;train&#39;, verbose=verbose)

        # convert examples
        tok, _ = self.get_preprocessor()
        dataset = hf_convert_examples(texts, y=y, tokenizer=tok, max_length=self.maxlen,
                                      pad_on_left=bool(self.name in [&#39;xlnet&#39;]),
                                      pad_token=tok.convert_tokens_to_ids([tok.pad_token][0]),
                                      pad_token_segment_id=4 if self.name in [&#39;xlnet&#39;] else 0,
                                      use_dynamic_shape=False if mode == &#39;train&#39; else True,
                                      verbose=verbose)
        self.set_multilabel(dataset, mode, verbose=verbose)
        if mode == &#39;train&#39;:  self.preprocess_train_called = True
        return dataset



    def preprocess_test(self, texts, y=None, mode=&#39;test&#39;, verbose=1):
        self.check_trained()
        return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)


    @classmethod
    def load_model_and_configure_from_data(cls, fpath, transformer_ds):
        &#34;&#34;&#34;
        loads model from file path and configures loss function and metrics automatically
        based on inspecting data
        Args:
          fpath(str): path to model folder
          transformer_ds(TransformerDataset): an instance of TransformerDataset
        &#34;&#34;&#34;
        is_regression = U.is_regression_from_data(transformer_ds)
        multilabel = U.is_multilabel(transformer_ds)
        model = TFAutoModelForSequenceClassification.from_pretrained(fpath)
        if is_regression:
            metrics = [&#39;mae&#39;]
            loss_fn = &#39;mse&#39;
        else:
            metrics = [&#39;accuracy&#39;]
            if multilabel:
                loss_fn =  keras.losses.BinaryCrossentropy(from_logits=True)
            else:
                loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)
        model.compile(loss=loss_fn,
                      optimizer=U.DEFAULT_OPT,
                      metrics=metrics)
        return model



    def _load_pretrained(self, mname, num_labels):
        &#34;&#34;&#34;
        load pretrained model
        &#34;&#34;&#34;
        if self.config is not None:
            self.config.num_labels = num_labels
            try:
                model = self.model_type.from_pretrained(mname, config=self.config)
            except:
                warnings.warn(&#39;Could not find Tensorflow version of model.  Attempting to download/load PyTorch version as TensorFlow model using from_pt=True. &#39; +\
                              &#39;You will need PyTorch installed for this.&#39;)
                try:
                    model = self.model_type.from_pretrained(mname, config=self.config, from_pt=True)
                except:
                    # load model as normal to expose error to user
                    model = self.model_type.from_pretrained(mname, config=self.config)
                    #raise ValueError(&#39;could not load pretrained model %s using both from_pt=False and from_pt=True&#39; % (mname))
        else:
            model = self.model_type.from_pretrained(mname, num_labels=num_labels)
        return model



    def get_classifier(self, fpath=None, multilabel=None, metrics=[&#39;accuracy&#39;]):
        &#34;&#34;&#34;
        creates a model for text classification
        Args:
          fpath(str): optional path to saved pretrained model. Typically left as None.
          multilabel(bool): If None, multilabel status is discovered from data [recommended].
                            If True, model will be forcibly configured for multilabel task.
                            If False, model will be forcibly configured for non-multilabel task.
                            It is recommended to leave this as None.
          metrics(list): metrics to use
        &#34;&#34;&#34;
        self.check_trained()
        if not self.get_classes():
            warnings.warn(&#39;no class labels were provided - treating as regression&#39;)
            return self.get_regression_model()

        # process multilabel task
        multilabel = self.multilabel if multilabel is None else multilabel
        if multilabel is True and self.multilabel is False:
            warnings.warn(&#39;The multilabel=True argument was supplied, but labels do not indicate &#39;+\
                          &#39;a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.&#39;)
        elif multilabel is False and self.multilabel is True:
                warnings.warn(&#39;The multilabel=False argument was supplied, but labels inidcate that  &#39;+\
                              &#39;this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.&#39;)

        # setup model
        num_labels = len(self.get_classes())
        mname = fpath if fpath is not None else self.model_name
        model = self._load_pretrained(mname, num_labels)
        if multilabel:
            loss_fn =  keras.losses.BinaryCrossentropy(from_logits=True)
        else:
            loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)
        model.compile(loss=loss_fn,
                      optimizer=U.DEFAULT_OPT,
                      metrics=metrics)
        return model


    def get_regression_model(self, fpath=None, metrics=[&#39;mae&#39;]):
        &#34;&#34;&#34;
        creates a model for text regression
        Args:
          fpath(str): optional path to saved pretrained model. Typically left as None.
          metrics(list): metrics to use
        &#34;&#34;&#34;
        self.check_trained()
        if self.get_classes():
            warnings.warn(&#39;class labels were provided - treating as classification problem&#39;)
            return self.get_classifier()
        num_labels = 1
        mname = fpath if fpath is not None else self.model_name
        model = self._load_pretrained(mname, num_labels)
        loss_fn = &#39;mse&#39;
        model.compile(loss=loss_fn,
                      optimizer=U.DEFAULT_OPT,
                      metrics=metrics)
        return model


    def get_model(self, fpath=None):
        self.check_trained()
        if not self.get_classes():
            return self.get_regression_model(fpath=fpath)
        else:
            return self.get_classifier(fpath=fpath)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></li>
<li><a title="ktrain.preprocessor.Preprocessor" href="../preprocessor.html#ktrain.preprocessor.Preprocessor">Preprocessor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ktrain.text.preprocessor.DistilBertPreprocessor" href="#ktrain.text.preprocessor.DistilBertPreprocessor">DistilBertPreprocessor</a></li>
<li><a title="ktrain.text.preprocessor.Transformer" href="#ktrain.text.preprocessor.Transformer">Transformer</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.load_model_and_configure_from_data"><code class="name flex">
<span>def <span class="ident">load_model_and_configure_from_data</span></span>(<span>fpath, transformer_ds)</span>
</code></dt>
<dd>
<div class="desc"><p>loads model from file path and configures loss function and metrics automatically
based on inspecting data</p>
<h2 id="args">Args</h2>
<p>fpath(str): path to model folder
transformer_ds(TransformerDataset): an instance of TransformerDataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load_model_and_configure_from_data(cls, fpath, transformer_ds):
    &#34;&#34;&#34;
    loads model from file path and configures loss function and metrics automatically
    based on inspecting data
    Args:
      fpath(str): path to model folder
      transformer_ds(TransformerDataset): an instance of TransformerDataset
    &#34;&#34;&#34;
    is_regression = U.is_regression_from_data(transformer_ds)
    multilabel = U.is_multilabel(transformer_ds)
    model = TFAutoModelForSequenceClassification.from_pretrained(fpath)
    if is_regression:
        metrics = [&#39;mae&#39;]
        loss_fn = &#39;mse&#39;
    else:
        metrics = [&#39;accuracy&#39;]
        if multilabel:
            loss_fn =  keras.losses.BinaryCrossentropy(from_logits=True)
        else:
            loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)
    model.compile(loss=loss_fn,
                  optimizer=U.DEFAULT_OPT,
                  metrics=metrics)
    return model</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.get_classifier"><code class="name flex">
<span>def <span class="ident">get_classifier</span></span>(<span>self, fpath=None, multilabel=None, metrics=['accuracy'])</span>
</code></dt>
<dd>
<div class="desc"><p>creates a model for text classification</p>
<h2 id="args">Args</h2>
<p>fpath(str): optional path to saved pretrained model. Typically left as None.
multilabel(bool): If None, multilabel status is discovered from data [recommended].
If True, model will be forcibly configured for multilabel task.
If False, model will be forcibly configured for non-multilabel task.
It is recommended to leave this as None.
metrics(list): metrics to use</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_classifier(self, fpath=None, multilabel=None, metrics=[&#39;accuracy&#39;]):
    &#34;&#34;&#34;
    creates a model for text classification
    Args:
      fpath(str): optional path to saved pretrained model. Typically left as None.
      multilabel(bool): If None, multilabel status is discovered from data [recommended].
                        If True, model will be forcibly configured for multilabel task.
                        If False, model will be forcibly configured for non-multilabel task.
                        It is recommended to leave this as None.
      metrics(list): metrics to use
    &#34;&#34;&#34;
    self.check_trained()
    if not self.get_classes():
        warnings.warn(&#39;no class labels were provided - treating as regression&#39;)
        return self.get_regression_model()

    # process multilabel task
    multilabel = self.multilabel if multilabel is None else multilabel
    if multilabel is True and self.multilabel is False:
        warnings.warn(&#39;The multilabel=True argument was supplied, but labels do not indicate &#39;+\
                      &#39;a multilabel problem (labels appear to be mutually-exclusive).  Using multilabel=True anyways.&#39;)
    elif multilabel is False and self.multilabel is True:
            warnings.warn(&#39;The multilabel=False argument was supplied, but labels inidcate that  &#39;+\
                          &#39;this is a multilabel problem (labels are not mutually-exclusive).  Using multilabel=False anyways.&#39;)

    # setup model
    num_labels = len(self.get_classes())
    mname = fpath if fpath is not None else self.model_name
    model = self._load_pretrained(mname, num_labels)
    if multilabel:
        loss_fn =  keras.losses.BinaryCrossentropy(from_logits=True)
    else:
        loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)
    model.compile(loss=loss_fn,
                  optimizer=U.DEFAULT_OPT,
                  metrics=metrics)
    return model</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    return self.config</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>self, fpath=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model(self, fpath=None):
    self.check_trained()
    if not self.get_classes():
        return self.get_regression_model(fpath=fpath)
    else:
        return self.get_classifier(fpath=fpath)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.get_preprocessor"><code class="name flex">
<span>def <span class="ident">get_preprocessor</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_preprocessor(self):
    return (self.get_tokenizer(), self.tok_dct)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.get_regression_model"><code class="name flex">
<span>def <span class="ident">get_regression_model</span></span>(<span>self, fpath=None, metrics=['mae'])</span>
</code></dt>
<dd>
<div class="desc"><p>creates a model for text regression</p>
<h2 id="args">Args</h2>
<p>fpath(str): optional path to saved pretrained model. Typically left as None.
metrics(list): metrics to use</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_regression_model(self, fpath=None, metrics=[&#39;mae&#39;]):
    &#34;&#34;&#34;
    creates a model for text regression
    Args:
      fpath(str): optional path to saved pretrained model. Typically left as None.
      metrics(list): metrics to use
    &#34;&#34;&#34;
    self.check_trained()
    if self.get_classes():
        warnings.warn(&#39;class labels were provided - treating as classification problem&#39;)
        return self.get_classifier()
    num_labels = 1
    mname = fpath if fpath is not None else self.model_name
    model = self._load_pretrained(mname, num_labels)
    loss_fn = &#39;mse&#39;
    model.compile(loss=loss_fn,
                  optimizer=U.DEFAULT_OPT,
                  metrics=metrics)
    return model</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.get_tokenizer"><code class="name flex">
<span>def <span class="ident">get_tokenizer</span></span>(<span>self, fpath=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tokenizer(self, fpath=None):
    model_name = self.model_name if fpath is None else fpath
    if self.tok is None:
        try:
            # use fast tokenizer if possible
            if self.name == &#39;bert&#39; and &#39;japanese&#39; not in model_name:
                from transformers import BertTokenizerFast
                self.tok = BertTokenizerFast.from_pretrained(model_name)
            elif self.name == &#39;distilbert&#39;:
                from transformers import DistilBertTokenizerFast
                self.tok = DistilBertTokenizerFast.from_pretrained(model_name)
            elif self.name == &#39;roberta&#39;:
                from transformers import RobertaTokenizerFast
                self.tok = RobertaTokenizerFast.from_pretrained(model_name)
            else:
                self.tok = self.tokenizer_type.from_pretrained(model_name)
        except:
            error_msg = f&#34;Could not load tokenizer from model_name: {model_name}. &#34; +\
                        f&#34;If {model_name} is a local path, please make sure it exists and contains tokenizer files from Hugging Face. &#34; +\
                        f&#34;You can also reset model_name with preproc.model_name = &#39;/your/new/path&#39;.&#34;
            raise ValueError(error_msg)
    return self.tok</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>self, texts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess(self, texts):
    tseq = self.preprocess_test(texts, verbose=0)
    return tseq.to_tfdataset(train=False)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.preprocess_test"><code class="name flex">
<span>def <span class="ident">preprocess_test</span></span>(<span>self, texts, y=None, mode='test', verbose=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_test(self, texts, y=None, mode=&#39;test&#39;, verbose=1):
    self.check_trained()
    return self.preprocess_train(texts, y=y, mode=mode, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.preprocess_train"><code class="name flex">
<span>def <span class="ident">preprocess_train</span></span>(<span>self, texts, y=None, mode='train', verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>preprocess training set</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_train(self, texts, y=None, mode=&#39;train&#39;, verbose=1):
    &#34;&#34;&#34;
    preprocess training set
    &#34;&#34;&#34;

    U.vprint(&#39;preprocessing %s...&#39; % (mode), verbose=verbose)
    U.check_array(texts, y=y, X_name=&#39;texts&#39;)

    # detect sentence pairs
    is_array, is_pair = detect_text_format(texts)
    if not is_array: raise ValueError(&#39;texts must be a list of strings or a list of sentence pairs&#39;)

    # detect language
    if self.lang is None and mode==&#39;train&#39;: self.lang = TU.detect_lang(texts)
    U.vprint(&#39;language: %s&#39; % (self.lang), verbose=verbose)

    # print stats
    if not is_pair: self.print_seqlen_stats(texts, mode, verbose=verbose)
    if is_pair: U.vprint(&#39;sentence pairs detected&#39;, verbose=verbose)

    # transform y
    if y is None and mode == &#39;train&#39;:
        raise ValueError(&#39;y is required for training sets&#39;)
    elif y is None:
        y = np.array([1] * len(texts))
    y = self._transform_y(y, train=mode==&#39;train&#39;, verbose=verbose)

    # convert examples
    tok, _ = self.get_preprocessor()
    dataset = hf_convert_examples(texts, y=y, tokenizer=tok, max_length=self.maxlen,
                                  pad_on_left=bool(self.name in [&#39;xlnet&#39;]),
                                  pad_token=tok.convert_tokens_to_ids([tok.pad_token][0]),
                                  pad_token_segment_id=4 if self.name in [&#39;xlnet&#39;] else 0,
                                  use_dynamic_shape=False if mode == &#39;train&#39; else True,
                                  verbose=verbose)
    self.set_multilabel(dataset, mode, verbose=verbose)
    if mode == &#39;train&#39;:  self.preprocess_train_called = True
    return dataset</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.save_tokenizer"><code class="name flex">
<span>def <span class="ident">save_tokenizer</span></span>(<span>self, fpath)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_tokenizer(self, fpath):
    if os.path.isfile(fpath):
        raise ValueError(f&#39;There is an existing file named {fpath}. &#39; +\
                          &#39;Please use dfferent value for fpath.&#39;)
    elif os.path.exists(fpath):
        pass
    elif not os.path.exists(fpath):
        os.makedirs(fpath)
    tok =self.get_tokenizer()
    tok.save_pretrained(fpath)
    return</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.set_config"><code class="name flex">
<span>def <span class="ident">set_config</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_config(self, config):
    self.config = config</code></pre>
</details>
</dd>
<dt id="ktrain.text.preprocessor.TransformersPreprocessor.set_tokenizer"><code class="name flex">
<span>def <span class="ident">set_tokenizer</span></span>(<span>self, tokenizer)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_tokenizer(self, tokenizer):
    self.tok = tokenizer</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></b></code>:
<ul class="hlist">
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats">print_seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.seqlen_stats">seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.undo" href="#ktrain.text.preprocessor.TextPreprocessor.undo">undo</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ktrain.text" href="index.html">ktrain.text</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ktrain.text.preprocessor.bert_tokenize" href="#ktrain.text.preprocessor.bert_tokenize">bert_tokenize</a></code></li>
<li><code><a title="ktrain.text.preprocessor.detect_text_format" href="#ktrain.text.preprocessor.detect_text_format">detect_text_format</a></code></li>
<li><code><a title="ktrain.text.preprocessor.file_len" href="#ktrain.text.preprocessor.file_len">file_len</a></code></li>
<li><code><a title="ktrain.text.preprocessor.fname_from_url" href="#ktrain.text.preprocessor.fname_from_url">fname_from_url</a></code></li>
<li><code><a title="ktrain.text.preprocessor.get_bert_path" href="#ktrain.text.preprocessor.get_bert_path">get_bert_path</a></code></li>
<li><code><a title="ktrain.text.preprocessor.get_coefs" href="#ktrain.text.preprocessor.get_coefs">get_coefs</a></code></li>
<li><code><a title="ktrain.text.preprocessor.get_wv_path" href="#ktrain.text.preprocessor.get_wv_path">get_wv_path</a></code></li>
<li><code><a title="ktrain.text.preprocessor.hf_convert_example" href="#ktrain.text.preprocessor.hf_convert_example">hf_convert_example</a></code></li>
<li><code><a title="ktrain.text.preprocessor.hf_convert_examples" href="#ktrain.text.preprocessor.hf_convert_examples">hf_convert_examples</a></code></li>
<li><code><a title="ktrain.text.preprocessor.hf_features_to_tfdataset" href="#ktrain.text.preprocessor.hf_features_to_tfdataset">hf_features_to_tfdataset</a></code></li>
<li><code><a title="ktrain.text.preprocessor.is_nospace_lang" href="#ktrain.text.preprocessor.is_nospace_lang">is_nospace_lang</a></code></li>
<li><code><a title="ktrain.text.preprocessor.load_wv" href="#ktrain.text.preprocessor.load_wv">load_wv</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ktrain.text.preprocessor.BERTPreprocessor" href="#ktrain.text.preprocessor.BERTPreprocessor">BERTPreprocessor</a></code></h4>
<ul class="">
<li><code><a title="ktrain.text.preprocessor.BERTPreprocessor.get_preprocessor" href="#ktrain.text.preprocessor.BERTPreprocessor.get_preprocessor">get_preprocessor</a></code></li>
<li><code><a title="ktrain.text.preprocessor.BERTPreprocessor.get_tokenizer" href="#ktrain.text.preprocessor.BERTPreprocessor.get_tokenizer">get_tokenizer</a></code></li>
<li><code><a title="ktrain.text.preprocessor.BERTPreprocessor.preprocess" href="#ktrain.text.preprocessor.BERTPreprocessor.preprocess">preprocess</a></code></li>
<li><code><a title="ktrain.text.preprocessor.BERTPreprocessor.preprocess_test" href="#ktrain.text.preprocessor.BERTPreprocessor.preprocess_test">preprocess_test</a></code></li>
<li><code><a title="ktrain.text.preprocessor.BERTPreprocessor.preprocess_train" href="#ktrain.text.preprocessor.BERTPreprocessor.preprocess_train">preprocess_train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ktrain.text.preprocessor.DistilBertPreprocessor" href="#ktrain.text.preprocessor.DistilBertPreprocessor">DistilBertPreprocessor</a></code></h4>
</li>
<li>
<h4><code><a title="ktrain.text.preprocessor.StandardTextPreprocessor" href="#ktrain.text.preprocessor.StandardTextPreprocessor">StandardTextPreprocessor</a></code></h4>
<ul class="two-column">
<li><code><a title="ktrain.text.preprocessor.StandardTextPreprocessor.get_preprocessor" href="#ktrain.text.preprocessor.StandardTextPreprocessor.get_preprocessor">get_preprocessor</a></code></li>
<li><code><a title="ktrain.text.preprocessor.StandardTextPreprocessor.get_tokenizer" href="#ktrain.text.preprocessor.StandardTextPreprocessor.get_tokenizer">get_tokenizer</a></code></li>
<li><code><a title="ktrain.text.preprocessor.StandardTextPreprocessor.ngram_count" href="#ktrain.text.preprocessor.StandardTextPreprocessor.ngram_count">ngram_count</a></code></li>
<li><code><a title="ktrain.text.preprocessor.StandardTextPreprocessor.preprocess" href="#ktrain.text.preprocessor.StandardTextPreprocessor.preprocess">preprocess</a></code></li>
<li><code><a title="ktrain.text.preprocessor.StandardTextPreprocessor.preprocess_test" href="#ktrain.text.preprocessor.StandardTextPreprocessor.preprocess_test">preprocess_test</a></code></li>
<li><code><a title="ktrain.text.preprocessor.StandardTextPreprocessor.preprocess_train" href="#ktrain.text.preprocessor.StandardTextPreprocessor.preprocess_train">preprocess_train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ktrain.text.preprocessor.TextPreprocessor" href="#ktrain.text.preprocessor.TextPreprocessor">TextPreprocessor</a></code></h4>
<ul class="two-column">
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.check_trained" href="#ktrain.text.preprocessor.TextPreprocessor.check_trained">check_trained</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.get_classes" href="#ktrain.text.preprocessor.TextPreprocessor.get_classes">get_classes</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.get_preprocessor" href="#ktrain.text.preprocessor.TextPreprocessor.get_preprocessor">get_preprocessor</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.get_tokenizer" href="#ktrain.text.preprocessor.TextPreprocessor.get_tokenizer">get_tokenizer</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.is_chinese" href="#ktrain.text.preprocessor.TextPreprocessor.is_chinese">is_chinese</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.is_nospace_lang" href="#ktrain.text.preprocessor.TextPreprocessor.is_nospace_lang">is_nospace_lang</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.migrate_classes" href="#ktrain.text.preprocessor.TextPreprocessor.migrate_classes">migrate_classes</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.preprocess" href="#ktrain.text.preprocessor.TextPreprocessor.preprocess">preprocess</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.print_seqlen_stats">print_seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.process_chinese" href="#ktrain.text.preprocessor.TextPreprocessor.process_chinese">process_chinese</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.seqlen_stats" href="#ktrain.text.preprocessor.TextPreprocessor.seqlen_stats">seqlen_stats</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.set_classes" href="#ktrain.text.preprocessor.TextPreprocessor.set_classes">set_classes</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.set_multilabel" href="#ktrain.text.preprocessor.TextPreprocessor.set_multilabel">set_multilabel</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TextPreprocessor.undo" href="#ktrain.text.preprocessor.TextPreprocessor.undo">undo</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ktrain.text.preprocessor.Transformer" href="#ktrain.text.preprocessor.Transformer">Transformer</a></code></h4>
<ul class="">
<li><code><a title="ktrain.text.preprocessor.Transformer.preprocess_test" href="#ktrain.text.preprocessor.Transformer.preprocess_test">preprocess_test</a></code></li>
<li><code><a title="ktrain.text.preprocessor.Transformer.preprocess_train" href="#ktrain.text.preprocessor.Transformer.preprocess_train">preprocess_train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ktrain.text.preprocessor.TransformerDataset" href="#ktrain.text.preprocessor.TransformerDataset">TransformerDataset</a></code></h4>
<ul class="">
<li><code><a title="ktrain.text.preprocessor.TransformerDataset.get_y" href="#ktrain.text.preprocessor.TransformerDataset.get_y">get_y</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformerDataset.nsamples" href="#ktrain.text.preprocessor.TransformerDataset.nsamples">nsamples</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformerDataset.to_tfdataset" href="#ktrain.text.preprocessor.TransformerDataset.to_tfdataset">to_tfdataset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ktrain.text.preprocessor.TransformerEmbedding" href="#ktrain.text.preprocessor.TransformerEmbedding">TransformerEmbedding</a></code></h4>
<ul class="">
<li><code><a title="ktrain.text.preprocessor.TransformerEmbedding.embed" href="#ktrain.text.preprocessor.TransformerEmbedding.embed">embed</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ktrain.text.preprocessor.TransformersPreprocessor" href="#ktrain.text.preprocessor.TransformersPreprocessor">TransformersPreprocessor</a></code></h4>
<ul class="">
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_classifier" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_classifier">get_classifier</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_config" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_config">get_config</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_model" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_model">get_model</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_preprocessor" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_preprocessor">get_preprocessor</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_regression_model" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_regression_model">get_regression_model</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.get_tokenizer" href="#ktrain.text.preprocessor.TransformersPreprocessor.get_tokenizer">get_tokenizer</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.load_model_and_configure_from_data" href="#ktrain.text.preprocessor.TransformersPreprocessor.load_model_and_configure_from_data">load_model_and_configure_from_data</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.preprocess" href="#ktrain.text.preprocessor.TransformersPreprocessor.preprocess">preprocess</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.preprocess_test" href="#ktrain.text.preprocessor.TransformersPreprocessor.preprocess_test">preprocess_test</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.preprocess_train" href="#ktrain.text.preprocessor.TransformersPreprocessor.preprocess_train">preprocess_train</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.save_tokenizer" href="#ktrain.text.preprocessor.TransformersPreprocessor.save_tokenizer">save_tokenizer</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.set_config" href="#ktrain.text.preprocessor.TransformersPreprocessor.set_config">set_config</a></code></li>
<li><code><a title="ktrain.text.preprocessor.TransformersPreprocessor.set_tokenizer" href="#ktrain.text.preprocessor.TransformersPreprocessor.set_tokenizer">set_tokenizer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>